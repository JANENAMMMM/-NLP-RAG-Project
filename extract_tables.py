import json
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence

import pandas as pd
import pdfplumber

ROOT_DIR = Path(__file__).resolve().parent

POSSIBLE_PDF_PATHS = [
    ROOT_DIR / "ewha.pdf",
    ROOT_DIR.parent / "ewha.pdf",
]
PDF_PATH = POSSIBLE_PDF_PATHS[0]

OUTPUT_DIR = ROOT_DIR
DEGREES_FILE = OUTPUT_DIR / "degrees.csv"
CONTRACT_FILE = OUTPUT_DIR / "contract_dept.csv"


def normalize_cell(value: Optional[str]) -> str:
    if value is None:
        return ""
    return str(value).replace("\n", " ").replace("\r", " ").replace("\xa0", " ").strip()


def normalize_header(value: str) -> str:
    cleaned = normalize_cell(value)
    for ch in [" ", "Â·", ".", "ã†"]:
        cleaned = cleaned.replace(ch, "")
    for ch in "()[]{}:;,-_/\\.":
        cleaned = cleaned.replace(ch, "")
    return cleaned


TABLE_SPECS = [
    {
        "name": "degrees",
        "targets": {"ì„¤ì¹˜ëŒ€í•™", "í•™ê³¼_ì „ê³µ", "í•™ìœ„_ì¢…ë¥˜"},
        "outputs": ["ì„¤ì¹˜ëŒ€í•™", "í•™ê³¼_ì „ê³µ", "í•™ìœ„_ì¢…ë¥˜"],
        "path": DEGREES_FILE,
        "page_range": (50, 52),  # ë³„í‘œ 2: í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜ (í˜ì´ì§€ 51)
        "exclude_columns": {"ì„¤ì¹˜í˜•íƒœ"},  # ì„¤ì¹˜í˜•íƒœ ì»¬ëŸ¼ì´ ì—†ì–´ì•¼ í•¨
    },
    {
        "name": "contract",
        "targets": {"ì„¤ì¹˜ëŒ€í•™", "ì„¤ì¹˜í˜•íƒœ", "í•™ê³¼_ì „ê³µ", "í•™ìœ„_ì¢…ë¥˜", "ì…í•™ì •ì›_ëª…", "ì„¤ì¹˜_ìš´ì˜ê¸°ê°„"},
        "outputs": ["ì„¤ì¹˜ëŒ€í•™", "ì„¤ì¹˜í˜•íƒœ", "í•™ê³¼_ì „ê³µ", "í•™ìœ„_ì¢…ë¥˜", "ì…í•™ì •ì›_ëª…", "ì„¤ì¹˜_ìš´ì˜ê¸°ê°„"],
        "path": CONTRACT_FILE,
        "page_range": (52, 54),  # ë³„í‘œ 3: ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ (í˜ì´ì§€ 53)
        "require_columns": {"ì„¤ì¹˜í˜•íƒœ"},  # ì„¤ì¹˜í˜•íƒœ ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨
    },
]

COLUMN_ALIASES: Dict[str, Iterable[str]] = {
    "ì„¤ì¹˜ëŒ€í•™": ["ì„¤ì¹˜ëŒ€í•™", "ëŒ€í•™", "ëŒ€ í•™"],
    "ì„¤ì¹˜í˜•íƒœ": ["ì„¤ì¹˜í˜•íƒœ"],
    "í•™ê³¼_ì „ê³µ": ["í•™ê³¼_ì „ê³µ", "í•™ê³¼ë˜ëŠ”ì „ê³µ", "í•™ê³¼ ë˜ëŠ” ì „ê³µ"],
    "í•™ìœ„_ì¢…ë¥˜": ["í•™ìœ„_ì¢…ë¥˜", "í•™ìœ„ì˜ì¢…ë¥˜"],
    "ì…í•™ì •ì›_ëª…": ["ì…í•™ì •ì›(ëª…)", "ì…í•™ì •ì›"],
    "ì„¤ì¹˜_ìš´ì˜ê¸°ê°„": ["ì„¤ì¹˜Â·ìš´ì˜ê¸°ê°„", "ì„¤ì¹˜ìš´ì˜ê¸°ê°„"],
}

HEADER_LOOKUP: Dict[str, str] = {}
for canonical, variants in COLUMN_ALIASES.items():
    for variant in variants:
        HEADER_LOOKUP[normalize_header(variant)] = canonical


def table_to_dataframe(table: Sequence[Sequence[str]]) -> Optional[pd.DataFrame]:
    rows = [[normalize_cell(cell) for cell in row] for row in table if any(str(cell).strip() for cell in row)]
    if not rows:
        return None
    header = [normalize_header(cell) for cell in rows[0]]
    data_rows = rows[1:]
    if not data_rows:
        return None

    max_len = max(len(header), *(len(r) for r in data_rows))
    header = (header + [""] * max_len)[:max_len]
    normalized_data = [
        (row + [""] * max_len)[:max_len]
        for row in data_rows
    ]
    return pd.DataFrame(normalized_data, columns=header)


def map_columns(df: pd.DataFrame) -> pd.DataFrame:
    renamed_cols = {}
    for col in df.columns:
        key = normalize_header(col)
        renamed = HEADER_LOOKUP.get(key)
        if renamed:
            renamed_cols[col] = renamed
    if not renamed_cols:
        return pd.DataFrame()
    return df[list(renamed_cols.keys())].rename(columns=renamed_cols)


def extract_tables(
    target_columns: set,
    page_range: Optional[tuple] = None,
    exclude_columns: Optional[set] = None,
    require_columns: Optional[set] = None,
) -> pd.DataFrame:
    """
    íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” í…Œì´ë¸” ì¶”ì¶œ
    
    Args:
        target_columns: í•„ìˆ˜ë¡œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” ì»¬ëŸ¼ë“¤
        page_range: (ì‹œì‘í˜ì´ì§€, ëí˜ì´ì§€) íŠœí”Œ. Noneì´ë©´ ëª¨ë“  í˜ì´ì§€ ê²€ìƒ‰
        exclude_columns: í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì•ˆ ë˜ëŠ” ì»¬ëŸ¼ë“¤
        require_columns: ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” ì»¬ëŸ¼ë“¤
    """
    frames: List[pd.DataFrame] = []
    with pdfplumber.open(PDF_PATH) as pdf:
        for page_number, page in enumerate(pdf.pages, start=1):
            # í˜ì´ì§€ ë²”ìœ„ í•„í„°ë§
            if page_range:
                start_page, end_page = page_range
                if not (start_page <= page_number <= end_page):
                    continue
            
            tables = page.extract_tables()
            for table in tables:
                df = table_to_dataframe(table)
                if df is None:
                    continue
                mapped = map_columns(df)
                if mapped.empty:
                    continue
                
                mapped_columns = set(mapped.columns)
                
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                if not target_columns.issubset(mapped_columns):
                    continue
                
                # ì œì™¸í•  ì»¬ëŸ¼ í™•ì¸
                if exclude_columns and exclude_columns.intersection(mapped_columns):
                    continue
                
                # í•„ìˆ˜ í¬í•¨ ì»¬ëŸ¼ í™•ì¸
                if require_columns and not require_columns.issubset(mapped_columns):
                    continue
                
                # ëŒ€ìƒ ì»¬ëŸ¼ë§Œ ì„ íƒ
                ordered = [col for col in mapped.columns if col in target_columns]
                frames.append(mapped[ordered])
    
    if not frames:
        raise ValueError(
            f"í•„ìš”í•œ ì»¬ëŸ¼ {target_columns} ì„ í¬í•¨í•œ í‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"(í˜ì´ì§€ ë²”ìœ„: {page_range}, ì œì™¸ ì»¬ëŸ¼: {exclude_columns}, í•„ìˆ˜ ì»¬ëŸ¼: {require_columns})"
        )
    combined = pd.concat(frames, ignore_index=True)
    combined = combined.loc[:, ~combined.columns.duplicated()]
    return combined


def main() -> None:
    global PDF_PATH
    for path in POSSIBLE_PDF_PATHS:
        if path.exists():
            PDF_PATH = path
            break
    else:
        raise FileNotFoundError("ewha.pdf íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("ğŸ“„ PDF í‘œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    summary: Dict[str, int] = {}
    for spec in TABLE_SPECS:
        df = extract_tables(
            target_columns=spec["targets"],
            page_range=spec.get("page_range"),
            exclude_columns=spec.get("exclude_columns"),
            require_columns=spec.get("require_columns"),
        )
        
        # ì¶œë ¥ ì»¬ëŸ¼ì— ì—†ëŠ” ì»¬ëŸ¼ì€ ë¹ˆ ê°’ìœ¼ë¡œ ì¶”ê°€
        for column in spec["outputs"]:
            if column not in df.columns:
                df[column] = ""
        
        # ì¶œë ¥ ì»¬ëŸ¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        df = df[spec["outputs"]]
        df.to_csv(spec["path"], index=False, encoding="utf-8-sig")
        summary[f"{spec['name']}_rows"] = len(df)
        print(f"  - {spec['name']} í‘œ {len(df)}ê±´ ì €ì¥ ({spec['path'].name})")

    print("âœ… CSV ì €ì¥ ì™„ë£Œ:", json.dumps(summary, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

