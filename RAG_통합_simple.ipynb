{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a197cc",
   "metadata": {},
   "source": [
    "# RAG ê¸°ë³¸ ë²„ì „\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ RAG ì‹œìŠ¤í…œì˜ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ë§Œ í¬í•¨í•©ë‹ˆë‹¤:\n",
    "- PDF ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬ ë° ì²­í‚¹\n",
    "- CSV í‘œ ë°ì´í„° ì²˜ë¦¬\n",
    "- FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "- ê¸°ë³¸ RAG ì§ˆì˜ì‘ë‹µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e75a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‘ì—… ë””ë ‰í† ë¦¬: c:\\Users\\janen\\Documents\\25-2 ê°•ì˜\\ìì—°ì–´ì²˜ë¦¬\\í”„ë¡œì íŠ¸\\ewha\n",
      "âœ… Upstage API Key: up_EoF0I0CzeHxuDYmf0...\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Upstage API Key ì„¤ì •\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"up_EoF0I0CzeHxuDYmf0we544GMPCFIT\")\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "CURRENT_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "if Path.cwd() != CURRENT_DIR:\n",
    "    os.chdir(CURRENT_DIR)\n",
    "\n",
    "print(f\"âœ… ì‘ì—… ë””ë ‰í† ë¦¬: {CURRENT_DIR}\")\n",
    "print(f\"âœ… Upstage API Key: {UPSTAGE_API_KEY[:20]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31860c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_upstage in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.3.35)\n",
      "Requirement already satisfied: openai in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (1.107.2)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.78 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (0.3.79)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (4.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21.0,>=0.20.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (0.20.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (6.0.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tokenizers<0.21.0,>=0.20.0->langchain_upstage) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (2025.10.0)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from wikipedia) (4.13.5)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install langchain_upstage langchain_community langchain-openai openai pdfplumber pandas faiss-cpu wikipedia-api wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "303f6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "âœ… ì´ 38í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“Œ ë¶€ì¹™ ì‹œì‘ ì¸ë±ìŠ¤: 20919\n",
      "ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: 20919\n",
      "ğŸ“„ ë¶€ì¹™ ê¸¸ì´: 20222\n",
      "ğŸ“‹ ë³¸ë¬¸ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\n",
      "   â†’ 108ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\n",
      "âœ… ë³¸ë¬¸ ì²­í¬ ìˆ˜: 108ê°œ\n",
      "ğŸ“‹ ë¶€ì¹™ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\n",
      "   â†’ 108ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\n",
      "âœ… ë¶€ì¹™ ì²­í¬ ìˆ˜: 109ê°œ\n",
      "\n",
      "ğŸ“¦ ì´ í…ìŠ¤íŠ¸ ì²­í¬: 217ê°œ\n",
      "   - ë³¸ë¬¸: 108ê°œ\n",
      "   - ë¶€ì¹™: 109ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 1. PDF ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬ ë° ì²­í‚¹ (ë‹¤í˜„ë‹˜ ì‘ì—… ê¸°ë°˜)\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "PDF_PATH = CURRENT_DIR / \"ewha.pdf\"\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{PDF_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()\n",
    "docs = [d for d in docs if int(d.metadata[\"page\"]) < 38]  # ë¶€ì¹™ ì´ì „ í˜ì´ì§€ë§Œ\n",
    "print(f\"âœ… ì´ {len(docs)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ì „ì²´ í…ìŠ¤íŠ¸ ë³‘í•©\n",
    "full_text = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# \"ë¶€ì¹™\" ë“±ì¥ ì§€ì  ì°¾ê¸°\n",
    "split_index = full_text.find(\"ë¶€ì¹™\")\n",
    "print(f\"ğŸ“Œ ë¶€ì¹™ ì‹œì‘ ì¸ë±ìŠ¤: {split_index}\")\n",
    "\n",
    "# ë³¸ë¬¸ / ë¶€ì¹™ ë¶„ë¦¬\n",
    "if split_index != -1:\n",
    "    main_text = full_text[:split_index]\n",
    "    appendix_text = full_text[split_index:]\n",
    "else:\n",
    "    main_text = full_text\n",
    "    appendix_text = \"\"\n",
    "\n",
    "print(f\"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(main_text)}\")\n",
    "print(f\"ğŸ“„ ë¶€ì¹™ ê¸¸ì´: {len(appendix_text)}\")\n",
    "\n",
    "# ==================================================================\n",
    "# PDF êµ¬ì¡° ë¶„ì„ ë° ì¡°í•­ë³„ ì²­í‚¹ í•¨ìˆ˜\n",
    "# ==================================================================\n",
    "\n",
    "def split_by_articles(text: str, is_appendix: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    PDF í…ìŠ¤íŠ¸ë¥¼ ì¡°í•­ë³„ë¡œ ì •í™•íˆ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    ì´í™”ì—¬ëŒ€ í•™ì¹™ êµ¬ì¡° ë¶„ì„:\n",
    "    - ë³„í‘œ 1, ë³„í‘œ 2, ë³„í‘œ 3 ë“± (í‘œ)\n",
    "    - ì œ1ì¡°, ì œ2ì¡°, ì œ3ì¡° ë“± (ì¡°í•­)\n",
    "    - â‘ , â‘¡, â‘¢ ë“± (í•­ëª©)\n",
    "    - ë¶€ì¹™ (ë¶€ì¹™ ì‹œì‘)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # ì¡°í•­ ì‹œì‘ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ)\n",
    "    # 1. ë³„í‘œ (í‘œëŠ” ë…ë¦½ì ì¸ ë‹¨ìœ„)\n",
    "    # 2. ì œXì¡° (ì¡°í•­)\n",
    "    # 3. â‘ í•­, â‘¡í•­ ë“± (í•­ëª© - ì¡°í•­ ë‚´ ì„¸ë¶€ì‚¬í•­)\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # íŒ¨í„´ 1: ë³„í‘œë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ (ê°€ì¥ í° ë‹¨ìœ„)\n",
    "    star_pattern = r'(ë³„í‘œ\\s*\\d+)'\n",
    "    star_matches = list(re.finditer(star_pattern, text))\n",
    "    \n",
    "    # íŒ¨í„´ 2: ì œXì¡°ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„\n",
    "    article_pattern = r'(ì œ\\d+ì¡°(?:\\s*\\([^)]+\\))?)'  # ì œ1ì¡°, ì œ1ì¡°(ëª©ì ) ë“±\n",
    "    article_matches = list(re.finditer(article_pattern, text))\n",
    "    \n",
    "    # ë³„í‘œê°€ ìˆìœ¼ë©´ ë³„í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ë¶„ë¦¬\n",
    "    if star_matches:\n",
    "        last_pos = 0\n",
    "        for match in star_matches:\n",
    "            pos = match.start()\n",
    "            if pos > last_pos:\n",
    "                chunk = text[last_pos:pos].strip()\n",
    "                if chunk:\n",
    "                    splits.append(chunk)\n",
    "            last_pos = pos\n",
    "        # ë§ˆì§€ë§‰ ë³„í‘œ ì´í›„ í…ìŠ¤íŠ¸\n",
    "        if last_pos < len(text):\n",
    "            chunk = text[last_pos:].strip()\n",
    "            if chunk:\n",
    "                splits.append(chunk)\n",
    "        \n",
    "        # ë³„í‘œë¡œ ë¶„ë¦¬ëœ ê° ì²­í¬ë¥¼ ì œXì¡°ë¡œ ë‹¤ì‹œ ë¶„ë¦¬\n",
    "        refined_splits = []\n",
    "        for split in splits:\n",
    "            split_article_matches = list(re.finditer(article_pattern, split))\n",
    "            if split_article_matches:\n",
    "                split_last_pos = 0\n",
    "                for match in split_article_matches:\n",
    "                    pos = match.start()\n",
    "                    if pos > split_last_pos:\n",
    "                        chunk = split[split_last_pos:pos].strip()\n",
    "                        if chunk:\n",
    "                            refined_splits.append(chunk)\n",
    "                    split_last_pos = pos\n",
    "                if split_last_pos < len(split):\n",
    "                    chunk = split[split_last_pos:].strip()\n",
    "                    if chunk:\n",
    "                        refined_splits.append(chunk)\n",
    "            else:\n",
    "                refined_splits.append(split)\n",
    "        splits = refined_splits\n",
    "    elif article_matches:\n",
    "        # ë³„í‘œê°€ ì—†ìœ¼ë©´ ì œXì¡° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "        last_pos = 0\n",
    "        for match in article_matches:\n",
    "            pos = match.start()\n",
    "            if pos > last_pos:\n",
    "                chunk = text[last_pos:pos].strip()\n",
    "                if chunk:\n",
    "                    splits.append(chunk)\n",
    "            last_pos = pos\n",
    "        # ë§ˆì§€ë§‰ ì¡°í•­ ì´í›„ í…ìŠ¤íŠ¸\n",
    "        if last_pos < len(text):\n",
    "            chunk = text[last_pos:].strip()\n",
    "            if chunk:\n",
    "                splits.append(chunk)\n",
    "    else:\n",
    "        # ë³„í‘œë„ ì œXì¡°ë„ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ\n",
    "        splits = [text]\n",
    "    \n",
    "    # íŒ¨í„´ 3: â‘ í•­, â‘¡í•­ ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ (ê¸´ ì¡°í•­ ë‚´ì—ì„œë§Œ)\n",
    "    # ì´ëŠ” ë‚˜ì¤‘ì— ì„¸ë¶€ ì²­í‚¹ì—ì„œ ì²˜ë¦¬\n",
    "    \n",
    "    # ë¶€ì¹™ì˜ ê²½ìš° \"ë¶€ì¹™\" í‚¤ì›Œë“œë¡œë„ ë¶„ë¦¬\n",
    "    if is_appendix and splits:\n",
    "        final_splits = []\n",
    "        for split in splits:\n",
    "            # \"ë¶€ì¹™\" í‚¤ì›Œë“œê°€ ì¤‘ê°„ì— ë‚˜ì˜¤ë©´ ë¶„ë¦¬\n",
    "            if \"ë¶€ì¹™\" in split:\n",
    "                parts = re.split(r'(ë¶€ì¹™)', split, maxsplit=1)\n",
    "                if len(parts) > 1:\n",
    "                    if parts[0].strip():\n",
    "                        final_splits.append(parts[0].strip())\n",
    "                    if len(parts) > 2:\n",
    "                        final_splits.append(parts[1] + parts[2].strip())\n",
    "                else:\n",
    "                    final_splits.append(split)\n",
    "            else:\n",
    "                final_splits.append(split)\n",
    "        splits = final_splits\n",
    "    \n",
    "    # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ ê¸¸ì´ ì²´í¬\n",
    "    splits = [s for s in splits if s and len(s.strip()) > 50]\n",
    "    \n",
    "    return splits if splits else [text]\n",
    "\n",
    "# ë³¸ë¬¸ì„ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬\n",
    "print(\"ğŸ“‹ ë³¸ë¬¸ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\")\n",
    "main_articles = split_by_articles(main_text, is_appendix=False)\n",
    "print(f\"   â†’ {len(main_articles)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\")\n",
    "\n",
    "# ê° ì¡°í•­ì„ ì²­í‚¹ (ì¡°í•­ì´ ë„ˆë¬´ ê¸¸ ê²½ìš°ë§Œ)\n",
    "main_chunks = []\n",
    "for i, article in enumerate(main_articles):\n",
    "    # ì¡°í•­ì´ 1200ì ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if len(article) <= 1200:\n",
    "        main_chunks.append(Document(\n",
    "            page_content=article,\n",
    "            metadata={\"source\": \"ewha.pdf ë³¸ë¬¸\", \"type\": \"main_text\", \"article_index\": i}\n",
    "        ))\n",
    "    else:\n",
    "        # ì¡°í•­ì´ ê¸¸ë©´ ì„¸ë¶€ í•­ëª©ë³„ë¡œ ë¶„ë¦¬\n",
    "        text_splitter_main = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1200,\n",
    "            chunk_overlap=200,  # overlap ê°ì†Œ (ì¡°í•­ ê²½ê³„ ìœ ì§€)\n",
    "            separators=[\"\\n\\nâ‘ \", \"\\n\\nâ‘¡\", \"\\n\\nâ‘¢\", \"\\n\\nâ‘£\", \"\\n\\nâ‘¤\", \"\\n\\nâ‘¥\", \"\\n\\nâ‘¦\", \"\\n\\nâ‘§\", \"\\n\\nâ‘¨\", \"\\n\\nâ‘©\",\n",
    "                       \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\", \"\\nâ‘¥\", \"\\nâ‘¦\", \"\\nâ‘§\", \"\\nâ‘¨\", \"\\nâ‘©\",\n",
    "                       \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "        )\n",
    "        sub_chunks = text_splitter_main.split_text(article)\n",
    "        for j, chunk in enumerate(sub_chunks):\n",
    "            main_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"source\": \"ewha.pdf ë³¸ë¬¸\", \"type\": \"main_text\", \"article_index\": i, \"sub_index\": j}\n",
    "            ))\n",
    "\n",
    "print(f\"âœ… ë³¸ë¬¸ ì²­í¬ ìˆ˜: {len(main_chunks)}ê°œ\")\n",
    "\n",
    "# ë¶€ì¹™ì„ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬\n",
    "if appendix_text:\n",
    "    print(\"ğŸ“‹ ë¶€ì¹™ ì¡°í•­ë³„ ë¶„ë¦¬ ì¤‘...\")\n",
    "    appendix_articles = split_by_articles(appendix_text, is_appendix=True)\n",
    "    print(f\"   â†’ {len(appendix_articles)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„ë¦¬ë¨\")\n",
    "    \n",
    "    appendix_chunks = []\n",
    "    for i, article in enumerate(appendix_articles):\n",
    "        # ë¶€ì¹™ ì¡°í•­ì€ ë” ì‘ê²Œ ì²­í‚¹ (ì„¸ë¶€ ê·œì •ì´ ë§ìŒ)\n",
    "        if len(article) <= 1000:\n",
    "            appendix_chunks.append(Document(\n",
    "                page_content=article,\n",
    "                metadata={\"source\": \"ewha.pdf ë¶€ì¹™\", \"type\": \"appendix_text\", \"article_index\": i}\n",
    "            ))\n",
    "        else:\n",
    "            # ë¶€ì¹™ ì¡°í•­ì´ ê¸¸ë©´ ì„¸ë¶€ í•­ëª©ë³„ë¡œ ë¶„ë¦¬\n",
    "            text_splitter_appendix = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=150,  # overlap ê°ì†Œ\n",
    "                separators=[\"\\n\\nâ‘ \", \"\\n\\nâ‘¡\", \"\\n\\nâ‘¢\", \"\\n\\nâ‘£\", \"\\n\\nâ‘¤\", \"\\n\\nâ‘¥\", \"\\n\\nâ‘¦\", \"\\n\\nâ‘§\", \"\\n\\nâ‘¨\", \"\\n\\nâ‘©\",\n",
    "                           \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\", \"\\nâ‘¥\", \"\\nâ‘¦\", \"\\nâ‘§\", \"\\nâ‘¨\", \"\\nâ‘©\",\n",
    "                           \"ë³„í‘œ\", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "            )\n",
    "            sub_chunks = text_splitter_appendix.split_text(article)\n",
    "            for j, chunk in enumerate(sub_chunks):\n",
    "                appendix_chunks.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": \"ewha.pdf ë¶€ì¹™\", \"type\": \"appendix_text\", \"article_index\": i, \"sub_index\": j}\n",
    "                ))\n",
    "    \n",
    "    print(f\"âœ… ë¶€ì¹™ ì²­í¬ ìˆ˜: {len(appendix_chunks)}ê°œ\")\n",
    "else:\n",
    "    appendix_chunks = []\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ í…ìŠ¤íŠ¸ ì²­í¬: {len(main_chunks) + len(appendix_chunks)}ê°œ\")\n",
    "print(f\"   - ë³¸ë¬¸: {len(main_chunks)}ê°œ\")\n",
    "print(f\"   - ë¶€ì¹™: {len(appendix_chunks)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ae83ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… degree_sentences_all.csv ì²˜ë¦¬ ì™„ë£Œ: 160ê°œ ë¬¸ì¥\n",
      "âœ… capacity.csv ì²˜ë¦¬ ì™„ë£Œ: 582ê°œ ë¬¸ì„œ\n",
      "âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: 1ê°œ ë¬¸ì„œ\n",
      "âœ… grade.csv ì²˜ë¦¬ ì™„ë£Œ: 26ê°œ ë¬¸ì„œ\n",
      "\n",
      "ğŸ“¦ ì´ CSV ë¬¸ì„œ: 769ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 2. CSV í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©)\n",
    "# ==================================================================\n",
    "\n",
    "def coalesce(*values: object) -> str:\n",
    "    \"\"\"ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’ì„ ë°˜í™˜\"\"\"\n",
    "    for value in values:\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            continue\n",
    "        text = str(value).strip()\n",
    "        if text:\n",
    "            return text\n",
    "    return \"\"\n",
    "\n",
    "# ë‹¤í˜„ë‹˜ ì‘ì—…: í•™ìœ„ ë¬¸ì¥ ìƒì„± ë°©ì‹\n",
    "def build_degree_sentences_from_csv(df: pd.DataFrame, year: str = \"ìµœì‹  ê°œì •\") -> List[str]:\n",
    "    \"\"\"CSVì—ì„œ í•™ìœ„ ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ ë°©ì‹)\"\"\"\n",
    "    sentences = []\n",
    "    for idx, row in df.iterrows():\n",
    "        college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"), row.get(\"ëŒ€í•™\"))\n",
    "        degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"), row.get(\"í•™ìœ„ì¢…ë¥˜\"), row.get(\"í•™ì‚¬\"))\n",
    "        major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"), row.get(\"ì „ê³µ\"))\n",
    "        \n",
    "        if not college or not degree or not major:\n",
    "            continue\n",
    "            \n",
    "        sentence = f\"{college}ì˜ {major} ì „ê³µì€ {degree} í•™ìœ„ë¥¼ ìˆ˜ì—¬í•œë‹¤. ({year})\"\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# ì‚¬ìš©ì ì‘ì—…: í‘œ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def build_quota_text(row: pd.Series) -> str:\n",
    "    \"\"\"ì…í•™ì •ì› í…ìŠ¤íŠ¸ ìƒì„± (capacity.csv êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)\"\"\"\n",
    "    year = coalesce(row.get(\"year\"), row.get(\"í•™ë…„ë„\"))\n",
    "    college = coalesce(row.get(\"college\"), row.get(\"ëŒ€í•™\"))\n",
    "    department = coalesce(row.get(\"department\"), row.get(\"í•™ë¶€\"))\n",
    "    major = coalesce(row.get(\"major\"), row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"))\n",
    "    quota = coalesce(row.get(\"quota\"), row.get(\"ì •ì›\"), row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    \n",
    "    # quota ê°’ ì •ë¦¬: \"[53]\" ê°™ì€ ê´„í˜¸ ë¶€ë¶„ ì œê±°\n",
    "    if quota and not pd.isna(quota):\n",
    "        import re\n",
    "        quota_str = str(quota).strip()\n",
    "        # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ì˜ˆ: \"99[53]\" -> \"99\")\n",
    "        quota_str = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', quota_str).strip()\n",
    "        quota = quota_str\n",
    "    \n",
    "    # ë¹ˆ ê°’ ì²´í¬\n",
    "    if not quota or pd.isna(quota) or str(quota).strip() == \"\":\n",
    "        quota = \"ë¯¸ìƒ\"\n",
    "    \n",
    "    # ì—°ë„ í¬ë§·íŒ…\n",
    "    if year and not pd.isna(year):\n",
    "        year_str = f\"{int(year)}í•™ë…„ë„\" if str(year).isdigit() else str(year)\n",
    "    else:\n",
    "        year_str = \"\"\n",
    "    \n",
    "    # ë¬¸ì¥ êµ¬ì„±\n",
    "    parts = []\n",
    "    if year_str:\n",
    "        parts.append(year_str)\n",
    "    if college:\n",
    "        parts.append(college)\n",
    "    if department:\n",
    "        parts.append(department)\n",
    "    \n",
    "    prefix = \" \".join(parts) if parts else \"\"\n",
    "    \n",
    "    # ì „ê³µì´ ìˆìœ¼ë©´ ì „ê³µ í¬í•¨\n",
    "    if major and str(major).strip():\n",
    "        return f\"{prefix} ì†Œì† {major}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "    # ì „ê³µì´ ì—†ê³  departmentì— í•™ê³¼ëª…ì´ ìˆìœ¼ë©´ department ì‚¬ìš©\n",
    "    elif department and str(department).strip() and (\"ê³¼\" in str(department) or \"í•™ë¶€\" in str(department)):\n",
    "        return f\"{prefix} ì†Œì† {department}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "    # departmentë§Œ ìˆìœ¼ë©´\n",
    "    elif department:\n",
    "        return f\"{prefix} ì†Œì† {department}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "    # collegeë§Œ ìˆìœ¼ë©´\n",
    "    elif college:\n",
    "        return f\"{prefix}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        return f\"{year_str} ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "def build_contract_text(row: pd.Series) -> str:\n",
    "    \"\"\"ê³„ì•½í•™ê³¼ í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    form = coalesce(row.get(\"ì„¤ì¹˜í˜•íƒœ\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    parts = [f\"ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ ì •ë³´: ì„¤ì¹˜ëŒ€í•™={college}\"]\n",
    "    if form:\n",
    "        parts.append(f\"ì„¤ì¹˜í˜•íƒœ={form}\")\n",
    "    if major:\n",
    "        parts.append(f\"í•™ê³¼/ì „ê³µ={major}\")\n",
    "    if degree:\n",
    "        parts.append(f\"ìˆ˜ì—¬ í•™ìœ„={degree}\")\n",
    "    if quota:\n",
    "        parts.append(f\"ì…í•™ ì •ì›={quota}ëª…\")\n",
    "    if period:\n",
    "        parts.append(f\"ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„={period}\")\n",
    "    return \", \".join(parts) + \".\"\n",
    "\n",
    "def build_grade_text(row: pd.Series) -> str:\n",
    "    \"\"\"ì„±ì ì  í…ìŠ¤íŠ¸ ìƒì„± (grade.csv êµ¬ì¡°ì— ë§ê²Œ)\"\"\"\n",
    "    year = coalesce(row.get(\"year\"))\n",
    "    grade = coalesce(row.get(\"grade\"))\n",
    "    gpa = coalesce(row.get(\"gpa\"))\n",
    "    \n",
    "    if not year or not grade or pd.isna(gpa):\n",
    "        return \"\"\n",
    "    \n",
    "    # yearê°€ \"from 1980\"ì´ë©´ \"1980í•™ë…„ë„ ì´í›„\", \"before 1980\"ì´ë©´ \"1980í•™ë…„ë„ ì´ì „\"\n",
    "    if \"from 1980\" in str(year).lower() or \"1980 ì´í›„\" in str(year):\n",
    "        year_desc = \"1980í•™ë…„ë„ ì´í›„ ì…í•™ìƒ\"\n",
    "    elif \"before 1980\" in str(year).lower() or \"1980 ì´ì „\" in str(year):\n",
    "        year_desc = \"1980í•™ë…„ë„ ì´ì „ ì…í•™ìƒ\"\n",
    "    else:\n",
    "        year_desc = str(year)\n",
    "    \n",
    "    return f\"{year_desc}ì— ì ìš©í•˜ëŠ” ë“±ê¸‰ {grade}ì˜ ì„±ì ì ì€ {gpa}ì ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "# CSV íŒŒì¼ ì²˜ë¦¬ (ewha csvs í´ë” í†µí•©)\n",
    "csv_documents = []\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì • (ewha csvs í´ë” ìš°ì„ )\n",
    "csvs_dir = CURRENT_DIR / \"ewha csvs\"\n",
    "if not csvs_dir.exists():\n",
    "    csvs_dir = CURRENT_DIR  # í´ë”ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "\n",
    "# 1) degrees ê´€ë ¨ íŒŒì¼ ì²˜ë¦¬ (ewha csvs í´ë” ìš°ì„ )\n",
    "degrees_dir = csvs_dir / \"degrees\" if (csvs_dir / \"degrees\").exists() else CURRENT_DIR / \"degrees\"\n",
    "degree_sentences_file = csvs_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\" if (csvs_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\").exists() else degrees_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\"\n",
    "degree_latest_file = csvs_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\" if (csvs_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\").exists() else degrees_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\"\n",
    "\n",
    "# ìš°ì„ ìˆœìœ„ 1: ì´ë¯¸ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ ì‚¬ìš©\n",
    "if degree_sentences_file.exists():\n",
    "    df_sentences = pd.read_csv(degree_sentences_file, encoding='utf-8-sig')\n",
    "    for _, row in df_sentences.iterrows():\n",
    "        sentence = str(row.get(\"sentence\", \"\")).strip()\n",
    "        if sentence:\n",
    "            csv_documents.append(Document(\n",
    "                page_content=sentence,\n",
    "                metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "            ))\n",
    "    print(f\"âœ… degree_sentences_all.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_sentences)}ê°œ ë¬¸ì¥\")\n",
    "# ìš°ì„ ìˆœìœ„ 2: degree_latest.csvë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ìƒì„±\n",
    "elif degree_latest_file.exists():\n",
    "    df_degrees = pd.read_csv(degree_latest_file, encoding='utf-8-sig')\n",
    "    # ë¹ˆ í–‰ ì œê±°\n",
    "    df_degrees = df_degrees.dropna(subset=['ëŒ€í•™', 'í•™ì‚¬', 'ì „ê³µ'], how='all')\n",
    "    # ë‹¤í˜„ë‹˜ ë°©ì‹: í•™ìœ„ ë¬¸ì¥ ìƒì„±\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degree_latest.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "# ìš°ì„ ìˆœìœ„ 3: ê¸°ì¡´ degrees.csv ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)\n",
    "elif (CURRENT_DIR / \"degrees.csv\").exists():\n",
    "    df_degrees = pd.read_csv(CURRENT_DIR / \"degrees.csv\", encoding='utf-8-sig')\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degrees.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ degrees ê´€ë ¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2) capacity.csv ì²˜ë¦¬ (ì…í•™ì •ì› ë°ì´í„°)\n",
    "capacity_path = csvs_dir / \"capacity.csv\" if (csvs_dir / \"capacity.csv\").exists() else CURRENT_DIR / \"capacity.csv\"\n",
    "if capacity_path.exists():\n",
    "    df_capacity = pd.read_csv(capacity_path, encoding='utf-8-sig')\n",
    "    # ë¹ˆ í–‰ ì œê±°\n",
    "    df_capacity = df_capacity.dropna(subset=['year', 'college', 'quota'], how='all')\n",
    "    \n",
    "    # quota ê°’ ì •ë¦¬: \"[53]\" ê°™ì€ ê´„í˜¸ ë¶€ë¶„ ì œê±° (ì˜ˆ: \"99[53]\" -> \"99\")\n",
    "    def clean_quota(quota_val):\n",
    "        if pd.isna(quota_val):\n",
    "            return quota_val\n",
    "        quota_str = str(quota_val).strip()\n",
    "        # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ì˜ˆ: \"99[53]\" -> \"99\", \"100(50)\" -> \"100\")\n",
    "        import re\n",
    "        quota_str = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', quota_str).strip()\n",
    "        return quota_str\n",
    "    \n",
    "    df_capacity['quota'] = df_capacity['quota'].apply(clean_quota)\n",
    "    \n",
    "    # \"ì „ì²´\" í–‰ ì œê±° (departmentë‚˜ majorì— \"ì „ì²´\"ê°€ í¬í•¨ëœ ê²½ìš°)\n",
    "    # ë‹¨, \"ì˜ê³¼ëŒ€í•™ ì „ì²´\" ê°™ì€ ê²½ìš°ëŠ” quota ê°’ì´ ìœ íš¨í•˜ë©´ ìœ ì§€ (ëŒ€í•™ ì „ì²´ ì •ì› ì •ë³´)\n",
    "    # quota ê°’ì´ ì´ë¯¸ ì •ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸\n",
    "    df_capacity = df_capacity[\n",
    "        ~(\n",
    "            # departmentì™€ major ëª¨ë‘ì— \"ì „ì²´\"ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì œê±°\n",
    "            (df_capacity['department'].astype(str).str.contains('ì „ì²´', na=False)) &\n",
    "            (df_capacity['major'].astype(str).str.contains('ì „ì²´', na=False))\n",
    "        ) |\n",
    "        (\n",
    "            # \"ì „ì²´\" í–‰ì´ì§€ë§Œ quota ê°’ì´ ìœ íš¨í•œ ìˆ«ìì¸ ê²½ìš°ëŠ” ìœ ì§€ (ëŒ€í•™ ì „ì²´ ì •ì›)\n",
    "            (df_capacity['department'].astype(str).str.contains('ì „ì²´', na=False)) &\n",
    "            (df_capacity['quota'].astype(str).str.match(r'^\\d+$', na=False))  # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°\n",
    "        )\n",
    "    ]\n",
    "    # majorê°€ ìˆê±°ë‚˜, departmentì— í•™ê³¼ëª…ì´ ìˆëŠ” í–‰ë§Œ ì‚¬ìš©\n",
    "    # (majorê°€ ë¹„ì–´ìˆì–´ë„ departmentì— êµ¬ì²´ì ì¸ í•™ê³¼ëª…ì´ ìˆìœ¼ë©´ í¬í•¨)\n",
    "    df_capacity = df_capacity[\n",
    "        (\n",
    "            (df_capacity['major'].notna()) & \n",
    "            (df_capacity['major'] != '') &\n",
    "            (df_capacity['major'].astype(str).str.strip() != '')\n",
    "        ) |\n",
    "        (\n",
    "            (df_capacity['department'].notna()) & \n",
    "            (df_capacity['department'] != '') &\n",
    "            (df_capacity['department'].astype(str).str.strip() != '') &\n",
    "            (df_capacity['department'].astype(str).str.contains('ê³¼|ë¶€|í•™ë¶€', na=False))\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for _, row in df_capacity.iterrows():\n",
    "        text = build_quota_text(row)\n",
    "        if text and text.strip():\n",
    "            year = str(row.get(\"year\", \"\")).strip()\n",
    "            csv_documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": f\"[ë³„í‘œ 1] {year}í•™ë…„ë„ ì…í•™ì •ì›\" if year else \"[ë³„í‘œ 1] ì…í•™ì •ì›\",\n",
    "                    \"page\": 39,\n",
    "                    \"type\": \"quota\",\n",
    "                    \"year\": year\n",
    "                }\n",
    "            ))\n",
    "    print(f\"âœ… capacity.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_capacity)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ capacity.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3) contract_dept.csv ì²˜ë¦¬\n",
    "contract_path = csvs_dir / \"contract_dept.csv\" if (csvs_dir / \"contract_dept.csv\").exists() else CURRENT_DIR / \"contract_dept.csv\"\n",
    "if contract_path.exists():\n",
    "    df_contract = pd.read_csv(contract_path, encoding='utf-8-sig')\n",
    "    for _, row in df_contract.iterrows():\n",
    "        text = build_contract_text(row)\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 3] ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜\", \"page\": 53, \"type\": \"contract\"}\n",
    "        ))\n",
    "    print(f\"âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_contract)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ contract_dept.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 4) grade.csv ì²˜ë¦¬ (1980ë…„ ê¸°ì¤€ ì„±ì ì  ë°ì´í„°)\n",
    "grade_path = csvs_dir / \"grade.csv\" if (csvs_dir / \"grade.csv\").exists() else CURRENT_DIR / \"grade.csv\"\n",
    "if grade_path.exists():\n",
    "    df_grade = pd.read_csv(grade_path, encoding='utf-8-sig')\n",
    "    for _, row in df_grade.iterrows():\n",
    "        text = build_grade_text(row)\n",
    "        if text and text.strip():\n",
    "            csv_documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": \"[ë³„í‘œ] ì„±ì ì  ë“±ê¸‰í‘œ\", \"page\": 0, \"type\": \"grade\"}\n",
    "            ))\n",
    "    print(f\"âœ… grade.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_grade)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ grade.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ CSV ë¬¸ì„œ: {len(csv_documents)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“š ì´ 986ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„±\n",
      "\n",
      "ğŸ“– Phase 1: ìµœìš°ì„  Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\n",
      "   - Kohlberg's stages of moral development ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 45ê°œ ì²­í¬ ìƒì„±\n",
      "   - Robbery ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 25ê°œ ì²­í¬ ìƒì„±\n",
      "   - Larceny ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 47ê°œ ì²­í¬ ìƒì„±\n",
      "   - Burglary ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 36ê°œ ì²­í¬ ìƒì„±\n",
      "   - Homo erectus ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 46ê°œ ì²­í¬ ìƒì„±\n",
      "   - Neoteny ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 20ê°œ ì²­í¬ ìƒì„±\n",
      "\n",
      "ğŸ“– Phase 2: ì°¨ìˆœìœ„ Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\n",
      "   - Tang dynasty ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 154ê°œ ì²­í¬ ìƒì„±\n",
      "   - Deutschland Ã¼ber alles ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 34ê°œ ì²­í¬ ìƒì„±\n",
      "   - Bipolar disorder ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 109ê°œ ì²­í¬ ìƒì„±\n",
      "   - Ethics in psychology ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 191ê°œ ì²­í¬ ìƒì„±\n",
      "   - Utilitarianism ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 129ê°œ ì²­í¬ ìƒì„±\n",
      "   - Categorical imperative ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 51ê°œ ì²­í¬ ìƒì„±\n",
      "   - Jurisprudence ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 66ê°œ ì²­í¬ ìƒì„±\n",
      "\n",
      "ğŸ“– Phase 3: ë³´ì¡° Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\n",
      "   - Human evolution ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 135ê°œ ì²­í¬ ìƒì„±\n",
      "   - Homo habilis ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 46ê°œ ì²­í¬ ìƒì„±\n",
      "   - Australopithecus afarensis ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 57ê°œ ì²­í¬ ìƒì„±\n",
      "   - Aristotelianism ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 27ê°œ ì²­í¬ ìƒì„±\n",
      "   - Peter Singer ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 78ê°œ ì²­í¬ ìƒì„±\n",
      "   - Green marketing ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 37ê°œ ì²­í¬ ìƒì„±\n",
      "   - Theory of multiple intelligences ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 74ê°œ ì²­í¬ ìƒì„±\n",
      "   - Approach-approach conflict ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "âš ï¸ Wikipedia ë¬¸ì„œ 'Approach-approach conflict' ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: Page id \"Approach-approach conflict\" does not match any pages. Try another id!\n",
      "     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "   - Receiving stolen property ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 19ê°œ ì²­í¬ ìƒì„±\n",
      "   - Legal positivism ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 31ê°œ ì²­í¬ ìƒì„±\n",
      "   - John Frere ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 7ê°œ ì²­í¬ ìƒì„±\n",
      "   - Du Fu ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "     âœ… 46ê°œ ì²­í¬ ìƒì„±\n",
      "   - Recognition of foreign judgments ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "âš ï¸ Wikipedia ë¬¸ì„œ 'Recognition of foreign judgments' ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: Page id \"Recognition of foreign judgments\" does not match any pages. Try another id!\n",
      "     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "\n",
      "âœ… Wikipedia ë¬¸ì„œ 1510ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ\n",
      "\n",
      "ğŸ“š ì´ 2496ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„± (í•™ì¹™: 986, Wikipedia: 1510)\n",
      "âœ… FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì„ë² ë”©\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import wikipedia\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-embedding-1-large\"\n",
    ")\n",
    "print(\"âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ í†µí•©\n",
    "all_documents = []\n",
    "all_documents.extend(main_chunks)\n",
    "if appendix_chunks:\n",
    "    all_documents.extend(appendix_chunks)\n",
    "if csv_documents:\n",
    "    all_documents.extend(csv_documents)\n",
    "\n",
    "print(f\"ğŸ“š ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "# ==================================================================\n",
    "# 3-1. ìˆ˜ë™ ì§€ì‹ ë¬¸ì„œ ë²¡í„°í™” (ì„ íƒ)\n",
    "# ==================================================================\n",
    "manual_documents = []\n",
    "manual_dir = CURRENT_DIR / \"manual_rules\"\n",
    "if manual_dir.exists():\n",
    "    print(\"\\nğŸ“˜ ìˆ˜ë™ ì§€ì‹ ë¬¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "    for file in manual_dir.glob(\"*.txt\"):\n",
    "        try:\n",
    "            text = file.read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ {file.name} ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "        if not text:\n",
    "            continue\n",
    "        text_splitter_manual = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        chunks = text_splitter_manual.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            manual_documents.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": f\"manual:{file.name}\",\n",
    "                    \"type\": \"manual\",\n",
    "                    \"priority\": \"high\",\n",
    "                    \"chunk_index\": i\n",
    "                }\n",
    "            ))\n",
    "    if manual_documents:\n",
    "        all_documents.extend(manual_documents)\n",
    "        print(f\"   âœ… ìˆ˜ë™ ì§€ì‹ ì²­í¬ {len(manual_documents)}ê°œ ì¶”ê°€\")\n",
    "else:\n",
    "    print(\"ğŸ“˜ ìˆ˜ë™ ì§€ì‹ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"ğŸ“š ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„± (ìˆ˜ë™ ì§€ì‹ í¬í•¨)\")\n",
    "\n",
    "# ==================================================================\n",
    "# 3-2. Wikipedia ë¬¸ì„œ ë²¡í„°í™” (ìµœìš°ì„  ë¬¸ì„œ)\n",
    "# ==================================================================\n",
    "\n",
    "def fetch_wikipedia_article(title: str, language: str = \"en\") -> Optional[str]:\n",
    "    \"\"\"Wikipedia ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    try:\n",
    "        wikipedia.set_lang(language)\n",
    "        page = wikipedia.page(title, auto_suggest=False)\n",
    "        return page.content\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # ë™ìŒì´ì˜ì–´ì¸ ê²½ìš° ì²« ë²ˆì§¸ ì˜µì…˜ ì‚¬ìš©\n",
    "        try:\n",
    "            page = wikipedia.page(e.options[0], auto_suggest=False)\n",
    "            return page.content\n",
    "        except:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Wikipedia ë¬¸ì„œ '{title}' ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_wikipedia_document(title: str, content: str, priority: str = \"high\") -> List[Document]:\n",
    "    \"\"\"Wikipedia ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
    "    if not content:\n",
    "        return []\n",
    "    \n",
    "    # ê¸´ ë¬¸ì„œëŠ” ì²­í‚¹\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(content)\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": f\"Wikipedia: {title}\",\n",
    "                \"type\": \"wikipedia\",\n",
    "                \"priority\": priority,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# ìµœìš°ì„  Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 1)\n",
    "priority_1_wiki_titles = {\n",
    "    \"Kohlberg's stages of moral development\": \"high\",\n",
    "    \"Robbery\": \"high\",\n",
    "    \"Larceny\": \"high\",\n",
    "    \"Burglary\": \"high\",\n",
    "    \"Homo erectus\": \"high\",\n",
    "    \"Neoteny\": \"high\",\n",
    "}\n",
    "\n",
    "# ì°¨ìˆœìœ„ Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 2)\n",
    "priority_2_wiki_titles = {\n",
    "    \"Tang dynasty\": \"medium\",\n",
    "    \"Deutschland Ã¼ber alles\": \"medium\",\n",
    "    \"Bipolar disorder\": \"medium\",\n",
    "    \"Ethics in psychology\": \"medium\",\n",
    "    \"Utilitarianism\": \"medium\",\n",
    "    \"Categorical imperative\": \"medium\",\n",
    "    \"Jurisprudence\": \"medium\",\n",
    "    \"Famine, Affluence and Morality\": \"medium\",\n",
    "}\n",
    "\n",
    "# ë³´ì¡° Wikipedia ë¬¸ì„œ ëª©ë¡ (Phase 3)\n",
    "priority_3_wiki_titles = {\n",
    "    \"Human evolution\": \"low\",\n",
    "    \"Homo habilis\": \"low\",\n",
    "    \"Australopithecus afarensis\": \"low\",\n",
    "    \"Aristotelianism\": \"low\",\n",
    "    \"Peter Singer\": \"low\",\n",
    "    \"Green marketing\": \"low\",\n",
    "    \"Theory of multiple intelligences\": \"low\",\n",
    "    \"Approach-approach conflict\": \"low\",\n",
    "    \"Receiving stolen property\": \"low\",\n",
    "    \"Legal positivism\": \"low\",\n",
    "    \"John Frere\": \"low\",  # Q46 ì¶”ê°€\n",
    "    \"Du Fu\": \"low\",  # Q29 ì¶”ê°€\n",
    "    \"Recognition of foreign judgments\": \"low\",  # Q42 ì¶”ê°€\n",
    "}\n",
    "\n",
    "# ë²¡í„°í™”í•  ë¬¸ì„œ ì„ íƒ (ê¸°ë³¸ê°’: ì „ì²´ ì ìš©)\n",
    "WIKI_PHASE = 3  # 1: ìµœìš°ì„ , 2: ìµœìš°ì„ +ì°¨ìˆœìœ„, 3: ì „ì²´\n",
    "\n",
    "wiki_documents = []\n",
    "\n",
    "if WIKI_PHASE >= 1:\n",
    "    print(\"\\nğŸ“– Phase 1: ìµœìš°ì„  Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_1_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "if WIKI_PHASE >= 2:\n",
    "    print(\"\\nğŸ“– Phase 2: ì°¨ìˆœìœ„ Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_2_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "if WIKI_PHASE >= 3:\n",
    "    print(\"\\nğŸ“– Phase 3: ë³´ì¡° Wikipedia ë¬¸ì„œ ë²¡í„°í™” ì¤‘...\")\n",
    "    for title, priority in priority_3_wiki_titles.items():\n",
    "        print(f\"   - {title} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        content = fetch_wikipedia_article(title)\n",
    "        if content:\n",
    "            docs = create_wikipedia_document(title, content, priority)\n",
    "            wiki_documents.extend(docs)\n",
    "            print(f\"     âœ… {len(docs)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"     âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "if wiki_documents:\n",
    "    all_documents.extend(wiki_documents)\n",
    "    print(f\"\\nâœ… Wikipedia ë¬¸ì„œ {len(wiki_documents)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸ“š ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„± (í•™ì¹™: {len(all_documents) - len(wiki_documents)}, Wikipedia: {len(wiki_documents)})\")\n",
    "\n",
    "# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vector_store = FAISS.from_documents(all_documents, embeddings)\n",
    "print(\"âœ… FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ\n",
      "âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 4. ê¸°ë³¸ RAG ì§ˆì˜ì‘ë‹µ\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# LLM ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro2\")\n",
    "\n",
    "# ì§ˆë¬¸ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "classification_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì´ ì´í™”ì—¬ìëŒ€í•™êµ í•™ì¹™(ì…í•™, í•™ì‚¬, ì„±ì , í•™ìœ„, ì „ê³µ, ìˆ˜ê°•, ì¡¸ì—… ë“±) ê´€ë ¨ ë¬¸ì œì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.\n",
    "    \n",
    "    ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œì˜ ì˜ˆì‹œ:\n",
    "    - ì´í™”ì—¬ëŒ€ì˜ ì…í•™ì •ì›, í•™ìœ„ ì¢…ë¥˜, í•™ê³¼ ì •ë³´\n",
    "    - ì´í™”ì—¬ëŒ€ì˜ í•™ì‚¬ ê·œì •, ì„±ì  ì ìˆ˜, í•™ì  ì´ìˆ˜\n",
    "    - ì´í™”ì—¬ëŒ€ì˜ ì „ê³µ, ë³µìˆ˜ì „ê³µ, ë¶€ì „ê³µ ê´€ë ¨ ê·œì •\n",
    "    - ì´í™”ì—¬ëŒ€ì˜ ì¡¸ì—… ìš”ê±´, ì¬í•™ ì—°í•œ ê´€ë ¨ ê·œì •\n",
    "    \n",
    "    ì´í™”ì—¬ëŒ€ í•™ì¹™ê³¼ ë¬´ê´€í•œ ë¬¸ì œì˜ ì˜ˆì‹œ:\n",
    "    - ì¼ë°˜ì ì¸ ì—­ì‚¬, ê³¼í•™, ì‹¬ë¦¬í•™, ì² í•™ ë¬¸ì œ\n",
    "    - ë‹¤ë¥¸ ëŒ€í•™ì´ë‚˜ ì¼ë°˜ì ì¸ ë²•ë¥  ë¬¸ì œ\n",
    "    - Wikipediaì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì¼ë°˜ ì§€ì‹ ë¬¸ì œ\n",
    "    \n",
    "    ì§ˆë¬¸: {question}\n",
    "    \n",
    "    ì‘ë‹µ í˜•ì‹: \"YES\" ë˜ëŠ” \"NO\"ë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    - \"YES\": ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œ\n",
    "    - \"NO\": ì´í™”ì—¬ëŒ€ í•™ì¹™ê³¼ ë¬´ê´€í•œ ë¬¸ì œ\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°œì„  ë²„ì „ - ë²”ìš©ì  ì •í™•ë„ í–¥ìƒ)\n",
    "rag_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    \n",
    "    **ì¤‘ìš” ì§€ì¹¨:**\n",
    "    1. ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš” (êµ¬ì²´ì ì¸ ê·œì •)\n",
    "    2. Wikipedia ë¬¸ì„œëŠ” ì¼ë°˜ì ì¸ í•™ë¬¸ ì§€ì‹ ì°¸ê³ ìš©ì…ë‹ˆë‹¤\n",
    "    3. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n",
    "    4. **ê° ì„ íƒì§€ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€ì¦í•˜ì„¸ìš”** - ëª¨ë“  ì„ íƒì§€ë¥¼ ìˆœì„œëŒ€ë¡œ í™•ì¸í•˜ê³  ì •í™•í•œ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”\n",
    "    5. ì •í™•íˆ í•˜ë‚˜ì˜ ì„ íƒì§€ë§Œ ë‹µë³€í•˜ì„¸ìš” (ì˜ˆ: \"ì •ë‹µ: (A)\" ë˜ëŠ” \"ì •ë‹µ: (A)\" ë˜ëŠ” \"(A)\")\n",
    "    6. ì—¬ëŸ¬ ì„ íƒì§€ê°€ ì–¸ê¸‰ë˜ì–´ë„ ìµœì¢… ë‹µë³€ì€ í•˜ë‚˜ë§Œ ì œì‹œí•˜ì„¸ìš”\n",
    "    7. ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ ì •í™•íˆ \"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”\n",
    "    \n",
    "    **ë‹µë³€ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**\n",
    "    - ë§ˆì§€ë§‰ì— \"ì •ë‹µ: (X)\" í˜•ì‹ìœ¼ë¡œ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”\n",
    "    - ì˜ˆ: \"ì •ë‹µ: (A)\" ë˜ëŠ” \"ì •ë‹µì€ (A)ì…ë‹ˆë‹¤\"\n",
    "    - ì„¤ëª…ì´ í•„ìš”í•˜ë©´ ê°„ë‹¨íˆ ì¶”ê°€í•˜ë˜, ì„ íƒì§€ëŠ” ë°˜ë“œì‹œ ëª…í™•íˆ í‘œì‹œ\n",
    "    \n",
    "    **íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:**\n",
    "    - \"í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²ƒ\", \"ì˜³ì§€ ì•Šì€ ê²ƒ\", \"ì˜ëª»ëœ ê²ƒ\" ë“± ë¶€ì •í˜• ì§ˆë¬¸ì— ì£¼ì˜í•˜ì„¸ìš”\n",
    "      â†’ ì§ˆë¬¸ì´ ë¬´ì—‡ì„ ì°¾ëŠ”ì§€ ì •í™•íˆ íŒŒì•…í•˜ê³ , ê° ì„ íƒì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€ì¡°í•˜ì„¸ìš”\n",
    "    - ìˆ˜í•™ ê³„ì‚° ë¬¸ì œëŠ” ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•˜ì—¬ ì •í™•í•œ ë‹µì„ êµ¬í•˜ì„¸ìš”\n",
    "    - ë²•ë¥  ìš©ì–´ëŠ” ì •í™•í•œ ì •ì˜ë¥¼ í™•ì¸í•˜ì„¸ìš” (robbery vs larceny vs burglary ë“±)\n",
    "      â†’ ê° ìš©ì–´ì˜ ì •í™•í•œ ì •ì˜ì™€ ì°¨ì´ì ì„ í™•ì¸í•˜ì„¸ìš”\n",
    "    - ì‹¬ë¦¬í•™ ì´ë¡ ì˜ ë‹¨ê³„ëŠ” ì •í™•í•œ ë²ˆí˜¸ë¥¼ í™•ì¸í•˜ì„¸ìš” (Stage 1, Stage 2 ë“±)\n",
    "      â†’ ë‹¨ê³„ ë²ˆí˜¸ì™€ íŠ¹ì§•ì„ ì •í™•íˆ ë§¤ì¹­í•˜ì„¸ìš”\n",
    "    - ì—­ì‚¬/ë¬¸í•™ ë¬¸ì œëŠ” ë§¥ë½ì„ ê³ ë ¤í•˜ì„¸ìš”\n",
    "      â†’ ì‘í’ˆì˜ ì—­ì‚¬ì  ë°°ê²½ê³¼ ì‘ê°€ì˜ ì˜ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”\n",
    "    \n",
    "    **ë‹µë³€ ì ˆì°¨:**\n",
    "    1. ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì„¸ìš” (ë¬´ì—‡ì„ ë¬»ëŠ”ì§€, ë¶€ì •í˜•ì¸ì§€)\n",
    "    2. ê° ì„ íƒì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€ì¡°í•˜ì„¸ìš”\n",
    "    3. ì„ íƒì§€ë³„ íŒë‹¨ì„ \"(A) O/X - ì´ìœ \" í˜•ì‹ìœ¼ë¡œ ê°„ê²°íˆ ê¸°ë¡í•˜ì„¸ìš”\n",
    "    4. ê°€ì¥ ì •í™•í•œ ì„ íƒì§€ë¥¼ ì„ íƒí•˜ê³  ë§ˆì§€ë§‰ì— \"ì •ë‹µ: (X)\" í˜•ì‹ìœ¼ë¡œ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”\n",
    "    \n",
    "    **ì¶œë ¥ í…œí”Œë¦¿ (ì˜ˆì‹œ)**\n",
    "    ì„ íƒì§€ ê²€í† :\n",
    "    - (A) O/X - ê·¼ê±°\n",
    "    - (B) ...\n",
    "    ê²°ë¡ : ...\n",
    "    ì •ë‹µ: (X)\n",
    "    \n",
    "    ---\n",
    "    ì§ˆë¬¸: {question}\n",
    "    ---\n",
    "    ì»¨í…ìŠ¤íŠ¸:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# ì¼ë°˜ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (RAG ì—†ì´)\n",
    "general_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ---\n",
    "    ì§ˆë¬¸: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "classification_chain = classification_prompt_template | llm\n",
    "rag_chain = rag_prompt_template | llm\n",
    "general_chain = general_prompt_template | llm\n",
    "\n",
    "# Retriever ì„¤ì • (ê°œì„ : ê²€ìƒ‰ ê°œìˆ˜ ì¦ê°€)\n",
    "# ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œëŠ” ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ í–¥ìƒ\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 10  # ê²€ìƒ‰ ê°œìˆ˜ ì¦ê°€ (5 â†’ 10) - ë²”ìš©ì  ì •í™•ë„ í–¥ìƒ\n",
    "    }\n",
    ")\n",
    "\n",
    "# MMR (Maximum Marginal Relevance) Retriever ì„¤ì •\n",
    "# ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ê²€ìƒ‰ìœ¼ë¡œ ì¤‘ë³µëœ ë¬¸ì„œ ì œê±°\n",
    "try:\n",
    "    from langchain_community.retrievers import BM25Retriever\n",
    "    from langchain.retrievers import EnsembleRetriever\n",
    "    # BM25 + FAISS í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)\n",
    "    # BM25ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ\n",
    "    print(\"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ BM25Retrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ\")\n",
    "\n",
    "# ì§ˆë¬¸ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_question(question: str) -> bool:\n",
    "    \"\"\"ì§ˆë¬¸ì´ ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œì¸ì§€ íŒë‹¨\"\"\"\n",
    "    try:\n",
    "        response = classification_chain.invoke({\"question\": question})\n",
    "        answer = response.content.strip().upper()\n",
    "        # \"YES\" ë˜ëŠ” \"YES\"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° True\n",
    "        return answer.startswith(\"YES\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’: ì´í™”ì—¬ëŒ€ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ True\n",
    "        ewha_keywords = [\"ì´í™”\", \"ì´í™”ì—¬ëŒ€\", \"ì´í™”ì—¬ìëŒ€í•™êµ\", \"ewha\"]\n",
    "        question_lower = question.lower()\n",
    "        return any(keyword in question_lower for keyword in ewha_keywords)\n",
    "\n",
    "# ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "def ask_question(question: str):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ìë™ ë¶„ë¥˜)\"\"\"\n",
    "    # ì§ˆë¬¸ ë¶„ë¥˜\n",
    "    is_ewha_related = classify_question(question)\n",
    "    \n",
    "    if is_ewha_related:\n",
    "        # ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œ: RAG ì‚¬ìš©\n",
    "        source_docs = retriever.invoke(question)\n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in source_docs) or \"\"\n",
    "        response = rag_chain.invoke({\"question\": question, \"context\": context_text})\n",
    "        print(f\"\\n[ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨] ì§ˆë¬¸: {question}\")\n",
    "        print(f\"ë‹µë³€: {response.content}\")\n",
    "        print(f\"ì°¸ê³  ë¬¸ì„œ: {len(source_docs)}ê°œ\")\n",
    "    else:\n",
    "        # ì¼ë°˜ ë¬¸ì œ: RAG ì—†ì´ LLMë§Œ ì‚¬ìš©\n",
    "        response = general_chain.invoke({\"question\": question})\n",
    "        print(f\"\\n[ì¼ë°˜ ë¬¸ì œ] ì§ˆë¬¸: {question}\")\n",
    "        print(f\"ë‹µë³€: {response.content}\")\n",
    "        print(f\"ì°¸ê³  ë¬¸ì„œ: ì—†ìŒ (RAG ë¯¸ì‚¬ìš©)\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ì˜ˆì‹œ ì§ˆë¬¸\n",
    "# ask_question(\"ì´í™”ì—¬ëŒ€ì˜ ì…í•™ì •ì›ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab287a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ testset.csv íŒŒì¼ ê²½ë¡œ: c:\\Users\\janen\\Documents\\25-2 ê°•ì˜\\ìì—°ì–´ì²˜ë¦¬\\í”„ë¡œì íŠ¸\\testset.csv\n",
      "âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 50ê°œ ë¡œë“œ ì™„ë£Œ\n",
      "âœ… ì •ë‹µ 50ê°œ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "â–¶ ì§ˆë¬¸ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„± ì‹œì‘... (ì´ 50ê°œ ì§ˆë¬¸)\n",
      "   ì§„í–‰: 10/50 ì™„ë£Œ (ì¼ë°˜ ë¬¸ì œ)\n",
      "   ìµœê·¼ ì§ˆë¬¸: QUESTION10) ì´í™”ì—¬ìëŒ€í•™êµì˜ ìœ„ì¹˜ëŠ” ì–´ë””ì¸ê°€ìš”? â€¨(A) ê°•ë‚¨êµ¬ â€¨(B) ì„œëŒ€ë¬¸êµ¬ â€¨(C) ì¢…ë¡œêµ¬ ...\n",
      "   ìµœê·¼ ì‘ë‹µ: ì´í™”ì—¬ìëŒ€í•™êµì˜ ìœ„ì¹˜ëŠ” **ì„œëŒ€ë¬¸êµ¬**ì— ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ì •ë‹µ: **(B) ì„œëŒ€ë¬¸êµ¬**  \n",
      "\n",
      "ì´í™”ì—¬ìëŒ€í•™êµì˜ ì •í™•í•œ ì£¼ì†ŒëŠ” **ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸...\n",
      "   ì§„í–‰: 20/50 ì™„ë£Œ (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨)\n",
      "   ìµœê·¼ ì§ˆë¬¸: QUESTION20) ì¬í•™ ì—°í•œ ì´ˆê³¼ë¡œ ì œì ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ”?\n",
      "(A) í•™ì‚¬ í¸ì…\n",
      "(B) ë³µìˆ˜ ì „ê³µ ì¤‘\n",
      "(C...\n",
      "   ìµœê·¼ ì‘ë‹µ: ì¬í•™ ì—°í•œ ì´ˆê³¼ë¡œ ì œì ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ì°¾ê¸° ìœ„í•´ ì£¼ì–´ì§„ í•™ì¹™ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **(A) í•™ì‚¬ í¸ì…**: ë¬¸ì„œ1ì— ë”°...\n",
      "   ì§„í–‰: 30/50 ì™„ë£Œ (ì¼ë°˜ ë¬¸ì œ)\n",
      "   ìµœê·¼ ì§ˆë¬¸: QUESTION30)Homo erectus differed from Homo habilis in which ...\n",
      "   ìµœê·¼ ì‘ë‹µ: **í•µì‹¬ íŒŒì•…:**  \n",
      "ì§ˆë¬¸ì€ *Homo erectus*ì™€ *Homo habilis*ì˜ ì°¨ì´ì ì„ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. ì„ íƒì§€ ì¤‘ ì •í™•í•œ ì°¨ì´ë¥¼ ì°¾ì•„ì•¼...\n",
      "   ì§„í–‰: 40/50 ì™„ë£Œ (ì¼ë°˜ ë¬¸ì œ)\n",
      "   ìµœê·¼ ì§ˆë¬¸: QUESTION40)____________ refers to a strategic process involv...\n",
      "   ìµœê·¼ ì‘ë‹µ: ì£¼ì–´ì§„ ì§ˆë¬¸ì€ \"____________ refers to a strategic process involving stakeholder assess...\n",
      "   ì§„í–‰: 50/50 ì™„ë£Œ (ì¼ë°˜ ë¬¸ì œ)\n",
      "   ìµœê·¼ ì§ˆë¬¸: QUESTION50) Which is the least accurate description of legal...\n",
      "   ìµœê·¼ ì‘ë‹µ: ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì„ íƒì§€ë¥¼ ê²€í† í•œ ê²°ê³¼, **ë²•ì‹¤ì¦ì£¼ì˜(legal positivism)**ì— ëŒ€í•œ ê°€ì¥ ë¶€ì •í™•í•œ ì„¤ëª…ì€ ë‹¤ìŒ...\n",
      "\n",
      "âœ… RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: 50ê°œ\n",
      "\n",
      "ğŸ“ ë‹µë³€ ì¶”ì¶œ ì¤‘...\n",
      "âœ… ë‹µë³€ ì¶”ì¶œ ë° ì±„ì  ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ì±„ì  ê²°ê³¼:\n",
      "   - ì´ ë¬¸ì œ ìˆ˜: 50ê°œ\n",
      "   - ì •ë‹µ ìˆ˜: 43ê°œ\n",
      "   - ì˜¤ë‹µ ìˆ˜: 7ê°œ\n",
      "   - ì •ë‹µë¥ : 86.00%\n",
      "\n",
      "ğŸ“‹ ì§ˆë¬¸ ë¶„ë¥˜ í†µê³„:\n",
      "   - ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨: 23ê°œ\n",
      "   - ì¼ë°˜ ë¬¸ì œ: 27ê°œ\n",
      "\n",
      "âŒ ì˜¤ë‹µ ë¬¸ì œ (7ê°œ):\n",
      "   ë¬¸ì œ ë²ˆí˜¸: 8, 32, 38, 39, 42, 45, 48\n",
      "\n",
      "================================================================================\n",
      "ì˜¤ë‹µ ë¬¸ì œ ìƒì„¸ ë¶„ì„:\n",
      "================================================================================\n",
      "\n",
      "[ë¬¸ì œ 8] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨)\n",
      "ì§ˆë¬¸: QUESTION8) ë³µìˆ˜ì „ê³µ ì‹ ì²­ ìê²©ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²ƒì€? \n",
      "(A) 1í•™ë…„ì„ ë§ˆì¹œ í•™ìƒ \n",
      "(B) í‰ê·  í‰ì ì´ 2.50 ì´ìƒì¸ í•™ìƒ \n",
      "(C) ì¡¸ì—… ì§ì „ í•™ê¸°ì— ìˆëŠ” í•™ìƒ \n",
      "(D) ì¬í•™ìƒ ì‹ ë¶„ì¸ ê²½ìš°\n",
      "ì˜ˆì¸¡ ë‹µë³€: ì¶”ì¶œ ì‹¤íŒ¨\n",
      "ì •ë‹µ: C\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨ - ì‘ë‹µì—ì„œ ì„ íƒì§€ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨\n",
      "     â†’ ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìŒ\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë³µìˆ˜ì „ê³µ ì‹ ì²­ ìê²©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì¡°í•­ì´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ì¤‘ [ë¬¸ì„œ 2] ì œ47ì¡°ëŠ” ë³µìˆ˜ì „ê³µì˜ ì •ì˜ì™€ ì´ìˆ˜ ê°€ëŠ¥ì„±ì„ ì–¸ê¸‰í•˜ì§€ë§Œ, ì‹ ì²­ ìê²© ìš”ê±´(ì˜ˆ: í•™ë…„, í‰ê·  í‰ì , ì¡¸ì—… ì§ì „ í•™ê¸° ì œí•œ ë“±)ì— ëŒ€í•´ì„œëŠ” ìì„¸íˆ ì„¤ëª…í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë”°ë¼ì„œ **\"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\"**ê°€ ì •í™•í•œ ë‹µë³€ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ì •ë‹µ: ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤**  \n",
      "\n",
      "(ë‹¨, ì¼ë°˜ì ì¸ ëŒ€í•™ ê·œì •ì„ ì°¸ê³ í•  ë•Œ ì¡¸ì—… ì§ì „ í•™ê¸°ì—ëŠ” ë³µìˆ˜ì „ê³µ ì‹ ì²­ì´ ì œí•œë  ìˆ˜ ìˆìœ¼ë‚˜, ì´ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ë¡ ì…ë‹ˆë‹¤.)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 32] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION32) This question refers to the following information: For the catastrophe of 1914 the Germans are responsible. â€¦ Germany, in this matter, was unfortunate enough to allow herself (in spite of her skill at dissimulation) to be betrayed into an excess of candour by her characteristic tendency to go to extremes. Deutschland Ã¼ber alles. Germany above everything! â€¦ There you have the ultimate framework of an old but childish race. Georges Clemenceau, Grandeur and Misery of Victory, 1930. From the passage, one may infer that Clemenceau believed: â€¨(A) that Germany's skill at dissimulation was responsible for its victory in the war â€¨(B) that the reason Germany lost the war was that it was betrayed from within â€¨(C) that the lyrics from the popular song Deutschland Ã¼ber alles (which eventually became the German national anthem) were evidence of Germany's aggressive attitude â€¨(D) that Germany's misfortune in the war was due to its lack of skill in dissimulation â€¨(E) that the lyrics from the popular song Deutschland Ã¼ber alles (which eventually became the German national anthem) were the reason Germany started the war â€¨(F) that Germany provided the ultimate framework for modern warfare â€¨(G) that Germany's excessive candour led to its downfall in the war\n",
      "ì˜ˆì¸¡ ë‹µë³€: G\n",
      "ì •ë‹µ: C\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: G, ì •ë‹µ: C\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì™€ ì„ íƒì§€ë¥¼ ë¶„ì„í•´ ë³´ê² ìŠµë‹ˆë‹¤.  \n",
      "\n",
      "í´ë ˆë§ì†Œì˜ ë°œì–¸ì—ì„œ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  \n",
      "1. **\"Germany, in this matter, was unfortunate enough to allow herself (in spite of her skill at dissimulation) to be betrayed into an excess of candour by her characteristic tendency to go to extremes.\"**  \n",
      "   - ë…ì¼ì€ ì¼ë°˜ì ìœ¼ë¡œ ìœ„ì¥(dissimulation)ì— ëŠ¥ìˆ™í•˜...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 38] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION38) A woman was standing in the aisle of a subway car and put her purse on the seat next to her. A man approached the woman from behind and grabbed the purse off the seat. He then pushed the woman out of the way and ran out of the subway car while carrying the purse. The man was apprehended on the subway platform while in possession of the purse. In a jurisdiction that follows the common law with respect to criminal offenses, of what crime can the man properly be convicted?\n",
      "\n",
      "(A) Fraud, because he took the purse without the woman's consent.\n",
      "(B) Larceny, because he took the purse without the woman's permission.\n",
      "(C) Burglary, because he entered the subway car with the intention of committing a theft.\n",
      "(D) Robbery, because he used force in leaving with the purse.\n",
      "(E) Robbery, because he used force to take possession of the purse.\n",
      "(F) Robbery, because he used force to remove the woman from the purse's vicinity.\n",
      "(G) Larceny, because force was not used until after he took the purse.\n",
      "(H) Assault, because he pushed the woman out of the way.\n",
      "(I) Larceny, because he made no threat to use force.\n",
      "(J) Robbery, because he physically took the purse from the woman's presence.\n",
      "ì˜ˆì¸¡ ë‹µë³€: E\n",
      "ì •ë‹µ: D\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: E, ì •ë‹µ: D\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•œ ê²°ê³¼, ì •ë‹µì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ì§ˆë¬¸ í•µì‹¬**: ì§€í•˜ì²  ì°¨ëŸ‰ ë‚´ì—ì„œ ì—¬ì„±ì´ ë†“ì€ ì§€ê°‘ì„ ë’¤ìª½ì—ì„œ ë‹¤ê°€ì˜¨ ë‚¨ì„±ì´ ë¹¼ì•—ê³ , ì—¬ì„±ì„ ë°€ì–´ ë„˜ì–´ëœ¨ë¦° í›„ ë„ì£¼í•œ ì‚¬ê±´ì—ì„œ, **ê³µí†µë²•(common law) ê´€í• ê¶Œ**ì—ì„œ ë‚¨ì„±ì´ ì–´ë–¤ ë²”ì£„ë¡œ ê¸°ì†Œë  ìˆ˜ ìˆëŠ”ì§€ ë¬»ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.  \n",
      "   - ë¶€ì •í˜•ì´ ì•„ë‹Œ **ì ì ˆí•œ ë²”ì£„ ìœ í˜•ì„ ì„ íƒ**í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **ì„ íƒì§€ ë¶„ì„**:  \n",
      "   - **(A) Fraud**: ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥´ë©´ ì‚¬ê¸°(Fraud)ëŠ” ì‚¬ê¸°ì  í–‰ìœ„ë¥¼ í†µí•´ ì¬ì‚°ì„ ì·¨í•˜ëŠ” ê²ƒì´ë©°, ì´ ì‚¬...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 39] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION39) A defendant met her friend at the electronics store where the friend worked. Unbeknownst to the defendant, her friend had taken a music player from the display case and stuffed it in her bag earlier in the day. Knowing that employees are subject to search when they leave the store, the friend surreptitiously put the music player into the defendant's bag. The defendant and her friend proceeded to a local tavern and drank shots of tequila. The friend became intoxicated and forgot to retrieve the stolen music player from the defendant's bag. After leaving the tavern, the defendant proceeded home. Upon opening her bag, she was surprised to see the music player. Believing that the music player had accidentally fallen into her bag at the bar, the defendant planned to return it the next day. The following day, however, the defendant changed her mind and decided to keep it. The defendant is guilty of:\n",
      "\n",
      "(A) larceny.\n",
      "(B) possession of stolen property.\n",
      "(C) burglary.\n",
      "(D) receiving stolen property.\n",
      "(E) theft.\n",
      "(F) robbery.\n",
      "(G) neither receiving stolen property nor larceny.\n",
      "(H) receiving stolen property and larceny.\n",
      "(I) conspiracy to commit theft.\n",
      "(J) aiding and abetting theft.\n",
      "ì˜ˆì¸¡ ë‹µë³€: B\n",
      "ì •ë‹µ: G\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: B, ì •ë‹µ: G\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì‚¬ë¡€ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì„ íƒì§€ë¥¼ ê²€í† í•´ ë³´ê² ìŠµë‹ˆë‹¤.  \n",
      "\n",
      "### **í•µì‹¬ ì‚¬ì‹¤ ì •ë¦¬**  \n",
      "1. **ì¹œêµ¬ì˜ í–‰ë™**: ì¹œêµ¬ê°€ ìŒì•… í”Œë ˆì´ì–´ë¥¼ ì ˆì·¨í•œ í›„ í”¼ê³ ì¸ì˜ ê°€ë°©ì— ëª°ë˜ ë„£ìŒ. (ì¹œêµ¬ëŠ” ì ˆì·¨ì˜ ì£¼ì²´)  \n",
      "2. **í”¼ê³ ì¸ì˜ ì´ˆê¸° ì¸ì§€**: í”¼ê³ ì¸ì€ ìŒì•… í”Œë ˆì´ì–´ê°€ ìì‹ ì˜ ê°€ë°©ì— ìˆëŠ” ê²ƒì„ ë°œê²¬í–ˆì§€ë§Œ, ìˆ ì§‘ì—ì„œ ì‹¤ìˆ˜ë¡œ ë“¤ì–´ê°„ ê²ƒìœ¼ë¡œ ì˜¤í•´í•˜ê³  ë‹¤ìŒ ë‚  ë°˜í™˜í•˜ë ¤ í•¨.  \n",
      "   - ì´ ì‹œì ì—ì„œëŠ” **ê³ ì˜(animus furandi)**ê°€ ì—†ì—ˆìŒ.  \n",
      "3. **í”¼ê³ ì¸ì˜ ìµœì¢… í–‰ë™**: ë‹¤ìŒ ë‚  ë§ˆìŒì´ ë°”ë€Œì–´ ìŒì•… í”Œë ˆì´ì–´ë¥¼ ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 42] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION42) Is the recognition of foreign judgments subject to the same rules as those applicable to the incorporation and transformation of treaties?\n",
      "(A) Foreign judgments are enforced on the basis of the doctrine of monism\n",
      "(B) Foreign judgments are enforced on the basis of the doctrine of dualism\n",
      "(C) Foreign judgments are enforced on the basis of the doctrine of incorporation\n",
      "(D) The recognition of foreign judgments is dependent on the existence of appropriate bilateral or multilateral treaties\n",
      "(E) The courts exercise discretion as to the enforcement of foreign judgments on the basis of the rule of reciprocity\n",
      "(F) Foreign judgments are automatically recognized and enforced without any additional process.\n",
      "(G) Foreign judgments are enforced on the basis of the doctrine of transformation\n",
      "ì˜ˆì¸¡ ë‹µë³€: F\n",
      "ì •ë‹µ: D\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: F, ì •ë‹µ: D\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì§ˆë¬¸ì€ \"ì™¸êµ­ íŒê²°ì˜ ìŠ¹ì¸ì€ ì¡°ì•½ì˜ í¸ì… ë° ë³€í˜•ì— ì ìš©ë˜ëŠ” ê·œì¹™ê³¼ ë™ì¼í•œ ê·œì¹™ì— ë”°ë¥´ëŠ”ê°€?\"ì´ë©°, ì„ íƒì§€ ì¤‘ ì •ë‹µì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ëŠ” ì£¼ë¡œ ë²•ì  ì‹¤ì¦ì£¼ì˜, ê·œì¹™ì˜ ì¢…ë¥˜, ë²•ê³¼ ë„ë•ì˜ ê´€ê³„ ë“±ì— ê´€í•œ ì¼ë°˜ ì´ë¡ ì„ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, **ì™¸êµ­ íŒê²°ì˜ ìŠ¹ì¸ ë˜ëŠ” ì§‘í–‰ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê·œì •ì€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**.  \n",
      "\n",
      "### í•µì‹¬ ë¶„ì„:\n",
      "1. **ì»¨í…ìŠ¤íŠ¸ í•œê³„**: ë¬¸ì„œ 1~8ì€ ë²•í•™ ì´ë¡ (ì¼ë°˜ë¡ )ì„ ì„¤ëª…í•˜ì§€ë§Œ, **ì™¸êµ­ íŒê²°ì˜ ìŠ¹ì¸/ì§‘í–‰ ì ˆì°¨ì— ëŒ€í•œ êµ¬ì²´ì  ê·œì •ì´ë‚˜ ë¹„êµë²•ì  ë‚´ìš©ì€ ì—†ìŠµë‹ˆë‹¤**. íŠ¹íˆ ì¡°ì•½ í¸ì…Â·ë³€í˜•ê³¼ì˜...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 45] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION45) One objection to Singerâ€™s theory that he considers is that it:\n",
      "(A) inappropriately makes proximity morally important.\n",
      "(B) fails to consider the cultural differences in moral norms.\n",
      "(C) overlooks the role of government and institutions in addressing suffering.\n",
      "(D) does not do enough to address suffering in other countries besides our own.\n",
      "(E) does not account for the fact that we have stricter obligations to our loved ones than to strangers.\n",
      "(F) does not address the issue of moral obligations to non-human animals.\n",
      "(G) ignores the importance of self-interest in human behavior.\n",
      "(H) assumes that all individuals have the same capacity to alleviate suffering.\n",
      "(I) does not provide a clear definition of what constitutes suffering.\n",
      "(J) requires too drastic a revision to our moral scheme.\n",
      "ì˜ˆì¸¡ ë‹µë³€: E\n",
      "ì •ë‹µ: J\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: E, ì •ë‹µ: J\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•œ ê²°ê³¼, ì‹±ì–´(Singer)ì˜ ì´ë¡ ì— ëŒ€í•œ ë°˜ë¡  ì¤‘ í•˜ë‚˜ë¡œ \"ìš°ë¦¬ê°€ ë‚¯ì„  ì‚¬ëŒë“¤ë³´ë‹¤ ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒë“¤ì—ê²Œ ë” ì—„ê²©í•œ ì˜ë¬´ê°€ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤\"ëŠ” ì£¼ì¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” **ë¬¸ì„œ 2**ì—ì„œ ì–¸ê¸‰ëœ \"agent-centered prerogative\"(í–‰ìœ„ì ì¤‘ì‹¬ íŠ¹ê¶Œ) ê°œë…ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë©ë‹ˆë‹¤. ë¬¸ì„œ 2ì—ì„œëŠ” ë‹¤ë¥¸ ì² í•™ìë“¤(ì˜ˆ: Scheffler, Kagan)ì´ ì‹±ì–´ì˜ ì´ë¡ ì´ ê°œì¸ì˜ ì´ìµì´ë‚˜ ì¹œë°€í•œ ê´€ê³„ë¥¼ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¹„íŒì„ ì œê¸°í•˜ë©°, ì´ì— ëŒ€í•œ ìˆ˜ì •ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹±ì–´ì˜ ì´ë¡ ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 48] (ì¼ë°˜ ë¬¸ì œ)\n",
      "ì§ˆë¬¸: QUESTION48) Which of the following describes a key change in hominids beginning at least as early as Homo erectus that is probably related to increasingly larger brain size?\n",
      "(A) cranial reduction\n",
      "(B) decreased dentition\n",
      "(C) microcephaly\n",
      "(D) supraorbital cortex\n",
      "(E) encephalization\n",
      "(F) opposable thumbs\n",
      "(G) prognathism\n",
      "(H) neoteny\n",
      "(I) bipedalism\n",
      "(J) increased body size\n",
      "ì˜ˆì¸¡ ë‹µë³€: E\n",
      "ì •ë‹µ: H\n",
      "ì›ì¸ ë¶„ì„:\n",
      "  âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: E, ì •ë‹µ: H\n",
      "     â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\n",
      "ìƒì„±ëœ ì‘ë‹µ: ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•œ ê²°ê³¼, í˜¸ëª¨ ì—ë ‰íˆ¬ìŠ¤(Homo erectus) ì‹œê¸°ë¶€í„° ì‹œì‘ëœ ë” í° ë‡Œ í¬ê¸°ì™€ ê´€ë ¨ëœ ì£¼ìš” ë³€í™”ë¥¼ ì„¤ëª…í•˜ëŠ” ì„ íƒì§€ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.  \n",
      "\n",
      "### í•µì‹¬ ë¶„ì„:\n",
      "- **ë¬¸ì„œ 3, 5, 7**ì—ì„œ í˜¸ëª¨ ì—ë ‰íˆ¬ìŠ¤ì˜ ë‡Œ í¬ê¸° ì¦ê°€(encephalization)ê°€ ëª…í™•íˆ ì–¸ê¸‰ë©ë‹ˆë‹¤.  \n",
      "  - ë¬¸ì„œ 3: \"encephalization started with Homo habilis... continued in Homo erectus\"  \n",
      "  - ë¬¸ì„œ 5: \"a process of rapid encephaliza...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… ì •ë‹µ ë¬¸ì œ (43ê°œ):\n",
      "   ë¬¸ì œ ë²ˆí˜¸: 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 40, 41, 43, 44, 46, 47, 49, 50\n",
      "\n",
      "================================================================================\n",
      "ì •ë‹µ ë¬¸ì œ ìƒì„¸:\n",
      "================================================================================\n",
      "\n",
      "[ë¬¸ì œ 1] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION1) ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ í•˜ë ¤ë©´ í•™ê¸° ê°œì‹œì¼ë¡œë¶€í„° ë©°ì¹  ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì•¼í•˜ë‚˜ìš”?\n",
      "(A) 30ì¼\n",
      "(B) 45ì¼ \n",
      "(C) 60ì¼\n",
      "(D) 90ì¼\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 2] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: A\n",
      "ì§ˆë¬¸: QUESTION2) 'ì¬ì…í•™ì€ aíšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ì œ 28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ ë‚ ë¶€í„° bë…„ì´ ê²½ê³¼í•œ í›„ ì¬ì…í•™ í•  ìˆ˜ ìˆë‹¤.' aì™€ bê°€ ìƒìˆ˜ì¼ ë•Œ a+bì˜ ê°’ì„ êµ¬í•˜ë©´?\n",
      "(A) 2\n",
      "(B) 3\n",
      "(C) 4\n",
      "(D) A,B,C ì¤‘ ë‹µ ì—†ìŒ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 3] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION3) í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ ëª‡í•™ì  ì´ìƒ ì·¨ë“í•œ ë•Œì— ë¶€ì „ê³µì„ ì´ìˆ˜í•œê²ƒìœ¼ë¡œ ì¸ì •ë°›ì„ ìˆ˜ ìˆëŠ”ê°€?\n",
      "(A) 15í•™ì \n",
      "(B) 18í•™ì \n",
      "(C) 21í•™ì \n",
      "(D) 25í•™ì \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 4] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION4) ë‹¤ìŒ ë³´ê¸°ì˜ í•™ìƒë“¤ ì¤‘ ì œì ì„ ë‹¹í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒì„ ê³ ë¥´ë©´?\n",
      "(A) íŒœ : ì§•ê³„ì— ì˜í•´ í‡´í•™ì²˜ë¶„ì„ ë°›ì•˜ìŒ \n",
      "(B) ì—˜ëª¨ : ì„±ì ì´ í‰ì í‰ê·  1.2 ë¡œ í•™ì‚¬ê²½ê³ ë¥¼ ì—°ì† 3íšŒ ë°›ì•˜ìŒ\n",
      "(C) ë¼ë§ˆ : ìˆ˜ì—…ë£Œ ê¸°íƒ€ ë‚©ì…ê¸ˆì„ ì†Œì • ê¸°ì¼ ë‚´ ë‚©ì…í•˜ì§€ ëª»í•˜ì˜€ìŒ\n",
      "(...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 5] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION5) 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ ëª‡ ëª…ì¸ê°€? \n",
      "(A) 90ëª… \n",
      "(B) 100ëª… \n",
      "(C) 110ëª… \n",
      "(D) 120ëª…\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 6] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION6) 1980í•™ë…„ë„ ì´ì „ ì…í•™ìƒì— ëŒ€í•˜ì—¬ ì ìš©í•˜ëŠ” ë“±ê¸‰ì— ë”°ë¥¸ ì„±ì ì ìœ¼ë¡œ ì˜ëª» ì—°ê²°ëœ ê²ƒì€ ë¬´ì—‡ì¸ê°€? \n",
      "(A) ë“±ê¸‰: A+, ì„±ì ì : 4 \n",
      "(B) ë“±ê¸‰: A-: ì„±ì ì : 3.5 \n",
      "(C) ë“±ê¸‰: B+, ì„±ì ì : 3 \n",
      "(D) ë“±ê¸‰: C, ì„±ì ì : 2\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 7] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION7) ì‚¬íšŒì²´ìœ¡í•™ê³¼ ì†Œì† í•™ìƒì—ê²Œ ìˆ˜ì—¬í•˜ëŠ” í•™ìœ„ëŠ” ë¬´ì—‡ì¸ê°€? \n",
      "(A) ê³µí•™ì‚¬ \n",
      "(B) ë¬¸í•™ì‚¬ \n",
      "(C) ì‚¬íšŒí•™ì‚¬ \n",
      "(D) ì´í•™ì‚¬ \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 9] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION9) ì´í™”ì—¬ìëŒ€í•™êµì˜ ì„¤ë¦½ ì •ì‹ ì€ ë¬´ì—‡ì¸ê°€ìš”? â€¨(A) ê³µì‚°ì£¼ì˜ ì´ë… â€¨(B) ë¶ˆêµ ì •ì‹  â€¨(C) ê¸°ë…êµ ì •ì‹  â€¨(D) ììœ ì£¼ì˜ ì‚¬ìƒ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 10] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION10) ì´í™”ì—¬ìëŒ€í•™êµì˜ ìœ„ì¹˜ëŠ” ì–´ë””ì¸ê°€ìš”? â€¨(A) ê°•ë‚¨êµ¬ â€¨(B) ì„œëŒ€ë¬¸êµ¬ â€¨(C) ì¢…ë¡œêµ¬ â€¨(D) ì†¡íŒŒêµ¬\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 11] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION11) í•™ì  ê¸°ì¤€ì— ë”°ë¥´ë©´ 1í•™ì ë‹¹ ìˆ˜ì—… ì‹œê°„ì€ ëª‡ ì‹œê°„ ì´ìƒì´ì–´ì•¼ í•˜ë‚˜ìš”? â€¨(A) 10ì‹œê°„ â€¨(B) 15ì‹œê°„ â€¨(C) 20ì‹œê°„ â€¨(D) 25ì‹œê°„\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 12] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION12) ì „ê³µê³¼ ê´€ë ¨ëœ ì¡°í•­ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?\n",
      "(A) 2 ì´ìƒì˜ í•™ë¶€ê°€ ì—°ê³„í•˜ì—¬ ì œê³µí•˜ëŠ” ì „ê³µì„ ë³µìˆ˜ì „ê³µì´ë¼ í•œë‹¤.\n",
      "(B) í•™ìƒì´ ì†Œì†í•œ ì œ1ì „ê³µê³¼ì •ì— ê¸°ë°˜í•˜ì—¬ íƒ€í•™ë¬¸ê³¼ì˜ ìœµë³µí•© êµìœ¡ê³¼ì •ì„ ì œê³µí•˜ëŠ” ì „ê³µì„ ìœµí•©ì „ê³µì´ë¼ í•œë‹¤.\n",
      "(C) 2 ì´ìƒì˜ ì „ê³µì„ ìê¸°ì„¤...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 13] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION13) ë“±ë¡ê¸ˆì— ê´€í•œ ì¡°í•­ìœ¼ë¡œ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²ƒì€?\n",
      "(A) ì´ì¥ì€ ë“±ë¡ê¸ˆì‹¬ì˜ìœ„ì›íšŒì˜ êµ¬ì„± ë° ìš´ì˜ì— ê´€í•œ ì‚¬í•­ì„ ì •í•œë‹¤.\n",
      "(B) ë“±ë¡ê¸°ê°„ ë§Œë£Œ ì „ì— íœ´í•™ì›ì„ ì œì¶œí•˜ì—¬ í—ˆê°€ë¥¼ ì–»ì€ ìëŠ” ë‹¹í•´ íœ´í•™ê¸°ê°„ì˜ ë“±ë¡ê¸ˆì´ ë©´ì œëœë‹¤.\n",
      "(C) ë“±ë¡ê¸ˆì´ ê³¼ì˜¤ë‚©ëœ ê²½ìš°ì—ëŠ” ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 14] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION14) ì „ê³µê³¼ëª©ì€ ì–´ë–»ê²Œ êµ¬ë¶„ë˜ëŠ”ê°€?\n",
      "(A) ì „ê³µê¸°ì´ˆê³¼ëª©ê³¼ ì „ê³µê³¼ëª©\n",
      "(B) ì´ì¥ì´ ë”°ë¡œ ì •í•œë‹¤\n",
      "(C) í•„ìˆ˜ê³¼ëª©ê³¼ ì„ íƒê³¼ëª©\n",
      "(D) êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 15] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: A\n",
      "ì§ˆë¬¸: QUESTION15) í•™ì¹™ê°œì •ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?\n",
      "(A) í•™ì¹™ ê°œì •ê³¼ ëŒ€í•™í‰ì˜ì›íšŒëŠ” ë¬´ê´€í•˜ë‹¤.\n",
      "(B) í•™ì¹™ ê°œì •ì´ ì´ë£¨ì–´ì§€ê¸° ì „ì—, ì‚¬ì „ê³µê³ ë¥¼ ê±°ì³ì•¼ í•œë‹¤.\n",
      "(C) í•™ì¹™ì˜ ê°œì •ì€ ì´ì¥ì´ í–‰í•œë‹¤.\n",
      "(D) í•™ì¹™ê°œì •ì€ êµë¬´íšŒì˜ì˜ ì‹¬ì˜ë¥¼ ê±°ì¹œë‹¤.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 16] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION16)ê³„ì ˆí•™ê¸°ì— ì·¨ë“í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í•™ì ì€ ëª‡ ì ì…ë‹ˆê¹Œ?\n",
      "(A) 3í•™ì \n",
      "(B) 6í•™ì \n",
      "(C) 9í•™ì \n",
      "(D) 12í•™ì \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 17] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION17)ì¡¸ì—…í•˜ê¸° ìœ„í•´ ì´í‰ê·  ì„±ì ì´ ì¶©ì¡±í•´ì•¼ í•˜ëŠ” ìµœì†Œ ê¸°ì¤€ì€?\n",
      "(A) 1.60\n",
      "(B) 1.70\n",
      "(C) 2.00\n",
      "(D) 2.50\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 18] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION18)ìˆ˜ì—…ì˜ ê²°ì„ì´ ì¶œì„ìœ¼ë¡œ ì¸ì •ë  ìˆ˜ ìˆëŠ” ì‚¬ìœ ê°€ ì•„ë‹Œ ê²ƒì€?\n",
      "(A) ì¤‘ëŒ€í•œ ì§ˆë³‘\n",
      "(B) ì§ê³„ì¡´ë¹„ì†ì˜ ì‚¬ë§\n",
      "(C) êµ­ì œ ëŒ€íšŒ ì°¸ê°€\n",
      "(D) ê°œì¸ ì‚¬ì •\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 19] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION19)ì¡°ê¸° ì¡¸ì—…ì„ ìœ„í•œ ì´ í‰ê·  ì„±ì  ê¸°ì¤€ì€?\n",
      "(A) 2.50\n",
      "(B) 3.00\n",
      "(C) 3.50\n",
      "(D) 3.75\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 20] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION20) ì¬í•™ ì—°í•œ ì´ˆê³¼ë¡œ ì œì ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ”?\n",
      "(A) í•™ì‚¬ í¸ì…\n",
      "(B) ë³µìˆ˜ ì „ê³µ ì¤‘\n",
      "(C) ì¬ì…í•™ í›„ 1ë…„ ì´ë‚´\n",
      "(D) íœ´í•™ ì¤‘\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 21] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION21) ì¡¸ì—…ì´ ì·¨ì†Œë  ìˆ˜ ìˆëŠ” ìƒí™©ì€?\n",
      "(A) ë…¼ë¬¸ ë¯¸ì œì¶œ\n",
      "(B) ë¶€ì •í•œ ë°©ë²•ìœ¼ë¡œ ì¡¸ì—… ì¸ì •\n",
      "(C) ì„±ì  ì €ì¡°\n",
      "(D) ì¡¸ì—… ìš”ê±´ ì¶©ì¡± ëª»í•¨\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 22] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: A\n",
      "ì§ˆë¬¸: QUESTION22) êµì–‘ê³¼ëª©ì„ ì œì™¸í•œ ì „ê³µê³¼ëª©ì˜ ë¶„ë¥˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?\n",
      "(A) í•„ìˆ˜/ì„ íƒ\n",
      "(B) ì •ê·œ/íŠ¹ë³„\n",
      "(C) ì¼ë°˜/ê³ ê¸‰\n",
      "(D) í•™ì  ê¸°ì¤€\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 23] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION23) ë‹¤ìŒì¤‘ ì˜³ê²Œ ì§ì§€ì–´ì§„ í•™ìœ„ë¥¼ ê³ ë¥´ì„¸ìš”.\n",
      "(A) ë¶í•œ í•™ê³¼ - ì´í•™ì‚¬\n",
      "(B) ê¸°ì—…ê°€ì •ì‹  - ë²¤ì²˜í•™ì‚¬\n",
      "(C) ë¯¸ìˆ í•™ì‚¬ - í•œêµ­ìŒì•…\n",
      "(D) ë¬¸í•™ì‚¬ - ì†Œë¹„ìí•™\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 24] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION24) ë‹¤ìŒ ì¤‘ ì´í™”ì—¬ìëŒ€í•™êµ í•™ì¹™ì—ì„œ í•™ì ê³¼ ê´€ë ¨í•´ ì˜³ì€ ê²ƒì„ ê³ ë¥´ì„¸ìš”\n",
      "(A) êµê³¼ê³¼ì •ì´ìˆ˜ ë‹¨ìœ„ëŠ” í•™ê¸°ë¡œ í•œë‹¤.\n",
      "(B) í•œ í•™ê¸°ë™ì•ˆ 10ì‹œê°„ ì´ìƒ ìˆ˜ì—…ì„ ë“¤ìœ¼ë©´ 1í•™ì ì„ ì¤€ë‹¤.\n",
      "(C) í•™ì ì·¨ë“íŠ¹ë³„ì‹œí—˜ì˜ ë°©ë²•ì€ ì´ì¥ê³¼ ìœ„ì›íšŒê°€ í•¨ê»˜ ì •í•œë‹¤.\n",
      "(D) ì œ35...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 25] (ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION25) 2019í•™ë…„ë„ ì…í•™ ì •ì›ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ ì˜³ì€ ê²ƒì„ ê³ ë¥´ì‹œì˜¤.\n",
      "(A) íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ ì…í•™ ì •ì›ê³¼ ë¬´ìš©ê³¼ì˜ ì…í•™ ì •ì›ì˜ í•©ì€ 149ëª…ì´ë‹¤.\n",
      "(B) ê±´ë°˜ì•…ê¸°ê³¼ì˜ ì…í•™ì •ì›ì€ 164ëª…ì´ë‹¤.\n",
      "(C) ìˆ˜í•™êµìœ¡ê³¼ì™€ êµ­ì–´ êµìœ¡ê³¼ì˜ ì…í•™ ì •ì›ì€ ê°ê° 37ëª…ê³¼...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 26] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: D\n",
      "ì§ˆë¬¸: QUESTION26) QUESTION 6) A psychologist is asked to see a 10-year-old child for counseling in a school setting. Which of the following statements descr...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 27] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: A\n",
      "ì§ˆë¬¸: QUESTION27) A man is at home in his apartment, alone, late at night. Someone repeatedly buzzes his apartment ringer outside the building, but he doesn...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 28] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: E\n",
      "ì§ˆë¬¸: QUESTION28) What do Homo sapiens and Australopithecus afarensis have in common?\n",
      "(A) \"a shared ability to use complex tools\"\n",
      "(B) \"a similar diet\"\n",
      "(C) \"...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 29] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: I\n",
      "ì§ˆë¬¸: QUESTION29)This question refers to the following information.\n",
      " I walk alongside the column, ask what's going on.\n",
      " A soldier says simply: \"They call up...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 30] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: E\n",
      "ì§ˆë¬¸: QUESTION30)Homo erectus differed from Homo habilis in which way?\n",
      " (A) Erectus was not bipedal.\n",
      " (B) Erectus was not capable of using tools.\n",
      " (C) Erect...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 31] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION31)During the manic phase of a bipolar disorder, individuals are most likely to experience\n",
      " (A) extreme fatigue\n",
      " (B) high self-esteem\n",
      " (C) mem...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 33] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: I\n",
      "ì§ˆë¬¸: QUESTION33) You receive a phone call from Hermann H., age 28, who says he is â€œtotally miserableâ€ because of the recent breakup with his girlfriend and...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 34] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: E\n",
      "ì§ˆë¬¸: QUESTION34) During the second stage of Kohlbergâ€™s preconventional level of moral development, children obey rules because:\n",
      "(A) they are taught that ru...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 35] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION35)  In satisfying Kant's Humanity formulation of the categorical imperative, we are obligated to adopt two very general goals: the goal of pr...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 36] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: J\n",
      "ì§ˆë¬¸: QUESTION36) Aristotle says  that what makes things be what they are--their essence--does not exist apart from individ-uals that exist in the world.  S...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 37] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION37) The ________ School of jurisprudence believes that the law is an aggregate of social traditions and customs that have developed over the c...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 40] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: I\n",
      "ì§ˆë¬¸: QUESTION40)____________ refers to a strategic process involving stakeholder assessment to create long-term relationships with customers, while maintai...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 41] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: E\n",
      "ì§ˆë¬¸: QUESTION41)This question refers to the following information. Seizing me, he led me down to the House of Darknessâ€¦ To the house where those who enter ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 43] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: C\n",
      "ì§ˆë¬¸: QUESTION43) Some contemporary intelligence researchers like Howard Gardner and Robert Sternberg complain that schools focus too much on\n",
      "(A) nonessenti...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 44] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: F\n",
      "ì§ˆë¬¸: QUESTION44) BobGafneyand Susan Medina invested $40,000 and $50,000 respectively in a luncheonette. Since Mr.Gafneyis the manager of the luncheonette, ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 46] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: I\n",
      "ì§ˆë¬¸: QUESTION46) In 1797, John Frere made a discovery that he described as:\n",
      "(A) a new type of metal alloy.\n",
      "(B) the earliest written documents.\n",
      "(C) the rema...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 47] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: G\n",
      "ì§ˆë¬¸: QUESTION47) Pick the correct description of the following term: Utilitarianism isâ€¦\n",
      "(A) A theory which states that an action is morally right if it pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 49] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: G\n",
      "ì§ˆë¬¸: QUESTION49) Delia was accepted to both Harvard University and Yale University and is having difficulty choosing which school to attend. With which of ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[ë¬¸ì œ 50] (ì¼ë°˜ ë¬¸ì œ) - ì •ë‹µ: B\n",
      "ì§ˆë¬¸: QUESTION50) Which is the least accurate description of legal positivism?\n",
      "(A) It perceives law as arbitrary and without any logical structure.\n",
      "(B) It r...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ’¾ ì‘ë‹µì„ c:\\Users\\janen\\Documents\\25-2 ê°•ì˜\\ìì—°ì–´ì²˜ë¦¬\\í”„ë¡œì íŠ¸\\ewha\\rag_responses.csvì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š ìš”ì•½:\n",
      "   - ì´ ì§ˆë¬¸ ìˆ˜: 50\n",
      "   - í‰ê·  ì°¸ê³  ë¬¸ì„œ ìˆ˜: 9.6\n",
      "   - ìµœì¢… ì ìˆ˜: 43/50 (86.00%)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 5. testset.csvì—ì„œ ì§ˆë¬¸ ë¶ˆëŸ¬ì™€ì„œ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± ë° ì±„ì \n",
    "# ==================================================================\n",
    "\n",
    "# testset.csv íŒŒì¼ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬)\n",
    "TESTSET_PATH = CURRENT_DIR.parent / \"testset.csv\"\n",
    "\n",
    "# testset.csv íŒŒì¼ ì½ê¸° í•¨ìˆ˜\n",
    "def read_testset(file_path: Path):\n",
    "    \"\"\"testset.csv íŒŒì¼ì—ì„œ ì§ˆë¬¸ê³¼ ì •ë‹µ ì½ê¸°\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        prompts = data[\"prompts\"].tolist() if \"prompts\" in data.columns else []\n",
    "        answers = data[\"answers\"].tolist() if \"answers\" in data.columns else None\n",
    "        return prompts, answers\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "        return [], None\n",
    "\n",
    "# ë‹µë³€ ì¶”ì¶œ í•¨ìˆ˜ (ê°œì„  ë²„ì „ - ë²”ìš©ì  ì •í™•ë„ í–¥ìƒ)\n",
    "def extract_answer(response: str, question: Optional[str] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ (ê°œì„  ë²„ì „ - ë²”ìš©ì )\n",
    "    - ì—¬ëŸ¬ ì„ íƒì§€ê°€ ìˆì„ ê²½ìš° ê°€ì¥ í™•ì‹¤í•œ ê²ƒ ì„ íƒ\n",
    "    - \"ì •ë‹µ:\", \"Answer:\", \"ë‹µ:\" ë“±ì˜ í‚¤ì›Œë“œ ìš°ì„ \n",
    "    - ì‘ë‹µì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ\n",
    "    - ìœ íš¨í•œ ì„ íƒì§€ë§Œ ì¶”ì¶œ (A-Zë§Œ í—ˆìš©)\n",
    "    - \"ì •ë‹µ: **C**\" ê°™ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ë„ ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    if not response or not response.strip():\n",
    "        return None\n",
    "    \n",
    "    # ìœ íš¨í•œ ì„ íƒì§€ (ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì„ íƒì§€)\n",
    "    valid_choices = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    \n",
    "    # ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë§ˆì§€ë§‰ ë¶€ë¶„ ìš°ì„  ê²€ìƒ‰\n",
    "    sentences = response.split('.')\n",
    "    last_sentences = sentences[-5:] if len(sentences) > 5 else sentences  # ë§ˆì§€ë§‰ 5ë¬¸ì¥\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì ì¸ ì •ë‹µ í‘œì‹œ (ë§ˆì§€ë§‰ ë¬¸ì¥ì—ì„œ ìš°ì„  ê²€ìƒ‰)\n",
    "    # \"ì •ë‹µ: **C**\", \"ì •ë‹µ: (C)\", \"ì •ë‹µì€ (C)ì…ë‹ˆë‹¤\" ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›\n",
    "    patterns = [\n",
    "        r\"ì •ë‹µ[ì€ëŠ”]?\\s*[:\\s]*\\*{0,2}\\s*\\(([A-Z])\\)\",  # ì •ë‹µ: **C** ë˜ëŠ” ì •ë‹µ: (C)\n",
    "        r\"ì •ë‹µ[ì€ëŠ”]?\\s*[:\\s]*\\*{1,2}([A-Z])\\*{0,2}\",  # ì •ë‹µ: **C** ë˜ëŠ” ì •ë‹µ: C\n",
    "        r\"ì •ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"Answer[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"\\[ANSWER\\]:\\s*\\(([A-Z])\\)\",\n",
    "        r\"The answer is\\s*\\(([A-Z])\\)\",\n",
    "        r\"Correct answer[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"ìµœì¢…[:\\s]*ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"ì •ë‹µ[ì€ëŠ”]?\\s*[:\\s]*([A-Z])\\b\",  # ì •ë‹µ: C (ê´„í˜¸ ì—†ìŒ)\n",
    "    ]\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë¬¸ì¥ë¶€í„° ì—­ìˆœìœ¼ë¡œ ê²€ìƒ‰\n",
    "    for sentence in reversed(last_sentences):\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                answer = match.group(1).upper()\n",
    "                if answer in valid_choices:\n",
    "                    return answer\n",
    "    \n",
    "    # ì „ì²´ ì‘ë‹µì—ì„œë„ ê²€ìƒ‰\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            answer = match.group(1).upper()\n",
    "            if answer in valid_choices:\n",
    "                return answer\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 2: \"ì •ë‹µì€ (X)ì…ë‹ˆë‹¤\" í˜•ì‹\n",
    "    pattern_final = r\"ì •ë‹µì€\\s*\\(([A-Z])\\)\\s*ì…ë‹ˆë‹¤\"\n",
    "    match = re.search(pattern_final, response, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).upper()\n",
    "        if answer in valid_choices:\n",
    "            return answer\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 3: (X) í˜•ì‹ (ë§ˆì§€ë§‰ ë°œìƒ - ë³´í†µ ìµœì¢… ë‹µë³€)\n",
    "    # ë§ˆì§€ë§‰ ë¬¸ì¥ì—ì„œ ìš°ì„  ê²€ìƒ‰\n",
    "    for sentence in reversed(last_sentences):\n",
    "        pattern2 = r\"\\(([A-Z])\\)\"\n",
    "        matches = list(re.finditer(pattern2, sentence))\n",
    "        if matches:\n",
    "            answer = matches[-1].group(1).upper()\n",
    "            if answer in valid_choices:\n",
    "                return answer\n",
    "    \n",
    "    # ì „ì²´ ì‘ë‹µì—ì„œ ê²€ìƒ‰\n",
    "    pattern2 = r\"\\(([A-Z])\\)\"\n",
    "    matches = list(re.finditer(pattern2, response))\n",
    "    if matches:\n",
    "        # ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ (ë³´í†µ ìµœì¢… ë‹µë³€)\n",
    "        answer = matches[-1].group(1).upper()\n",
    "        if answer in valid_choices:\n",
    "            return answer\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 4: ë‹¨ë… ë¬¸ì (ë§ˆì§€ë§‰ ë°œìƒ, ë¬¸ì¥ ë ê·¼ì²˜)\n",
    "    # ë§ˆì§€ë§‰ ë¬¸ì¥ì—ì„œ ìš°ì„  ê²€ìƒ‰\n",
    "    for sentence in reversed(last_sentences):\n",
    "        # ë¬¸ì¥ ëì— ìˆëŠ” ë‹¨ë… ë¬¸ì ìš°ì„  (ì˜ˆ: \"ì •ë‹µ: **C**\")\n",
    "        pattern3 = r\"([A-Z])\\s*[\\.\\*]*\\s*$\"  # ë¬¸ì¥ ëì˜ ë‹¨ë… ë¬¸ì\n",
    "        match = re.search(pattern3, sentence.strip())\n",
    "        if match:\n",
    "            answer = match.group(1).upper()\n",
    "            if answer in valid_choices:\n",
    "                return answer\n",
    "        \n",
    "        # ì¼ë°˜ ë‹¨ë… ë¬¸ì\n",
    "        pattern3 = r\"\\b([A-Z])\\b\"\n",
    "        matches = list(re.finditer(pattern3, sentence))\n",
    "        if matches:\n",
    "            for match in reversed(matches):\n",
    "                answer = match.group(1).upper()\n",
    "                if answer in valid_choices:\n",
    "                    return answer\n",
    "    \n",
    "    # ì „ì²´ ì‘ë‹µì—ì„œ ê²€ìƒ‰\n",
    "    pattern3 = r\"\\b([A-Z])\\b\"\n",
    "    matches = list(re.finditer(pattern3, response))\n",
    "    if matches:\n",
    "        # ë§ˆì§€ë§‰ ë‹¨ë… ë¬¸ì (ë³´í†µ ìµœì¢… ë‹µë³€)\n",
    "        for match in reversed(matches):\n",
    "            answer = match.group(1).upper()\n",
    "            if answer in valid_choices:\n",
    "                return answer\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ì •ë‹µ ì¶”ì¶œ í•¨ìˆ˜ (ì •ë‹µ ë¬¸ìì—´ì—ì„œ ì„ íƒì§€ ì¶”ì¶œ)\n",
    "def extract_gold_answer(gold_str: str) -> Optional[str]:\n",
    "    \"\"\"ì •ë‹µ ë¬¸ìì—´ì—ì„œ ì„ íƒì§€ ë¬¸ì ì¶”ì¶œ\"\"\"\n",
    "    import re\n",
    "    if pd.isna(gold_str) or not gold_str:\n",
    "        return None\n",
    "    \n",
    "    gold_str = str(gold_str).strip()\n",
    "    # (A), (B) í˜•ì‹ ì¶”ì¶œ\n",
    "    match = re.search(r\"\\(([A-Z])\\)\", gold_str)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    # ë‹¨ë… ë¬¸ì ì¶”ì¶œ\n",
    "    match = re.search(r\"\\b([A-Z])\\b\", gold_str)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# testset.csv íŒŒì¼ ë¡œë“œ\n",
    "print(f\"ğŸ“‚ testset.csv íŒŒì¼ ê²½ë¡œ: {TESTSET_PATH}\")\n",
    "prompts, gold_answers = read_testset(TESTSET_PATH)\n",
    "\n",
    "if not prompts:\n",
    "    print(f\"âŒ {TESTSET_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ ë¡œë“œ ì™„ë£Œ\")\n",
    "    if gold_answers:\n",
    "        print(f\"âœ… ì •ë‹µ {len(gold_answers)}ê°œ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ë²”ìš©ì )\n",
    "def extract_keywords_from_question(question: str) -> List[str]:\n",
    "    \"\"\"ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ë²”ìš©ì )\"\"\"\n",
    "    import re\n",
    "    keywords = []\n",
    "    \n",
    "    # ì§ˆë¬¸ì—ì„œ ì£¼ìš” ëª…ì‚¬/í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    # ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ\n",
    "    english_words = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', question)\n",
    "    keywords.extend([w.lower() for w in english_words if len(w) > 3])\n",
    "    \n",
    "    # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)\n",
    "    korean_words = re.findall(r'[ê°€-í£]{2,}', question)\n",
    "    keywords.extend(korean_words)\n",
    "    \n",
    "    # ì„ íƒì§€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    choice_pattern = r'\\([A-Z]\\)\\s*([^\\(\\)]+?)(?=\\s*\\([A-Z]\\)|$)'\n",
    "    choices = re.findall(choice_pattern, question)\n",
    "    for choice in choices:\n",
    "        # ì„ íƒì§€ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        words = re.findall(r'\\b[A-Z][a-z]+\\b|[ê°€-í£]{2,}', choice)\n",
    "        keywords.extend([w.lower() if w.isalpha() else w for w in words if len(w) > 2])\n",
    "    \n",
    "    return list(set(keywords))[:10]  # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œ\n",
    "\n",
    "# ì¬ê²€ìƒ‰ í•¨ìˆ˜ (ë²”ìš©ì  - ì„ íƒì§€ ê¸°ë°˜ ì¬ê²€ìƒ‰ í¬í•¨)\n",
    "def enhanced_retrieval(question: str, retriever, is_ewha_related: bool, max_docs: int = 10):\n",
    "    \"\"\"í–¥ìƒëœ ê²€ìƒ‰ (ì¬ê²€ìƒ‰ ë¡œì§ í¬í•¨ - ë²”ìš©ì )\"\"\"\n",
    "    # ì²« ë²ˆì§¸ ê²€ìƒ‰\n",
    "    source_docs = retriever.invoke(question)\n",
    "    existing_sources = {doc.metadata.get('source', '') + str(doc.metadata.get('article_index', '')) for doc in source_docs}\n",
    "    \n",
    "    # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ì¬ê²€ìƒ‰\n",
    "    if len(source_docs) < 5 or (is_ewha_related and len(source_docs) < 3):\n",
    "        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ê²€ìƒ‰\n",
    "        keywords = extract_keywords_from_question(question)\n",
    "        if keywords:\n",
    "            # ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰\n",
    "            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©\n",
    "                additional_docs = retriever.invoke(keyword)\n",
    "                for doc in additional_docs:\n",
    "                    doc_id = doc.metadata.get('source', '') + str(doc.metadata.get('article_index', ''))\n",
    "                    if doc_id not in existing_sources:\n",
    "                        source_docs.append(doc)\n",
    "                        existing_sources.add(doc_id)\n",
    "                if len(source_docs) >= max_docs:\n",
    "                    break\n",
    "        \n",
    "        # ì„ íƒì§€ ê¸°ë°˜ ì¬ê²€ìƒ‰ (íŠ¹íˆ í•™ì¹™ ê´€ë ¨ ë¬¸ì œ)\n",
    "        if is_ewha_related:\n",
    "            import re\n",
    "            # íŠ¹ì • í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰ (ë³µìˆ˜ì „ê³µ, ì‹ ì²­ ìê²© ë“±)\n",
    "            special_keywords = []\n",
    "            if \"ë³µìˆ˜ì „ê³µ\" in question or \"ì‹ ì²­ ìê²©\" in question:\n",
    "                special_keywords = [\"ë³µìˆ˜ì „ê³µ\", \"ì‹ ì²­\", \"ìê²©\", \"ì¡°ê±´\"]\n",
    "            elif \"ì¡¸ì—…\" in question:\n",
    "                special_keywords = [\"ì¡¸ì—…\", \"ìš”ê±´\", \"ê¸°ì¤€\"]\n",
    "            elif \"ì¬ì…í•™\" in question:\n",
    "                special_keywords = [\"ì¬ì…í•™\", \"íšŸìˆ˜\", \"ì œí•œ\"]\n",
    "            \n",
    "            for keyword in special_keywords:\n",
    "                additional_docs = retriever.invoke(keyword)\n",
    "                for doc in additional_docs:\n",
    "                    doc_id = doc.metadata.get('source', '') + str(doc.metadata.get('article_index', ''))\n",
    "                    if doc_id not in existing_sources:\n",
    "                        source_docs.append(doc)\n",
    "                        existing_sources.add(doc_id)\n",
    "                if len(source_docs) >= max_docs:\n",
    "                    break\n",
    "            \n",
    "            # ì„ íƒì§€ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "            choice_pattern = r'\\([A-Z]\\)\\s*([^\\(\\)]+?)(?=\\s*\\([A-Z]\\)|$)'\n",
    "            choices = re.findall(choice_pattern, question)\n",
    "            for choice in choices[:2]:  # ìƒìœ„ 2ê°œ ì„ íƒì§€ë§Œ\n",
    "                # ì„ íƒì§€ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "                choice_keywords = re.findall(r'[ê°€-í£]{2,}|\\b[A-Z][a-z]+\\b', choice)\n",
    "                for keyword in choice_keywords[:2]:  # ê° ì„ íƒì§€ì—ì„œ 2ê°œ í‚¤ì›Œë“œ\n",
    "                    if len(keyword) > 2:\n",
    "                        additional_docs = retriever.invoke(keyword)\n",
    "                        for doc in additional_docs:\n",
    "                            doc_id = doc.metadata.get('source', '') + str(doc.metadata.get('article_index', ''))\n",
    "                            if doc_id not in existing_sources:\n",
    "                                source_docs.append(doc)\n",
    "                                existing_sources.add(doc_id)\n",
    "                        if len(source_docs) >= max_docs:\n",
    "                            break\n",
    "    \n",
    "    return source_docs[:max_docs]\n",
    "\n",
    "# RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ì§ˆë¬¸ ë¶„ë¥˜ í¬í•¨, ê°œì„  ë²„ì „ - ë²”ìš©ì )\n",
    "def generate_rag_response(question: str, retriever, rag_chain, general_chain, classification_chain):\n",
    "    \"\"\"ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (ìë™ ë¶„ë¥˜ í›„ RAG ë˜ëŠ” ì¼ë°˜ LLM ì‚¬ìš©, ê°œì„  ë²„ì „ - ë²”ìš©ì )\"\"\"\n",
    "    # ì§ˆë¬¸ ë¶„ë¥˜\n",
    "    is_ewha_related = classify_question(question)\n",
    "    \n",
    "    # ìˆ˜í•™ ê³„ì‚° ë¬¸ì œ ê°ì§€\n",
    "    is_math_problem = any(keyword in question.lower() for keyword in [\n",
    "        'invested', 'income', 'divided', 'calculate', 'how much', 'ì–¼ë§ˆ', 'ê³„ì‚°', 'ë‚˜ëˆ”', 'íˆ¬ì'\n",
    "    ])\n",
    "    \n",
    "    if is_ewha_related:\n",
    "        # ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œ: RAG ì‚¬ìš©\n",
    "        # í–¥ìƒëœ ê²€ìƒ‰ (ì¬ê²€ìƒ‰ ë¡œì§ í¬í•¨)\n",
    "        source_docs = enhanced_retrieval(question, retriever, is_ewha_related=True, max_docs=10)\n",
    "        \n",
    "        # ë¬¸ì„œ ê´€ë ¨ì„± í•„í„°ë§ (ìœ ì‚¬ë„ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°)\n",
    "        filtered_docs = []\n",
    "        for doc in source_docs:\n",
    "            # ë©”íƒ€ë°ì´í„°ì— scoreê°€ ìˆìœ¼ë©´ í™•ì¸\n",
    "            if hasattr(doc, 'metadata') and 'score' in doc.metadata:\n",
    "                if doc.metadata['score'] >= 0.2:  # ì„ê³„ê°’ ë‚®ì¶¤ (ë” ë§ì€ ë¬¸ì„œ í¬í•¨)\n",
    "                    filtered_docs.append(doc)\n",
    "            else:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        # í•„í„°ë§ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©\n",
    "        if not filtered_docs:\n",
    "            filtered_docs = source_docs[:8]  # ìµœëŒ€ 8ê°œ ì‚¬ìš©\n",
    "        \n",
    "        # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë¬¸ì„œ ë²ˆí˜¸ ì¶”ê°€ë¡œ ì°¸ì¡° ìš©ì´)\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(filtered_docs[:8], 1):  # ìµœëŒ€ 8ê°œ ì‚¬ìš©\n",
    "            doc_text = doc.page_content\n",
    "            source_info = doc.metadata.get('source', 'Unknown')\n",
    "            context_parts.append(f\"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source_info})\\n{doc_text}\")\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "        \n",
    "        response = rag_chain.invoke({\"question\": question, \"context\": context_text})\n",
    "        \n",
    "        return {\n",
    "            \"response\": response.content,\n",
    "            \"sources\": [doc.metadata for doc in filtered_docs],\n",
    "            \"num_sources\": len(filtered_docs),\n",
    "            \"is_ewha_related\": True\n",
    "        }\n",
    "    else:\n",
    "        # ì¼ë°˜ ë¬¸ì œ: Wikipedia ë²¡í„°í™”ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "        # í–¥ìƒëœ ê²€ìƒ‰ (ì¬ê²€ìƒ‰ ë¡œì§ í¬í•¨)\n",
    "        source_docs = enhanced_retrieval(question, retriever, is_ewha_related=False, max_docs=10)\n",
    "        \n",
    "        # Wikipedia ë¬¸ì„œ í•„í„°ë§ (priorityê°€ ìˆëŠ” ê²½ìš° ìš°ì„ )\n",
    "        filtered_wiki_docs = []\n",
    "        ewha_docs = []\n",
    "        \n",
    "        for doc in source_docs:\n",
    "            doc_type = doc.metadata.get('type', '')\n",
    "            if doc_type == 'wikipedia':\n",
    "                filtered_wiki_docs.append(doc)\n",
    "            elif doc_type in ['main_text', 'appendix_text', 'degree', 'quota', 'contract', 'grade']:\n",
    "                ewha_docs.append(doc)\n",
    "        \n",
    "        # Wikipedia ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "        if filtered_wiki_docs:\n",
    "            # priority ìˆœìœ¼ë¡œ ì •ë ¬ (high > medium > low)\n",
    "            priority_order = {'high': 3, 'medium': 2, 'low': 1}\n",
    "            filtered_wiki_docs.sort(\n",
    "                key=lambda x: priority_order.get(x.metadata.get('priority', 'low'), 1),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Wikipedia ë¬¸ì„œ ìš°ì„  ì‚¬ìš©\n",
    "            context_parts = []\n",
    "            for i, doc in enumerate(filtered_wiki_docs[:8], 1):  # ìµœëŒ€ 8ê°œ ì‚¬ìš©\n",
    "                doc_text = doc.page_content\n",
    "                source_info = doc.metadata.get('source', 'Unknown')\n",
    "                priority = doc.metadata.get('priority', 'medium')\n",
    "                context_parts.append(f\"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source_info}, ìš°ì„ ìˆœìœ„: {priority})\\n{doc_text}\")\n",
    "            \n",
    "            context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "            response = rag_chain.invoke({\"question\": question, \"context\": context_text})\n",
    "            \n",
    "            return {\n",
    "                \"response\": response.content,\n",
    "                \"sources\": [doc.metadata for doc in filtered_wiki_docs],\n",
    "                \"num_sources\": len(filtered_wiki_docs),\n",
    "                \"is_ewha_related\": False\n",
    "            }\n",
    "        elif is_math_problem:\n",
    "            # ìˆ˜í•™ ë¬¸ì œëŠ” LLMì—ê²Œ ì§ì ‘ ê³„ì‚° ìš”ì²­\n",
    "            math_prompt = f\"\"\"\n",
    "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ì •í™•íˆ ê³„ì‚°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n",
    "ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ê³ , ë§ˆì§€ë§‰ì— \"ì •ë‹µ: (X)\" í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì œ: {question}\n",
    "\"\"\"\n",
    "            response = general_chain.invoke({\"question\": math_prompt})\n",
    "            \n",
    "            return {\n",
    "                \"response\": response.content,\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0,\n",
    "                \"is_ewha_related\": False\n",
    "            }\n",
    "        else:\n",
    "            # Wikipedia ë¬¸ì„œê°€ ì—†ìœ¼ë©´ LLMë§Œ ì‚¬ìš©\n",
    "            response = general_chain.invoke({\"question\": question})\n",
    "            \n",
    "            return {\n",
    "                \"response\": response.content,\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0,\n",
    "                \"is_ewha_related\": False\n",
    "            }\n",
    "\n",
    "# ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‘ë‹µ ìƒì„±\n",
    "responses = []\n",
    "if prompts and len(prompts) > 0:\n",
    "    print(f\"\\nâ–¶ ì§ˆë¬¸ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„± ì‹œì‘... (ì´ {len(prompts)}ê°œ ì§ˆë¬¸)\")\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts, 1):\n",
    "        try:\n",
    "            result = generate_rag_response(prompt, retriever, rag_chain, general_chain, classification_chain)\n",
    "            responses.append({\n",
    "                \"question_num\": idx,\n",
    "                \"question\": prompt,\n",
    "                \"response\": result[\"response\"],\n",
    "                \"sources\": result[\"sources\"],\n",
    "                \"num_sources\": result[\"num_sources\"],\n",
    "                \"is_ewha_related\": result.get(\"is_ewha_related\", False)\n",
    "            })\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰)\n",
    "            if idx % 10 == 0 or idx == len(prompts):\n",
    "                ewha_status = \"ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨\" if result.get(\"is_ewha_related\", False) else \"ì¼ë°˜ ë¬¸ì œ\"\n",
    "                print(f\"   ì§„í–‰: {idx}/{len(prompts)} ì™„ë£Œ ({ewha_status})\")\n",
    "                print(f\"   ìµœê·¼ ì§ˆë¬¸: {prompt[:60]}...\")\n",
    "                print(f\"   ìµœê·¼ ì‘ë‹µ: {result['response'][:80]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì§ˆë¬¸ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            responses.append({\n",
    "                \"question_num\": idx,\n",
    "                \"question\": prompt,\n",
    "                \"response\": f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0,\n",
    "                \"is_ewha_related\": False\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nâœ… RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(responses)}ê°œ\")\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    results_df = pd.DataFrame(responses)\n",
    "    \n",
    "    # ì •ë‹µì´ ìˆìœ¼ë©´ ì¶”ê°€ ë° ì±„ì \n",
    "    if gold_answers and len(gold_answers) == len(responses):\n",
    "        results_df[\"gold_answer\"] = gold_answers\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë‹µë³€ ì¶”ì¶œ\n",
    "        print(\"\\nğŸ“ ë‹µë³€ ì¶”ì¶œ ì¤‘...\")\n",
    "        results_df[\"predicted_answer\"] = results_df[\"response\"].apply(\n",
    "            lambda x: extract_answer(x) if pd.notna(x) else None\n",
    "        )\n",
    "        \n",
    "        # ì •ë‹µ ì¶”ì¶œ\n",
    "        results_df[\"gold_answer_extracted\"] = results_df[\"gold_answer\"].apply(\n",
    "            lambda x: extract_gold_answer(x) if pd.notna(x) else None\n",
    "        )\n",
    "        \n",
    "        # ì •ì˜¤ ì±„ì \n",
    "        results_df[\"is_correct\"] = (\n",
    "            results_df[\"predicted_answer\"] == results_df[\"gold_answer_extracted\"]\n",
    "        )\n",
    "        \n",
    "        # ì •ë‹µë¥  ê³„ì‚°\n",
    "        total_questions = len(results_df)\n",
    "        correct_count = results_df[\"is_correct\"].sum()\n",
    "        accuracy = (correct_count / total_questions * 100) if total_questions > 0 else 0\n",
    "        \n",
    "        print(f\"âœ… ë‹µë³€ ì¶”ì¶œ ë° ì±„ì  ì™„ë£Œ\")\n",
    "        print(f\"\\nğŸ“Š ì±„ì  ê²°ê³¼:\")\n",
    "        print(f\"   - ì´ ë¬¸ì œ ìˆ˜: {total_questions}ê°œ\")\n",
    "        print(f\"   - ì •ë‹µ ìˆ˜: {correct_count}ê°œ\")\n",
    "        print(f\"   - ì˜¤ë‹µ ìˆ˜: {total_questions - correct_count}ê°œ\")\n",
    "        print(f\"   - ì •ë‹µë¥ : {accuracy:.2f}%\")\n",
    "        \n",
    "        # ì§ˆë¬¸ ë¶„ë¥˜ í†µê³„\n",
    "        if \"is_ewha_related\" in results_df.columns:\n",
    "            ewha_related_count = results_df[\"is_ewha_related\"].sum()\n",
    "            general_count = total_questions - ewha_related_count\n",
    "            print(f\"\\nğŸ“‹ ì§ˆë¬¸ ë¶„ë¥˜ í†µê³„:\")\n",
    "            print(f\"   - ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨: {ewha_related_count}ê°œ\")\n",
    "            print(f\"   - ì¼ë°˜ ë¬¸ì œ: {general_count}ê°œ\")\n",
    "        \n",
    "        # ë¬¸ì œë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ì˜¤ë‹µë§Œ) + ì›ì¸ ë¶„ì„\n",
    "        wrong_answers = results_df[~results_df[\"is_correct\"]]\n",
    "        if len(wrong_answers) > 0:\n",
    "            print(f\"\\nâŒ ì˜¤ë‹µ ë¬¸ì œ ({len(wrong_answers)}ê°œ):\")\n",
    "            wrong_nums = wrong_answers[\"question_num\"].tolist() if \"question_num\" in wrong_answers.columns else []\n",
    "            print(f\"   ë¬¸ì œ ë²ˆí˜¸: {', '.join(map(str, wrong_nums))}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ì˜¤ë‹µ ë¬¸ì œ ìƒì„¸ ë¶„ì„:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # ì˜¤ë‹µ ì›ì¸ ë¶„ì„ í•¨ìˆ˜\n",
    "            def analyze_wrong_answer(row):\n",
    "                \"\"\"ì˜¤ë‹µ ì›ì¸ ë¶„ì„\"\"\"\n",
    "                q_num = row.get(\"question_num\", \"N/A\")\n",
    "                pred = row[\"predicted_answer\"] if pd.notna(row[\"predicted_answer\"]) else None\n",
    "                gold = row[\"gold_answer_extracted\"] if pd.notna(row[\"gold_answer_extracted\"]) else None\n",
    "                response_text = row.get(\"response\", \"\") if pd.notna(row.get(\"response\", \"\")) else \"\"\n",
    "                is_ewha = row.get(\"is_ewha_related\", False) if pd.notna(row.get(\"is_ewha_related\", False)) else False\n",
    "                num_sources = row.get(\"num_sources\", 0) if pd.notna(row.get(\"num_sources\", 0)) else 0\n",
    "                \n",
    "                reasons = []\n",
    "                \n",
    "                # ì›ì¸ 1: ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨\n",
    "                if pred is None:\n",
    "                    reasons.append(\"âŒ ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨ - ì‘ë‹µì—ì„œ ì„ íƒì§€ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨\")\n",
    "                    if \"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\" in response_text or \"not present\" in response_text.lower():\n",
    "                        reasons.append(\"   â†’ ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìŒ\")\n",
    "                    else:\n",
    "                        reasons.append(\"   â†’ ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„\")\n",
    "                \n",
    "                # ì›ì¸ 2: ì¼ë°˜ ë¬¸ì œì¸ë° RAG ë¯¸ì‚¬ìš©\n",
    "                elif not is_ewha and num_sources == 0:\n",
    "                    reasons.append(\"âš ï¸ ì¼ë°˜ ë¬¸ì œ - Wikipedia/ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\")\n",
    "                    # ì£¼ì œ ì¶”ì •\n",
    "                    question_lower = str(row.get(\"question\", \"\")).lower()\n",
    "                    if any(kw in question_lower for kw in [\"kohlberg\", \"moral\", \"stage\"]):\n",
    "                        reasons.append(\"   â†’ í•„ìš”í•œ ë¬¸ì„œ: Kohlberg's stages of moral development\")\n",
    "                    elif any(kw in question_lower for kw in [\"robbery\", \"larceny\", \"burglary\", \"stolen\"]):\n",
    "                        reasons.append(\"   â†’ í•„ìš”í•œ ë¬¸ì„œ: Robbery, Larceny, Burglary ë¹„êµ ë¬¸ì„œ\")\n",
    "                    elif any(kw in question_lower for kw in [\"homo\", \"erectus\", \"hominid\", \"neoteny\"]):\n",
    "                        reasons.append(\"   â†’ í•„ìš”í•œ ë¬¸ì„œ: Homo erectus, Neoteny\")\n",
    "                    elif any(kw in question_lower for kw in [\"tang\", \"dynasty\", \"nomadic\"]):\n",
    "                        reasons.append(\"   â†’ í•„ìš”í•œ ë¬¸ì„œ: Tang dynasty (nomadic peoples)\")\n",
    "                    elif any(kw in question_lower for kw in [\"deutschland\", \"clemenceau\"]):\n",
    "                        reasons.append(\"   â†’ í•„ìš”í•œ ë¬¸ì„œ: Deutschland Ã¼ber alles, Clemenceau\")\n",
    "                \n",
    "                # ì›ì¸ 3: í•™ì¹™ ê´€ë ¨ ë¬¸ì œì¸ë° ì˜ëª»ëœ ë‹µë³€\n",
    "                elif is_ewha:\n",
    "                    reasons.append(\"âš ï¸ ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨ ë¬¸ì œ - ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶€ì •í™•í•˜ê±°ë‚˜ ë¶€ì¡±í•¨\")\n",
    "                    if num_sources < 3:\n",
    "                        reasons.append(f\"   â†’ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ ë¶€ì¡± ({num_sources}ê°œ)\")\n",
    "                \n",
    "                # ì›ì¸ 4: ë‹µë³€ì€ ì¶”ì¶œí–ˆì§€ë§Œ ì˜ëª»ë¨\n",
    "                elif pred != gold:\n",
    "                    reasons.append(f\"âŒ ì˜ëª»ëœ ë‹µë³€ ì¶”ì¶œ - ì˜ˆì¸¡: {pred}, ì •ë‹µ: {gold}\")\n",
    "                    if not is_ewha:\n",
    "                        reasons.append(\"   â†’ ì™¸ë¶€ ë¬¸ì„œ ë²¡í„°í™” í•„ìš”\")\n",
    "                \n",
    "                return reasons\n",
    "            \n",
    "            for _, row in wrong_answers.iterrows():\n",
    "                q_num = row.get(\"question_num\", \"N/A\")\n",
    "                pred = row[\"predicted_answer\"] if pd.notna(row[\"predicted_answer\"]) else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                gold = row[\"gold_answer_extracted\"] if pd.notna(row[\"gold_answer_extracted\"]) else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                question_text = row.get(\"question\", \"\") if pd.notna(row.get(\"question\", \"\")) else \"\"\n",
    "                response_text = row.get(\"response\", \"\") if pd.notna(row.get(\"response\", \"\")) else \"\"\n",
    "                is_ewha = row.get(\"is_ewha_related\", False) if pd.notna(row.get(\"is_ewha_related\", False)) else False\n",
    "                question_type = \"ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨\" if is_ewha else \"ì¼ë°˜ ë¬¸ì œ\"\n",
    "                \n",
    "                print(f\"\\n[ë¬¸ì œ {q_num}] ({question_type})\")\n",
    "                print(f\"ì§ˆë¬¸: {question_text}\")\n",
    "                print(f\"ì˜ˆì¸¡ ë‹µë³€: {pred}\")\n",
    "                print(f\"ì •ë‹µ: {gold}\")\n",
    "                \n",
    "                # ì›ì¸ ë¶„ì„\n",
    "                reasons = analyze_wrong_answer(row)\n",
    "                if reasons:\n",
    "                    print(\"ì›ì¸ ë¶„ì„:\")\n",
    "                    for reason in reasons:\n",
    "                        print(f\"  {reason}\")\n",
    "                \n",
    "                print(f\"ìƒì„±ëœ ì‘ë‹µ: {response_text[:300]}...\" if len(response_text) > 300 else f\"ìƒì„±ëœ ì‘ë‹µ: {response_text}\")\n",
    "                print(\"-\" * 80)\n",
    "        \n",
    "        # ì •ë‹µ ë¬¸ì œ ì „ì²´ ì¶œë ¥\n",
    "        correct_answers = results_df[results_df[\"is_correct\"]]\n",
    "        if len(correct_answers) > 0:\n",
    "            print(f\"\\nâœ… ì •ë‹µ ë¬¸ì œ ({len(correct_answers)}ê°œ):\")\n",
    "            correct_nums = correct_answers[\"question_num\"].tolist() if \"question_num\" in correct_answers.columns else []\n",
    "            print(f\"   ë¬¸ì œ ë²ˆí˜¸: {', '.join(map(str, correct_nums))}\")\n",
    "            \n",
    "            # ì •ë‹µ ë¬¸ì œ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ì •ë‹µ ë¬¸ì œ ìƒì„¸:\")\n",
    "            print(\"=\" * 80)\n",
    "            for _, row in correct_answers.iterrows():\n",
    "                q_num = row.get(\"question_num\", \"N/A\")\n",
    "                pred = row[\"predicted_answer\"] if pd.notna(row[\"predicted_answer\"]) else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                gold = row[\"gold_answer_extracted\"] if pd.notna(row[\"gold_answer_extracted\"]) else \"ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "                question_text = row.get(\"question\", \"\") if pd.notna(row.get(\"question\", \"\")) else \"\"\n",
    "                is_ewha = row.get(\"is_ewha_related\", False) if pd.notna(row.get(\"is_ewha_related\", False)) else False\n",
    "                question_type = \"ì´í™”ì—¬ëŒ€ í•™ì¹™ ê´€ë ¨\" if is_ewha else \"ì¼ë°˜ ë¬¸ì œ\"\n",
    "                \n",
    "                print(f\"\\n[ë¬¸ì œ {q_num}] ({question_type}) - ì •ë‹µ: {gold}\")\n",
    "                print(f\"ì§ˆë¬¸: {question_text[:150]}...\" if len(question_text) > 150 else f\"ì§ˆë¬¸: {question_text}\")\n",
    "                print(\"-\" * 80)\n",
    "        \n",
    "        # ìµœì¢… ì ìˆ˜ ì •ë³´ ì¶”ê°€\n",
    "        results_df[\"final_score\"] = f\"{correct_count}/{total_questions} ({accuracy:.2f}%)\"\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ ì •ë‹µ ì •ë³´ê°€ ì—†ì–´ ì±„ì ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    output_path = CURRENT_DIR / \"rag_responses.csv\"\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ì‘ë‹µì„ {output_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“Š ìš”ì•½:\")\n",
    "    print(f\"   - ì´ ì§ˆë¬¸ ìˆ˜: {len(responses)}\")\n",
    "    print(f\"   - í‰ê·  ì°¸ê³  ë¬¸ì„œ ìˆ˜: {results_df['num_sources'].mean():.1f}\")\n",
    "    if \"is_correct\" in results_df.columns:\n",
    "        print(f\"   - ìµœì¢… ì ìˆ˜: {results_df['final_score'].iloc[0] if 'final_score' in results_df.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì²˜ë¦¬í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
