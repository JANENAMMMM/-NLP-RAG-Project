{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae73e88",
   "metadata": {},
   "source": [
    "# RAG í†µí•© ìµœì¢… ë²„ì „\n",
    "## ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì„¸ ë¶„ì˜ ì‘ì—…ì„ í†µí•©í•œ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤:\n",
    "- **ë‹¤í˜„ë‹˜ ì‘ì—…**: ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬, í•™ìœ„/ì •ì› ë¬¸ì¥ ìƒì„±, ë³¸ë¬¸ ì²­í‚¹\n",
    "- **ë‚˜í˜„ë‹˜ ì‘ì—…**: UpstageDocumentParseLoader í™œìš© (ì°¸ê³ )\n",
    "- **ì‚¬ìš©ì ì‘ì—…**: í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„°í™”\n",
    "\n",
    "### í†µí•© ì „ëµ\n",
    "1. PDF ë³¸ë¬¸ê³¼ ë¶€ì¹™ì„ ë¶„ë¦¬í•˜ì—¬ ì²­í‚¹\n",
    "2. CSV í‘œ ë°ì´í„°ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "3. ëª¨ë“  ë¬¸ì„œë¥¼ FAISS ë²¡í„° ìŠ¤í† ì–´ì— í†µí•©\n",
    "4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° í‰ê°€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‘ì—… ë””ë ‰í† ë¦¬: c:\\Users\\user\\-NLP-RAG-Project\n",
      "âœ… Upstage API Key: up_g01lLro3nefoqy9IZ...\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Upstage API Key ì„¤ì •\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"up_g01lLro3nefoqy9IZGw3NK4ExjfIQ\")\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "CURRENT_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "if Path.cwd() != CURRENT_DIR:\n",
    "    os.chdir(CURRENT_DIR)\n",
    "\n",
    "\n",
    "print(f\"âœ… ì‘ì—… ë””ë ‰í† ë¦¬: {CURRENT_DIR}\")\n",
    "print(f\"âœ… Upstage API Key: {UPSTAGE_API_KEY[:20]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31860c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_upstage in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.35)\n",
      "Requirement already satisfied: openai in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.78 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (0.3.80)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (4.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21.0,>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (0.20.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.4.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (6.0.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tokenizers<0.21.0,>=0.20.0->langchain_upstage) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (2025.5.1)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (11.2.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install langchain_upstage langchain_community langchain-openai openai pdfplumber pandas faiss-cpu wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303f6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "âœ… ë³¸ë¬¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: 38 í˜ì´ì§€\n",
      "ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: 20919\n",
      "ğŸ“„ ë¶€ì¹™ ê¸¸ì´: 20222\n",
      "ğŸ“Œ ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ë¶„ë¦¬ ìˆ˜: 223\n",
      "ğŸ“Œ ìµœì¢… ë³¸ë¬¸ ì²­í¬ ìˆ˜: 223\n",
      "ğŸ“Œ 1ì°¨ ë¶€ì¹™ ê°œì • ë‹¨ìœ„ ìˆ˜: 80\n",
      "ğŸ“Œ 2ì°¨ ë¶€ì¹™ ì¡°í•­ ë¶„ë¦¬ ìˆ˜: 250\n",
      "ğŸ“Œ ìµœì¢… ë¶€ì¹™ ì²­í¬ ìˆ˜: 251\n",
      "\n",
      "================ RESULT ================\n",
      "ë³¸ë¬¸ ì²­í¬ ìˆ˜: 223\n",
      "ë¶€ì¹™ ì²­í¬ ìˆ˜: 251\n",
      "ì´ ì²­í¬ ìˆ˜: 474\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "PDF_PATH = CURRENT_DIR / \"ewha.pdf\"\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_PATH}\")\n",
    "\n",
    "print(\"ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¶€ì¹™ ì‹œì‘ ì „ê¹Œì§€ë§Œ ì„ íƒ (38page ê¸°ì¤€)\n",
    "docs = [d for d in docs if int(d.metadata[\"page\"]) < 38]\n",
    "\n",
    "print(f\"âœ… ë³¸ë¬¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "full_text = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# ë¶€ì¹™(\"ë¶€ì¹™\") ë“±ì¥ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "# -------------------------------------------------------------\n",
    "split_index = full_text.find(\"ë¶€ì¹™\")\n",
    "\n",
    "if split_index == -1:\n",
    "    print(\"âš ï¸ 'ë¶€ì¹™' ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë³¸ë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    main_text = full_text\n",
    "    appendix_text = \"\"\n",
    "else:\n",
    "    main_text = full_text[:split_index]\n",
    "    appendix_text = full_text[split_index:]\n",
    "\n",
    "print(f\"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(main_text)}\")\n",
    "print(f\"ğŸ“„ ë¶€ì¹™ ê¸¸ì´: {len(appendix_text)}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 2. ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ê¸°ë°˜ ë¶„ë¦¬\n",
    "# =============================================================\n",
    "def split_by_structure(text: str):\n",
    "    \"\"\"\n",
    "    ì´í™” í•™ì¹™ ë¬¸ì„œë¥¼ ì¡°í•­/í•­ëª© ê¸°ë°˜ìœ¼ë¡œ 1ì°¨ ë¶„ë¦¬\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "        (?=ì œ\\d+ì¡°)               # ì œ1ì¡°\n",
    "        |(?=\\n\\d+\\.)              # 1.\n",
    "        |(?=\\n\\d+\\))              # 1)\n",
    "        |(?=\\n\\d+\\s*-\\s*\\d+\\s*-\\s*\\d+)  # 2 - 2 - 39\n",
    "    \"\"\"\n",
    "    parts = re.split(pattern, text, flags=re.VERBOSE)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "structured_sections = split_by_structure(main_text)\n",
    "print(\"ğŸ“Œ ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ë¶„ë¦¬ ìˆ˜:\", len(structured_sections))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 3. ë³¸ë¬¸ RecursiveCharacterTextSplitter ì²­í‚¹\n",
    "# =============================================================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=900,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"ë³„í‘œ\", \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \" \"]  # ìš°ì„ ìˆœìœ„ ë†’ì€ ìˆœ\n",
    ")\n",
    "\n",
    "main_chunks = []\n",
    "for sec in structured_sections:\n",
    "    split_docs = text_splitter.create_documents([sec])\n",
    "    main_chunks.extend(split_docs)\n",
    "\n",
    "print(\"ğŸ“Œ ìµœì¢… ë³¸ë¬¸ ì²­í¬ ìˆ˜:\", len(main_chunks))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 4. ë¶€ì¹™ ì²­í‚¹\n",
    "# =============================================================\n",
    "\n",
    "# --- 4-1) ë¶€ì¹™ì„ 'ë¶€ì¹™(YYYY.MM.DD ê°œì •)' ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "def split_appendix(appendix: str):\n",
    "    pattern = r\"(?=ë¶€ì¹™\\(\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2}\\s*ê°œì •\\))\"\n",
    "    parts = re.split(pattern, appendix)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "appendix_sections = split_appendix(appendix_text)\n",
    "print(\"ğŸ“Œ 1ì°¨ ë¶€ì¹™ ê°œì • ë‹¨ìœ„ ìˆ˜:\", len(appendix_sections))\n",
    "\n",
    "\n",
    "# --- 4-2) ê° ë¶€ì¹™ ë‚´ ì¡°í•­ ê¸°ë°˜ ì¶”ê°€ ë¶„ë¦¬\n",
    "def split_by_articles(text: str):\n",
    "    pattern = r\"(?=ì œ\\d+ì¡°)|(?=\\n?\\d+\\s*\\()\"\n",
    "    parts = re.split(pattern, text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "fine_grained_sections = []\n",
    "for sec in appendix_sections:\n",
    "    fine_grained_sections.extend(split_by_articles(sec))\n",
    "\n",
    "print(\"ğŸ“Œ 2ì°¨ ë¶€ì¹™ ì¡°í•­ ë¶„ë¦¬ ìˆ˜:\", len(fine_grained_sections))\n",
    "\n",
    "\n",
    "# --- 4-3) ìµœì¢… ë¶€ì¹™ ì²­í‚¹\n",
    "appendix_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=900,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "appendix_chunks = appendix_splitter.create_documents(fine_grained_sections)\n",
    "\n",
    "print(\"ğŸ“Œ ìµœì¢… ë¶€ì¹™ ì²­í¬ ìˆ˜:\", len(appendix_chunks))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 5. ê²°ê³¼ ìš”ì•½\n",
    "# =============================================================\n",
    "print(\"\\n================ RESULT ================\")\n",
    "print(f\"ë³¸ë¬¸ ì²­í¬ ìˆ˜: {len(main_chunks)}\")\n",
    "print(f\"ë¶€ì¹™ ì²­í¬ ìˆ˜: {len(appendix_chunks)}\")\n",
    "print(f\"ì´ ì²­í¬ ìˆ˜: {len(main_chunks) + len(appendix_chunks)}\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310634f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ degree ë¬¸ì„œ ìƒì„±: 160ê°œ\n",
      "ğŸ‰ capacity ë¬¸ì„œ ìˆ˜: 626ê°œ\n",
      "ğŸ‰ ê³„ì•½í•™ê³¼ ë¬¸ì„œ: 1ê°œ\n",
      "ğŸ‰ GPA ë¬¸ì„œ ìƒì„±: 26ê°œ\n",
      "\n",
      "ğŸ“¦ CSV ê¸°ë°˜ ì´ Document ìˆ˜: 813\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# ğŸ“¦ CSV ì²˜ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ (VS Code ìƒëŒ€ê²½ë¡œ ê¸°ì¤€)\n",
    "# ==================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "CURRENT_DIR = Path.cwd()\n",
    "\n",
    "# ==================================================================\n",
    "# 1) degrees í´ë” â†’ í•™ìœ„ ë¬¸ì¥ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "DEG_DIR = CURRENT_DIR / \"degrees\"\n",
    "\n",
    "degree_files = [\n",
    "    DEG_DIR / \"degree_2006.csvì˜ ì‚¬ë³¸\",\n",
    "    DEG_DIR / \"degree_2009.csvì˜ ì‚¬ë³¸\",\n",
    "    DEG_DIR / \"degree_2017.csvì˜ ì‚¬ë³¸\",\n",
    "    DEG_DIR / \"degree_latest.csvì˜ ì‚¬ë³¸\",\n",
    "]\n",
    "\n",
    "degree_sentences = []\n",
    "\n",
    "for path in degree_files:\n",
    "    match = re.search(r\"(20\\d{2})\", str(path))\n",
    "    year = f\"{match.group(1)} ê°œì •\" if match else \"ìµœì‹  ê°œì •\"\n",
    "\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        college = row.get(\"ëŒ€í•™\")\n",
    "        degree = row.get(\"í•™ì‚¬\")\n",
    "        major = row.get(\"ì „ê³µ\")\n",
    "\n",
    "        if pd.isna(college) or pd.isna(degree) or pd.isna(major):\n",
    "            continue\n",
    "\n",
    "        s = f\"{str(college).strip()}ì˜ {str(major).strip()} ì „ê³µì€ {str(degree).strip()} í•™ìœ„ë¥¼ ìˆ˜ì—¬í•œë‹¤. ({year})\"\n",
    "        degree_sentences.append(s)\n",
    "\n",
    "# ì €ì¥\n",
    "out_degree = DEG_DIR / \"degree_sentences_all.csv\"\n",
    "pd.DataFrame({\"sentence\": degree_sentences}).to_csv(out_degree, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 1) degree_sentences â†’ Documentë¡œ ë³€í™˜\n",
    "degree_documents = [\n",
    "    Document(\n",
    "        page_content=s,\n",
    "        metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„\", \"type\": \"degree\"}\n",
    "    )\n",
    "    for s in degree_sentences\n",
    "]\n",
    "print(f\"ğŸ“ degree ë¬¸ì„œ ìƒì„±: {len(degree_documents)}ê°œ\")\n",
    "\n",
    "# ==================================================================\n",
    "# 2) capacity.csv â†’ ì…í•™ì •ì› ë¬¸ì„œ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "csv_documents = []\n",
    "\n",
    "cap_path = CURRENT_DIR / \"capacity.csv\"\n",
    "df_cap = pd.read_csv(cap_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "years = sorted(df_cap[\"year\"].unique())\n",
    "\n",
    "def build_quota_text(row):\n",
    "    return f\"\"\"\n",
    "    {int(row['year'])}í•™ë…„ë„ ì…í•™ì •ì› ì •ë³´:\n",
    "    - ëŒ€í•™: {row['college']}\n",
    "    - í•™ë¶€/í•™ê³¼: {row['department']}\n",
    "    - ì „ê³µ: {row['major']}\n",
    "    - ì •ì›: {row['quota']}ëª…\n",
    "    \"\"\".strip()\n",
    "\n",
    "for year in years:\n",
    "    df_year = df_cap[df_cap[\"year\"] == year]\n",
    "    for _, row in df_year.iterrows():\n",
    "        text = build_quota_text(row)\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": f\"[ë³„í‘œ 1] {year}í•™ë…„ë„ ì…í•™ì •ì›\",\n",
    "                \"type\": \"quota\"\n",
    "            }\n",
    "        ))\n",
    "\n",
    "print(f\"ğŸ‰ capacity ë¬¸ì„œ ìˆ˜: {len(csv_documents)}ê°œ\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 3) contract_dept.csv â†’ ê³„ì•½í•™ê³¼ ë¬¸ì„œ\n",
    "# ==================================================================\n",
    "\n",
    "def build_contract_text(row):\n",
    "    return f\"\"\"\n",
    "    ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ ì •ë³´:\n",
    "    - ì„¤ì¹˜ëŒ€í•™: {row.get('ì„¤ì¹˜ëŒ€í•™','')}\n",
    "    - ì„¤ì¹˜í˜•íƒœ: {row.get('ì„¤ì¹˜í˜•íƒœ','')}\n",
    "    - í•™ê³¼/ì „ê³µ: {row.get('í•™ê³¼_ì „ê³µ','')}\n",
    "    - ìˆ˜ì—¬ í•™ìœ„: {row.get('í•™ìœ„_ì¢…ë¥˜','')}\n",
    "    - ì…í•™ ì •ì›: {row.get('ì…í•™ì •ì›_ëª…','')}ëª…\n",
    "    - ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„: {row.get('ì„¤ì¹˜_ìš´ì˜ê¸°ê°„','')}\n",
    "    \"\"\".strip()\n",
    "\n",
    "contract_path = CURRENT_DIR / \"contract_dept.csv\"\n",
    "if contract_path.exists():\n",
    "    df_ct = pd.read_csv(contract_path, encoding=\"utf-8-sig\")\n",
    "    for _, row in df_ct.iterrows():\n",
    "        csv_documents.append(Document(\n",
    "            page_content=build_contract_text(row),\n",
    "            metadata={\"source\": \"[ë³„í‘œ 3] ê³„ì•½í•™ê³¼\", \"type\": \"contract\"}\n",
    "        ))\n",
    "    print(f\"ğŸ‰ ê³„ì•½í•™ê³¼ ë¬¸ì„œ: {len(df_ct)}ê°œ\")\n",
    "else:\n",
    "    print(\"âš  contract_dept.csv ì—†ìŒ\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# 4) grade.csv â†’ GPA ë³€í™˜í‘œ ë¬¸ì„œ\n",
    "# ==================================================================\n",
    "\n",
    "grade_documents = []\n",
    "\n",
    "grade_path = CURRENT_DIR / \"grade.csv\"\n",
    "df_grade = pd.read_csv(grade_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "def build_gpa_text(row):\n",
    "    return f\"\"\"\n",
    "    GPA ë³€í™˜ ê¸°ì¤€:\n",
    "    - ê¸°ì¤€ ì—°ë„: {row['year']}\n",
    "    - ë“±ê¸‰: {row['grade']}\n",
    "    - í™˜ì‚° ì ìˆ˜: {row['gpa']}\n",
    "    \"\"\".strip()\n",
    "\n",
    "for _, row in df_grade.iterrows():\n",
    "    grade_documents.append(Document(\n",
    "        page_content=build_gpa_text(row),\n",
    "        metadata={\"source\": \"GPA ë³€í™˜í‘œ\", \"type\": \"grade\"}\n",
    "    ))\n",
    "\n",
    "print(f\"ğŸ‰ GPA ë¬¸ì„œ ìƒì„±: {len(grade_documents)}ê°œ\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ìµœì¢… ì¶œë ¥\n",
    "# ==================================================================\n",
    "\n",
    "print(\"\\nğŸ“¦ CSV ê¸°ë°˜ ì´ Document ìˆ˜:\",\n",
    "      len(degree_sentences) + len(csv_documents) + len(grade_documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2027823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“¦ ì´ ë¬¸ì„œ(ì²­í¬) ìˆ˜: 1287ê°œ\n",
      "ğŸ§¹ ìœ íš¨ ë¬¸ì„œ ìˆ˜: 1287ê°œ\n",
      "\n",
      "ğŸ“Š ë¬¸ì„œ íƒ€ì… í†µê³„:\n",
      "  - main_text: 223ê°œ\n",
      "  - appendix_text: 251ê°œ\n",
      "  - degree: 160ê°œ\n",
      "  - quota: 626ê°œ\n",
      "  - contract: 1ê°œ\n",
      "  - grade: 26ê°œ\n",
      "ğŸ“‚ Windows TEMP ë””ë ‰í† ë¦¬ ì‚¬ìš©: C:\\Users\\user\\AppData\\Local\\Temp\\rag_vectorstore\n",
      "\n",
      "ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ìƒˆë¡œ ìƒì„± ì¤‘... (ì´ 1287ê°œ ë¬¸ì„œ)\n",
      "âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "ğŸ¯ ìµœì¢… ë¬¸ì„œ ìˆ˜: 1287ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 3. ëª¨ë“  ë¬¸ì„œ í†µí•© ë° FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
    "# ==================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import platform, tempfile\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Upstage ì„ë² ë”© ë¡œë“œ\n",
    "# -------------------------------------------------------\n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-embedding-1-large\"\n",
    ")\n",
    "print(\"âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) ë¬¸ì„œ í†µí•© (PDF ë³¸ë¬¸ + ë¶€ì¹™ + CSV + GPA)\n",
    "# -------------------------------------------------------\n",
    "all_documents = main_chunks + appendix_chunks  + degree_documents + csv_documents + grade_documents\n",
    "print(f\"ğŸ“¦ ì´ ë¬¸ì„œ(ì²­í¬) ìˆ˜: {len(all_documents)}ê°œ\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) ë³¸ë¬¸/ë¶€ì¹™ metadata ì •ë¦¬\n",
    "# -------------------------------------------------------\n",
    "for doc in main_chunks:\n",
    "    doc.metadata = doc.metadata or {}\n",
    "    doc.metadata.update({\"source\": \"ewha.pdf ë³¸ë¬¸\", \"type\": \"main_text\"})\n",
    "\n",
    "for doc in appendix_chunks:\n",
    "    doc.metadata = doc.metadata or {}\n",
    "    doc.metadata.update({\"source\": \"ewha.pdf ë¶€ì¹™\", \"type\": \"appendix_text\"})\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) ë¹ˆ ë¬¸ì„œ ì œê±°\n",
    "# -------------------------------------------------------\n",
    "valid_documents = [d for d in all_documents if d.page_content and d.page_content.strip()]\n",
    "print(f\"ğŸ§¹ ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(valid_documents)}ê°œ\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„ ì¶œë ¥\n",
    "# -------------------------------------------------------\n",
    "type_stats = {}\n",
    "for d in valid_documents:\n",
    "    t = d.metadata.get(\"type\", \"unknown\")\n",
    "    type_stats[t] = type_stats.get(t, 0) + 1\n",
    "\n",
    "print(\"\\nğŸ“Š ë¬¸ì„œ íƒ€ì… í†µê³„:\")\n",
    "for k, v in type_stats.items():\n",
    "    print(f\"  - {k}: {v}ê°œ\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) ë²¡í„° DB ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "#     â†’ Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ TEMP ì‚¬ìš©\n",
    "# -------------------------------------------------------\n",
    "if platform.system() == \"Windows\":\n",
    "    base_dir = Path(tempfile.gettempdir()) / \"rag_vectorstore\"\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    print(f\"ğŸ“‚ Windows TEMP ë””ë ‰í† ë¦¬ ì‚¬ìš©: {base_dir}\")\n",
    "else:\n",
    "    base_dir = CURRENT_DIR / \"vectorstore\"\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "vector_db_path = str(base_dir.resolve())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) ë²¡í„° ìŠ¤í† ì–´ ìƒì„±/ë¡œë“œ\n",
    "# -------------------------------------------------------\n",
    "FORCE_REBUILD = True\n",
    "\n",
    "if not FORCE_REBUILD:\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(\n",
    "            folder_path=vector_db_path,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        print(\"ğŸ“‚ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ\")\n",
    "    except:\n",
    "        print(\"âš  ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ â†’ ìƒˆë¡œ ìƒì„±\")\n",
    "        FORCE_REBUILD = True\n",
    "\n",
    "if FORCE_REBUILD:\n",
    "    print(f\"\\nğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ìƒˆë¡œ ìƒì„± ì¤‘... (ì´ {len(valid_documents)}ê°œ ë¬¸ì„œ)\")\n",
    "    vector_store = FAISS.from_documents(valid_documents, embeddings)\n",
    "    vector_store.save_local(vector_db_path)\n",
    "    print(\"âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(valid_documents)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4972f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def retrieve_wikipedia_docs(query: str, lang: str = 'en', chunk_size: int = 1000, chunk_overlap: int = 100):\n",
    "    docs = []\n",
    "    try:\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(\n",
    "            language=lang,\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "            user_agent=\"NLP_Project\"\n",
    "        )\n",
    "        \n",
    "        page_py = wiki_wiki.page(query)\n",
    "        \n",
    "        if page_py.exists() and page_py.text:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            chunks = text_splitter.split_text(page_py.text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if chunk and chunk.strip():\n",
    "                    docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"source\": \"wikipedia\",\n",
    "                            \"title\": page_py.title,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"type\": \"wikipedia\"\n",
    "                        }\n",
    "                    ))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return docs\n",
    "def extract_wikipedia_keywords(question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    í†µí•© ì—…ê·¸ë ˆì´ë“œ ë²„ì „:\n",
    "    - ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ\n",
    "    - ì˜ì–´ ê¸°ë°˜ MMLU ì§€ì‹ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    - í•œêµ­ì–´ ë¶„ì•¼ ë§¤í•‘\n",
    "    - Law / Psych / Philosophy / History ë¶„ì•¼ ìë™ ê°ì§€\n",
    "    - ëŒ€í‘œ Wikipedia ë¬¸ì„œ ìë™ ë§¤í•‘\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = []\n",
    "    q = question.lower()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1) ê³ ìœ ëª…ì‚¬(ì¸ë¬¼/ì‚¬ê±´/ê°œë…) ì¶”ì¶œ (ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ìŒ)\n",
    "    # ----------------------------------------------------\n",
    "    proper_nouns = re.findall(r\"\\b[A-Z][a-zA-Z]+\\b\", question)\n",
    "    keywords.extend(proper_nouns)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2) ê¸°ì¡´ academic_keywords (ì˜ì–´ ê¸°ë°˜ ë§¤í•‘)\n",
    "    # ----------------------------------------------------\n",
    "    academic_keywords = {\n",
    "        'homo': 'Human evolution',\n",
    "        'sapiens': 'Human evolution',\n",
    "        'australopithecus': 'Human evolution',\n",
    "        'erectus': 'Human evolution',\n",
    "        'habilis': 'Human evolution',\n",
    "        'bipolar': 'Psychology',\n",
    "        'disorder': 'Psychology',\n",
    "        'manic': 'Psychology',\n",
    "        'kohlberg': 'Moral development',\n",
    "        'kant': 'Philosophy',\n",
    "        'crime': 'Criminology',\n",
    "        'aristotle': 'Philosophy',\n",
    "        'jurisprudence': 'Law',\n",
    "        'ethics': 'Ethics',\n",
    "        'larceny': 'Law',\n",
    "        'criminal': 'Law',\n",
    "        'clemenceau': 'History',\n",
    "        'world war': 'History',\n",
    "        'du fu': 'Literature',\n",
    "        'poem': 'Literature',\n",
    "    }\n",
    "\n",
    "    for key, value in academic_keywords.items():\n",
    "        if key in q:\n",
    "            keywords.append(value)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3) í•œêµ­ì–´ í‚¤ì›Œë“œ â†’ ë¶„ì•¼ ë§¤í•‘\n",
    "    # ----------------------------------------------------\n",
    "    keyword_mapping_kr = {\n",
    "        'ì‹¬ë¦¬': 'Psychology',\n",
    "        'ì‹¬ë¦¬í•™': 'Psychology',\n",
    "        'ì‚¬íšŒ': 'Sociology',\n",
    "        'ì‚¬íšŒí•™': 'Sociology',\n",
    "        'í†µê³„': 'Statistics',\n",
    "        'ì² í•™': 'Philosophy',\n",
    "        'ì—­ì‚¬': 'History',\n",
    "        'ê²½ì œ': 'Economics',\n",
    "        'ì •ì¹˜': 'Political Science',\n",
    "        'ë²•': 'Law',\n",
    "        'ì˜í•™': 'Medicine',\n",
    "        'ê°„í˜¸': 'Nursing',\n",
    "        'ë¬¸í•™': 'Literature',\n",
    "    }\n",
    "\n",
    "    for k, v in keyword_mapping_kr.items():\n",
    "        if k in question:\n",
    "            keywords.append(v)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4) ë¶„ì•¼ ê°ì§€ìš© íŠ¸ë¦¬ê±° ë‹¨ì–´ (Law/Psych/Philosophy/History)\n",
    "    # ----------------------------------------------------\n",
    "    field_triggers = {\n",
    "        \"law\": [\"law\", \"legal\", \"crime\", \"contract\", \"tort\", \"jurisprudence\", \"homicide\", \"larceny\"],\n",
    "        \"psych\": [\"psychology\", \"behavior\", \"cognitive\", \"mental\", \"disorder\", \"personality\"],\n",
    "        \"phil\": [\"philosophy\", \"ethics\", \"moral\", \"kant\", \"aristotle\", \"epistemology\", \"metaphysics\", \"logic\"],\n",
    "        \"hist\": [\"history\", \"war\", \"revolution\", \"ancient\", \"empire\", \"dynasty\", \"ww1\", \"ww2\", \"cold war\"],\n",
    "    }\n",
    "\n",
    "    detected_fields = set()\n",
    "    for field, triggers in field_triggers.items():\n",
    "        for t in triggers:\n",
    "            if t in q:\n",
    "                detected_fields.add(field)\n",
    "                break\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 5) ëŒ€í‘œ Wikipedia ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë§¤í•‘\n",
    "    # ----------------------------------------------------\n",
    "    representative_docs = {\n",
    "        \"law\": [\n",
    "            \"Law\", \"Jurisprudence\", \"Criminal law\", \"Contract\", \"Constitutional law\",\n",
    "            \"International law\", \"Law of war\", \"Mens rea\", \"Larceny\"\n",
    "        ],\n",
    "        \"psych\": [\n",
    "            \"Psychology\", \"Cognitive psychology\", \"Behaviorism\",\n",
    "            \"Abnormal psychology\", \"Personality psychology\",\n",
    "            \"Bipolar disorder\", \"Anxiety disorder\", \"Classical conditioning\"\n",
    "        ],\n",
    "        \"phil\": [\n",
    "            \"Philosophy\", \"Ethics\", \"Deontological ethics\", \"Utilitarianism\",\n",
    "            \"Virtue ethics\", \"Epistemology\", \"Metaphysics\",\n",
    "            \"Immanuel Kant\", \"Aristotle\", \"Categorical imperative\"\n",
    "        ],\n",
    "        \"hist\": [\n",
    "            \"History\", \"World history\", \"French Revolution\", \"American Revolution\",\n",
    "            \"World War I\", \"World War II\", \"Cold War\", \"Industrial Revolution\",\n",
    "            \"History of Europe\", \"History of Asia\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for field in detected_fields:\n",
    "        keywords.extend(representative_docs[field])\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 6) ì˜ì–´ ê¸°ë°˜ ë¶„ì•¼ ë§¤í•‘ (ì¶”ê°€ ê°ì§€)\n",
    "    # ----------------------------------------------------\n",
    "    english_keywords = [\n",
    "        'Psychology', 'Education', 'Sociology', 'Statistics', 'Mathematics',\n",
    "        'Computer Science', 'Biology', 'Chemistry', 'Physics', 'Philosophy',\n",
    "        'History', 'Literature', 'Economics', 'Political Science', 'Law',\n",
    "        'Medicine', 'Nursing', 'Pharmacy', 'Art', 'Music'\n",
    "    ]\n",
    "\n",
    "    for keyword in english_keywords:\n",
    "        if keyword.lower() in q:\n",
    "            keywords.append(keyword)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 7) ì¤‘ë³µ ì œê±° & ê³ ìœ ëª…ì‚¬ ìš°ì„ ìˆœìœ„ ì •ë ¬\n",
    "    # ----------------------------------------------------\n",
    "    keywords = list(set(keywords))\n",
    "\n",
    "    def priority(k):\n",
    "        return 0 if k in proper_nouns else 1\n",
    "\n",
    "    keywords.sort(key=priority)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def extract_choices_from_question(question: str):\n",
    "    \"\"\"\n",
    "    ì¤„ë°”ê¿ˆ ê¸°ë°˜ ì„ íƒì§€ ì¶”ì¶œ (ë‹¤í˜„ testset.csv ìµœì í™”)\n",
    "    ex)\n",
    "    (A) text\n",
    "    (B) text\n",
    "    (C) text\n",
    "    ...\n",
    "    \"\"\"\n",
    "    pattern = r\"\\([A-Z]\\)\\s*(.+)\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return [m.strip() for m in matches if m.strip()]\n",
    "\n",
    "def extract_keywords_from_choices(choices: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    ì„ íƒì§€ í…ìŠ¤íŠ¸ì—ì„œ Wikipedia ê²€ìƒ‰ì— ì‚¬ìš©í•  í›„ë³´ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "\n",
    "    for choice in choices:\n",
    "        # ì˜ˆ) \"Appeal to novelty\" â†’ ë°”ë¡œ Wikipedia ê²€ìƒ‰ ê°€ëŠ¥\n",
    "        # ì˜ˆ) \"Bipolar disorder\" â†’ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "        # ì˜ˆ) \"Straw man\" â†’ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "        if len(choice.split()) <= 6:  # ë„ˆë¬´ ê¸´ ê±´ ì œê±°\n",
    "            keywords.append(choice)\n",
    "\n",
    "        # ê³ ìœ ëª…ì‚¬(ì˜ˆ: Kant, Aristotle)\n",
    "        proper = re.findall(r\"\\b[A-Z][a-zA-Z]+\\b\", choice)\n",
    "        keywords.extend(proper)\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    return list(set(keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cfb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: 5)\n",
      "âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 50ê°œ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "â–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "\n",
      "Q1: QUESTION1) ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ í•˜ë ¤ë©´ í•™ê¸° ê°œì‹œì¼ë¡œë¶€í„° ë©°ì¹  ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì•¼í•˜ë‚˜ìš”?\n",
      "(...\n",
      "ì‘ë‹µ: ì •ë‹µ: (D) 90ì¼\n",
      "\n",
      "Contextì—ì„œ ëª…ì‹œëœ í•™ì¹™ ì œ26ì¡° â‘¥í•­ì— ë”°ë¥´ë©´, ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ í•˜ê³ ì í•˜ëŠ” ê²½ìš° \"í•™ê¸°ê°œì‹œì¼ë¡œë¶€í„° 90ì¼ ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì—¬ì•¼ í•œë‹¤\"ê³  ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì •ë‹µì€ (D)ì…ë‹ˆë‹¤....\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: 60ì¼, 90ì¼, 30ì¼, 45ì¼)\n",
      "\n",
      "Q2: QUESTION2) 'ì¬ì…í•™ì€ aíšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ì œ 28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ...\n",
      "ì‘ë‹µ: ì •ë‹µ: (B) 3\n",
      "\n",
      "**ê·¼ê±°:**\n",
      "- **a ê°’:** \"ì¬ì…í•™ì€ 1íšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤\" (ì œ30ì¡° ì œ2í•­) â†’ a = 1\n",
      "- **b ê°’:** \"ì œ28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ ë‚ ë¶€í„° 1ë…„ì´ ê²½ê³¼í•œ í›„ ì¬ì…í•™í•  ìˆ˜ ìˆë‹¤\" (ì œ28ì¡° ì œ4í˜¸) â†’ b = 1...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 30ê°œ (í‚¤ì›Œë“œ: 4, A,B,C ì¤‘ ë‹µ ì—†ìŒ, 3, 2)\n",
      "\n",
      "Q3: QUESTION3) í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ ëª‡í•™ì  ì´ìƒ ...\n",
      "ì‘ë‹µ: ì •ë‹µ: (C) 21í•™ì \n",
      "\n",
      "ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ 6.23) â‘¢í•­ì— ëª…ì‹œëœ ë‚´ìš©ì— ë”°ë¥´ë©´, \"í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ 21í•™ì  ì´ìƒ ì·¨ë“í•œ ë•Œì—ëŠ” ë¶€ì „ê³µì„ ì´ìˆ˜í•œ ê²ƒìœ¼ë¡œ ì¸ì •í•œë‹¤\"ê³  ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì •í™•í•œ ë‹µë³€ì€ ...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: 18í•™ì , 25í•™ì , 15í•™ì , 21í•™ì )\n",
      "\n",
      "Q4: QUESTION4) ë‹¤ìŒ ë³´ê¸°ì˜ í•™ìƒë“¤ ì¤‘ ì œì ì„ ë‹¹í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒì„ ê³ ë¥´ë©´?\n",
      "(A) íŒœ : ì§•ê³„ì— ì˜í•´ í‡´...\n",
      "ì‘ë‹µ: ì •ë‹µ: (D)  \n",
      "\n",
      "**í•´ì„¤**:  \n",
      "- (A) \"í‡´í•™ì²˜ë¶„\"ì€ ì œì  ì‚¬ìœ ì— í•´ë‹¹í•©ë‹ˆë‹¤.  \n",
      "- (B) í•™ì¹™ ì œ40ì¡° â‘£í•­ì— ë”°ë¼ **í‰ì í‰ê·  1.60 ë¯¸ë§Œ** ì‹œ í•™ì‚¬ê²½ê³ ê°€ ë¶€ì—¬ë˜ë©°, **ì—°ì† 3íšŒ** ì‹œ ì œì ë©ë‹ˆë‹¤. ì—˜ëª¨ì˜ í‰ì  1.2ëŠ” ì´ ê¸°ì¤€ì— ëª…í™•íˆ í•´ë‹¹ë©ë‹ˆë‹¤....\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: íŒœ : ì§•ê³„ì— ì˜í•´ í‡´í•™ì²˜ë¶„ì„ ë°›ì•˜ìŒ)\n",
      "\n",
      "Q5: QUESTION5) 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ ëª‡ ëª…ì¸ê°€? \n",
      "(A) 90ëª… \n",
      "(B) 1...\n",
      "ì‘ë‹µ: ì •ë‹µ: (C) \n",
      "\n",
      "ë¬¸ë§¥ìƒ 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ 110ëª…ìœ¼ë¡œ ëª…ì‹œë˜ì–´ ìˆìœ¼ë¯€ë¡œ (C)ê°€ ì •í™•í•œ ì„ íƒì§€ì…ë‹ˆë‹¤. \n",
      "\n",
      "(C)...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: 110ëª…, 90ëª…, 120ëª…, 100ëª…)\n",
      "\n",
      "âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ê²½ë¡œ\n",
    "TESTSET_PATH = CURRENT_DIR / \"testset.csv\"\n",
    "TOP_K = 5  # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì¦ê°€\n",
    "\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "# LLM ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "llm = ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro2\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    \n",
    "    **ì¤‘ìš” ì§€ì¹¨:**\n",
    "    1. ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš” (êµ¬ì²´ì ì¸ ê·œì •)\n",
    "    2. Wikipedia ë¬¸ì„œëŠ” ì¼ë°˜ì ì¸ í•™ë¬¸ ì§€ì‹ ì°¸ê³ ìš©ì…ë‹ˆë‹¤\n",
    "    3. ì •í™•íˆ í•˜ë‚˜ì˜ ì„ íƒì§€ë§Œ ë‹µë³€í•˜ì„¸ìš”\n",
    "    4. ë‹µë³€ í˜•ì‹: \"ì •ë‹µ: (X)\" ë˜ëŠ” \"(X)\" í˜•ì‹ìœ¼ë¡œ ë‹µë³€\n",
    "    5. í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¨ìˆœíˆ í¬í•¨ëœê²Œ ì•„ë‹ˆë¼, ë¬¸ë§¥ìƒ ì •ë‹µì„ì„ í™•ì‹ í•  ë•Œë§Œ ì„ íƒì§€ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "    \n",
    "    \n",
    "    ì§ˆë¬¸ì´ ì„ íƒì§€ í˜•ì‹ì¸ ê²½ìš°, ì •ë‹µ ì„ íƒì§€ì˜ ë¬¸ì(ì˜ˆ: (A), (B), (C), (D), (E), (I), (J), (G), (H) ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.\n",
    "    \n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "print(f\"âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: {TOP_K})\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ\n",
    "def read_data(file_path: Path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data[\"prompts\"], data.get(\"answers\")\n",
    "\n",
    "try:\n",
    "    prompts, answers = read_data(TESTSET_PATH)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ ë¡œë“œ ì™„ë£Œ\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ {TESTSET_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    prompts, answers = [], None\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²˜ìŒ 5ê°œë§Œ)\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"\\nâ–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompts[:5], 1):\n",
    "        # 1. ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰\n",
    "        source_docs = retriever.invoke(prompt)\n",
    "       # ê¸°ì¡´ í‚¤ì›Œë“œ\n",
    "        wiki_keywords = extract_wikipedia_keywords(prompt)\n",
    "\n",
    "        # 1) ì„ íƒì§€ ì¶”ì¶œ\n",
    "        choices = extract_choices_from_question(prompt)\n",
    "\n",
    "        # 2) ì„ íƒì§€ ê¸°ë°˜ Wikipedia í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        choice_keywords = extract_keywords_from_choices(choices)\n",
    "\n",
    "        # 3) í•©ì¹˜ê¸°\n",
    "        wiki_keywords = list(set(wiki_keywords + choice_keywords))\n",
    "        \n",
    "        wiki_docs = []\n",
    "        if wiki_keywords:\n",
    "            for keyword in wiki_keywords:\n",
    "                try:\n",
    "                    keyword_docs = retrieve_wikipedia_docs(keyword, lang='en')\n",
    "                    wiki_docs.extend(keyword_docs)\n",
    "                except Exception as e:\n",
    "                    # ê°œë³„ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰\n",
    "                    pass\n",
    "        \n",
    "        # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context_parts = []\n",
    "        if source_docs:\n",
    "            context_parts.append(\"=== ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in source_docs))\n",
    "        if wiki_docs:\n",
    "            context_parts.append(\"=== Wikipedia ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in wiki_docs))\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "        \n",
    "        # 4. ì‘ë‹µ ìƒì„±\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "        \n",
    "        print(f\"Q{i}: {prompt[:60]}...\")\n",
    "        print(f\"ì‘ë‹µ: {response.content[:150]}...\")\n",
    "        print(f\"ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ {len(source_docs)}ê°œ, Wikipedia {len(wiki_docs)}ê°œ (í‚¤ì›Œë“œ: {', '.join(wiki_keywords) if wiki_keywords else 'ì—†ìŒ'})\\n\")\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d5b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\n",
      "\n",
      "ğŸ“‹ ì´ 50ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\n",
      "\n",
      "ğŸ”„ Q1/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q2/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q3/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q4/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q5/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q6/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q7/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q8/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q9/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q10/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 10/50 (20.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q11/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q12/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q13/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q14/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q15/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q16/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q17/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q18/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q19/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q20/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "  â””â”€ ì§„í–‰ë¥ : 20/50 (40.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q21/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q22/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q23/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: B, Gold: (D) (D)\n",
      "ğŸ”„ Q24/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: C, Gold: (D) (D)\n",
      "ğŸ”„ Q25/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q26/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q27/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q28/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "ğŸ”„ Q29/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: I, Gold: (I) (I)\n",
      "ğŸ”„ Q30/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "  â””â”€ ì§„í–‰ë¥ : 30/50 (60.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q31/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q32/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: G, Gold: (C) (C)\n",
      "ğŸ”„ Q33/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: I, Gold: (I) (I)\n",
      "ğŸ”„ Q34/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: G, Gold: (E) (E)\n",
      "ğŸ”„ Q35/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (B) (B)\n",
      "ğŸ”„ Q36/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: J, Gold: (J) (J)\n",
      "ğŸ”„ Q37/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q38/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: B, Gold: (D) (D)\n",
      "ğŸ”„ Q39/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: D, Gold: (G) (G)\n",
      "ğŸ”„ Q40/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (I) (I)\n",
      "  â””â”€ ì§„í–‰ë¥ : 40/50 (80.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q41/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "ğŸ”„ Q42/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q43/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q44/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: B, Gold: (F) (F)\n",
      "ğŸ”„ Q45/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (J) (J)\n",
      "ğŸ”„ Q46/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: I, Gold: (I) (I)\n",
      "ğŸ”„ Q47/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: G, Gold: (G) (G)\n",
      "ğŸ”„ Q48/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (H) (H)\n",
      "ğŸ”„ Q49/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: G, Gold: (G) (G)\n",
      "ğŸ”„ Q50/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 50/50 (100.0%)\n",
      "----------------------------------------\n",
      "\n",
      "âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: c:\\Users\\user\\-NLP-RAG-Project\\rag_results_combined.csv\n",
      "\n",
      "ğŸ“Š ì •í™•ë„: 39/50 (78.0%)\n",
      "   - ì •ë‹µ: 39ê°œ\n",
      "   - ì˜¤ë‹µ: 11ê°œ\n",
      "   - ë¯¸ì‘ë‹µ: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 5. ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ (ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ ë°©ì‹ ê²°í•©)\n",
    "# ==================================================================\n",
    "\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ ê°œì„  ë²„ì „ (ëª¨ë“  ì•ŒíŒŒë²³ ì„ íƒì§€ ì§€ì›: A-Z)\n",
    "    - ì—¬ëŸ¬ ì„ íƒì§€ê°€ ìˆì„ ê²½ìš° ê°€ì¥ í™•ì‹¤í•œ ê²ƒ ì„ íƒ\n",
    "    - \"ì •ë‹µ:\", \"Answer:\", \"ë‹µ:\" ë“±ì˜ í‚¤ì›Œë“œ ìš°ì„ \n",
    "    \"\"\"\n",
    "    # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì ì¸ ì •ë‹µ í‘œì‹œ (í•œêµ­ì–´/ì˜ì–´)\n",
    "    explicit_patterns = [\n",
    "        r\"ì •ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"Answer[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"\\[ANSWER\\]:\\s*\\(([A-Z])\\)\",\n",
    "        r\"ìµœì¢…\\s*ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"Final\\s*Answer[:\\s]*\\(([A-Z])\\)\",\n",
    "    ]\n",
    "    for pattern in explicit_patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 2: (X) í˜•ì‹ (ë§ˆì§€ë§‰ ë°œìƒ) - ëª¨ë“  ì•ŒíŒŒë²³ ì§€ì›\n",
    "    # ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ (ë³´í†µ ìµœì¢… ë‹µë³€)\n",
    "    pattern2 = r\"\\(([A-Z])\\)\"\n",
    "    matches = list(re.finditer(pattern2, response))\n",
    "    if matches:\n",
    "        return matches[-1].group(1)\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 3: ë‹¨ë… ë¬¸ì (ë§ˆì§€ë§‰ ë°œìƒ) - ëª¨ë“  ì•ŒíŒŒë²³ ì§€ì›\n",
    "    # ë‹¨ì–´ ê²½ê³„ì—ì„œ ì•ŒíŒŒë²³ë§Œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)\n",
    "    pattern3 = r\"\\b([A-Z])\\b\"\n",
    "    matches = list(re.finditer(pattern3, response, re.IGNORECASE))\n",
    "    if matches:\n",
    "        return matches[-1].group(1).upper()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ì „ì²´ í‰ê°€ ì‹¤í–‰\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\\n\")\n",
    "    print(f\"ğŸ“‹ ì´ {len(prompts)}ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\\n\")\n",
    "    \n",
    "    all_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        try:\n",
    "            print(f\"ğŸ”„ Q{i}/{len(prompts)} ì²˜ë¦¬ ì¤‘...\", end=\" \")\n",
    "            \n",
    "            # 1. ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰\n",
    "            source_docs = retriever.invoke(prompt)\n",
    "            \n",
    "            # 2. Wikipedia ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± (ì„ íƒì§€ ê¸°ë°˜ + ë¶„ì•¼ ê°ì§€ + ê³ ìœ ëª…ì‚¬ ê¸°ë°˜)\n",
    "            base_keywords = extract_wikipedia_keywords(prompt)\n",
    "\n",
    "            # ì„ íƒì§€ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "            choices = extract_choices_from_question(prompt)\n",
    "            choice_keywords = extract_keywords_from_choices(choices)\n",
    "\n",
    "            # ìµœì¢… í‚¤ì›Œë“œ í•©ì¹˜ê¸°\n",
    "            wiki_keywords = list(set(base_keywords + choice_keywords))  \n",
    "            # Wikipedia ë¬¸ì„œ ê²€ìƒ‰\n",
    "            wiki_docs = []\n",
    "            for keyword in wiki_keywords:\n",
    "                try:\n",
    "                    wiki_docs.extend(retrieve_wikipedia_docs(keyword, lang='en'))\n",
    "                except:\n",
    "                    pass\n",
    "            # ìœ„í‚¤ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì„ ê²½ìš° 10ê°œ ì œí•œ\n",
    "            if len(wiki_docs) > 10:\n",
    "                wiki_docs = wiki_docs[:10]\n",
    "            \n",
    "            # 3. ê¸°ì¡´ ë¬¸ì„œì™€ Wikipedia ë¬¸ì„œ í†µí•©\n",
    "            all_source_docs = source_docs + wiki_docs\n",
    "            \n",
    "            # 4. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            context_parts = []\n",
    "            if source_docs:\n",
    "                context_parts.append(\"=== ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in source_docs))\n",
    "            if wiki_docs:\n",
    "                context_parts.append(\"=== Wikipedia ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in wiki_docs))\n",
    "            \n",
    "            context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "            \n",
    "            # 5. ì‘ë‹µ ìƒì„±\n",
    "            response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "            \n",
    "            pred_raw = response.content.strip()\n",
    "            pred = extract_answer(pred_raw)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            all_responses.append({\n",
    "                \"question_num\": i,\n",
    "                \"question\": prompt[:100],\n",
    "                \"prediction\": pred,\n",
    "                \"response\": pred_raw[:200],\n",
    "                \"context_count\": len(source_docs),\n",
    "                \"wikipedia_count\": len(wiki_docs),\n",
    "                \"wikipedia_keywords\": \", \".join(wiki_keywords) if wiki_keywords else \"\"\n",
    "            })\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ëª¨ë“  ë¬¸ì œì— ëŒ€í•´)\n",
    "            gold = answers.iloc[i-1] if answers is not None else \"N/A\"\n",
    "            # Gold ë‹µë³€ì—ì„œ ì„ íƒì§€ ë¬¸ì ì¶”ì¶œ (ì˜ˆ: (D) -> D)\n",
    "            gold_match = re.search(r\"\\(([A-Z])\\)\", str(gold))\n",
    "            gold_letter = gold_match.group(1) if gold_match else str(gold).strip()\n",
    "            \n",
    "            # ì •ë‹µ ì—¬ë¶€ í™•ì¸ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)\n",
    "            is_correct = pred and str(pred).upper() == str(gold_letter).upper()\n",
    "            status = \"âœ…\" if is_correct else \"âŒ\" if pred else \"âš ï¸\"\n",
    "            print(f\"{status} Pred: {pred}, Gold: {gold} ({gold_letter})\")\n",
    "            \n",
    "            # 10ê°œë§ˆë‹¤ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "            if i % 10 == 0 or i == len(prompts):\n",
    "                print(f\"  â””â”€ ì§„í–‰ë¥ : {i}/{len(prompts)} ({i/len(prompts)*100:.1f}%)\")\n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = str(e)\n",
    "            error_trace = traceback.format_exc()\n",
    "            print(f\"âŒ ì—ëŸ¬ ë°œìƒ: {error_msg}\")\n",
    "            print(f\"  â””â”€ Q{i} ì²˜ë¦¬ ì‹¤íŒ¨, Noneìœ¼ë¡œ ì €ì¥\")\n",
    "            if i == 43:  # 43ë²ˆ ë¬¸ì œ ë””ë²„ê¹…\n",
    "                print(f\"  â””â”€ ìƒì„¸ ì—ëŸ¬:\\n{error_trace}\")\n",
    "            predictions.append(None)\n",
    "            all_responses.append({\n",
    "                \"question_num\": i,\n",
    "                \"question\": prompt[:100] if prompt else \"\",\n",
    "                \"prediction\": None,\n",
    "                \"response\": f\"Error: {error_msg}\",\n",
    "                \"context_count\": 0,\n",
    "                \"wikipedia_count\": 0,\n",
    "                \"wikipedia_keywords\": \"\"\n",
    "            })\n",
    "            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰\n",
    "            continue\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results_df = pd.DataFrame(all_responses)\n",
    "    results_path = CURRENT_DIR / \"rag_results_combined.csv\"\n",
    "    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}\")\n",
    "    \n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    if answers is not None:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (pred, gold) in enumerate(zip(predictions, answers)):\n",
    "            if pred is None:\n",
    "                continue\n",
    "            gold_match = re.search(r\"\\(([A-Z])\\)\", str(gold))\n",
    "            gold_letter = gold_match.group(1) if gold_match else str(gold).strip()\n",
    "            if str(pred).upper() == str(gold_letter).upper():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        if total > 0:\n",
    "            accuracy = correct / total * 100\n",
    "            print(f\"\\nğŸ“Š ì •í™•ë„: {correct}/{total} ({accuracy:.1f}%)\")\n",
    "            print(f\"   - ì •ë‹µ: {correct}ê°œ\")\n",
    "            print(f\"   - ì˜¤ë‹µ: {total - correct}ê°œ\")\n",
    "            print(f\"   - ë¯¸ì‘ë‹µ: {len(predictions) - total}ê°œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
