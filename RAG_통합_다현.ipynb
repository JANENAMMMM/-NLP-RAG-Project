{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae73e88",
   "metadata": {},
   "source": [
    "# RAG í†µí•© ìµœì¢… ë²„ì „\n",
    "## ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì„¸ ë¶„ì˜ ì‘ì—…ì„ í†µí•©í•œ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤:\n",
    "- **ë‹¤í˜„ë‹˜ ì‘ì—…**: ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬, í•™ìœ„/ì •ì› ë¬¸ì¥ ìƒì„±, ë³¸ë¬¸ ì²­í‚¹\n",
    "- **ë‚˜í˜„ë‹˜ ì‘ì—…**: UpstageDocumentParseLoader í™œìš© (ì°¸ê³ )\n",
    "- **ì‚¬ìš©ì ì‘ì—…**: í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„°í™”\n",
    "\n",
    "### í†µí•© ì „ëµ\n",
    "1. PDF ë³¸ë¬¸ê³¼ ë¶€ì¹™ì„ ë¶„ë¦¬í•˜ì—¬ ì²­í‚¹\n",
    "2. CSV í‘œ ë°ì´í„°ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "3. ëª¨ë“  ë¬¸ì„œë¥¼ FAISS ë²¡í„° ìŠ¤í† ì–´ì— í†µí•©\n",
    "4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° í‰ê°€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‘ì—… ë””ë ‰í† ë¦¬: c:\\Users\\user\\-NLP-RAG-Project\n",
      "âœ… Upstage API Key: up_g01lLro3nefoqy9IZ...\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Upstage API Key ì„¤ì •\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"up_g01lLro3nefoqy9IZGw3NK4ExjfIQ\")\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "CURRENT_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "if Path.cwd() != CURRENT_DIR:\n",
    "    os.chdir(CURRENT_DIR)\n",
    "\n",
    "\n",
    "print(f\"âœ… ì‘ì—… ë””ë ‰í† ë¦¬: {CURRENT_DIR}\")\n",
    "print(f\"âœ… Upstage API Key: {UPSTAGE_API_KEY[:20]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31860c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_upstage in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.35)\n",
      "Requirement already satisfied: openai in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.78 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (0.3.80)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (4.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21.0,>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_upstage) (0.20.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.4.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (6.0.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tokenizers<0.21.0,>=0.20.0->langchain_upstage) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (2025.5.1)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (11.2.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install langchain_upstage langchain_community langchain-openai openai pdfplumber pandas faiss-cpu wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303f6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "âœ… ë³¸ë¬¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: 38 í˜ì´ì§€\n",
      "ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: 20919\n",
      "ğŸ“„ ë¶€ì¹™ ê¸¸ì´: 20222\n",
      "ğŸ“Œ ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ë¶„ë¦¬ ìˆ˜: 223\n",
      "ğŸ“Œ ìµœì¢… ë³¸ë¬¸ ì²­í¬ ìˆ˜: 223\n",
      "ğŸ“Œ 1ì°¨ ë¶€ì¹™ ê°œì • ë‹¨ìœ„ ìˆ˜: 80\n",
      "ğŸ“Œ 2ì°¨ ë¶€ì¹™ ì¡°í•­ ë¶„ë¦¬ ìˆ˜: 250\n",
      "ğŸ“Œ ìµœì¢… ë¶€ì¹™ ì²­í¬ ìˆ˜: 253\n",
      "\n",
      "================ RESULT ================\n",
      "ë³¸ë¬¸ ì²­í¬ ìˆ˜: 223\n",
      "ë¶€ì¹™ ì²­í¬ ìˆ˜: 253\n",
      "ì´ ì²­í¬ ìˆ˜: 476\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "PDF_PATH = CURRENT_DIR / \"ewha.pdf\"\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_PATH}\")\n",
    "\n",
    "print(\"ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¶€ì¹™ ì‹œì‘ ì „ê¹Œì§€ë§Œ ì„ íƒ (38page ê¸°ì¤€)\n",
    "docs = [d for d in docs if int(d.metadata[\"page\"]) < 38]\n",
    "\n",
    "print(f\"âœ… ë³¸ë¬¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {len(docs)} í˜ì´ì§€\")\n",
    "\n",
    "full_text = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# ë¶€ì¹™(\"ë¶€ì¹™\") ë“±ì¥ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "# -------------------------------------------------------------\n",
    "split_index = full_text.find(\"ë¶€ì¹™\")\n",
    "\n",
    "if split_index == -1:\n",
    "    print(\"âš ï¸ 'ë¶€ì¹™' ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë³¸ë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    main_text = full_text\n",
    "    appendix_text = \"\"\n",
    "else:\n",
    "    main_text = full_text[:split_index]\n",
    "    appendix_text = full_text[split_index:]\n",
    "\n",
    "print(f\"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(main_text)}\")\n",
    "print(f\"ğŸ“„ ë¶€ì¹™ ê¸¸ì´: {len(appendix_text)}\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 2. ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ê¸°ë°˜ ë¶„ë¦¬\n",
    "# =============================================================\n",
    "def split_by_structure(text: str):\n",
    "    \"\"\"\n",
    "    ì´í™” í•™ì¹™ ë¬¸ì„œë¥¼ ì¡°í•­/í•­ëª© ê¸°ë°˜ìœ¼ë¡œ 1ì°¨ ë¶„ë¦¬\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "        (?=ì œ\\d+ì¡°)               # ì œ1ì¡°\n",
    "        |(?=\\n\\d+\\.)              # 1.\n",
    "        |(?=\\n\\d+\\))              # 1)\n",
    "        |(?=\\n\\d+\\s*-\\s*\\d+\\s*-\\s*\\d+)  # 2 - 2 - 39\n",
    "    \"\"\"\n",
    "    parts = re.split(pattern, text, flags=re.VERBOSE)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "structured_sections = split_by_structure(main_text)\n",
    "print(\"ğŸ“Œ ë³¸ë¬¸ 1ì°¨ êµ¬ì¡° ë¶„ë¦¬ ìˆ˜:\", len(structured_sections))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 3. ë³¸ë¬¸ RecursiveCharacterTextSplitter ì²­í‚¹\n",
    "# =============================================================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"ë³„í‘œ\", \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \" \"]  # ìš°ì„ ìˆœìœ„ ë†’ì€ ìˆœ\n",
    ")\n",
    "\n",
    "main_chunks = []\n",
    "for sec in structured_sections:\n",
    "    split_docs = text_splitter.create_documents([sec])\n",
    "    main_chunks.extend(split_docs)\n",
    "\n",
    "print(\"ğŸ“Œ ìµœì¢… ë³¸ë¬¸ ì²­í¬ ìˆ˜:\", len(main_chunks))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 4. ë¶€ì¹™ ì²­í‚¹\n",
    "# =============================================================\n",
    "\n",
    "# --- 4-1) ë¶€ì¹™ì„ 'ë¶€ì¹™(YYYY.MM.DD ê°œì •)' ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "def split_appendix(appendix: str):\n",
    "    pattern = r\"(?=ë¶€ì¹™\\(\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2}\\s*ê°œì •\\))\"\n",
    "    parts = re.split(pattern, appendix)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "appendix_sections = split_appendix(appendix_text)\n",
    "print(\"ğŸ“Œ 1ì°¨ ë¶€ì¹™ ê°œì • ë‹¨ìœ„ ìˆ˜:\", len(appendix_sections))\n",
    "\n",
    "\n",
    "# --- 4-2) ê° ë¶€ì¹™ ë‚´ ì¡°í•­ ê¸°ë°˜ ì¶”ê°€ ë¶„ë¦¬\n",
    "def split_by_articles(text: str):\n",
    "    pattern = r\"(?=ì œ\\d+ì¡°)|(?=\\n?\\d+\\s*\\()\"\n",
    "    parts = re.split(pattern, text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "fine_grained_sections = []\n",
    "for sec in appendix_sections:\n",
    "    fine_grained_sections.extend(split_by_articles(sec))\n",
    "\n",
    "print(\"ğŸ“Œ 2ì°¨ ë¶€ì¹™ ì¡°í•­ ë¶„ë¦¬ ìˆ˜:\", len(fine_grained_sections))\n",
    "\n",
    "\n",
    "# --- 4-3) ìµœì¢… ë¶€ì¹™ ì²­í‚¹\n",
    "appendix_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "appendix_chunks = appendix_splitter.create_documents(fine_grained_sections)\n",
    "\n",
    "print(\"ğŸ“Œ ìµœì¢… ë¶€ì¹™ ì²­í¬ ìˆ˜:\", len(appendix_chunks))\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 5. ê²°ê³¼ ìš”ì•½\n",
    "# =============================================================\n",
    "print(\"\\n================ RESULT ================\")\n",
    "print(f\"ë³¸ë¬¸ ì²­í¬ ìˆ˜: {len(main_chunks)}\")\n",
    "print(f\"ë¶€ì¹™ ì²­í¬ ìˆ˜: {len(appendix_chunks)}\")\n",
    "print(f\"ì´ ì²­í¬ ìˆ˜: {len(main_chunks) + len(appendix_chunks)}\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae83ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… degree_sentences_all.csv ì²˜ë¦¬ ì™„ë£Œ: 160ê°œ ë¬¸ì¥\n",
      "ğŸ“Œ CSVì— í¬í•¨ëœ ì—°ë„: [np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019)]\n",
      "âœ… 2014ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 100ê°œ ë¬¸ì„œ ìƒì„±\n",
      "âœ… 2015ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 104ê°œ ë¬¸ì„œ ìƒì„±\n",
      "âœ… 2016ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 107ê°œ ë¬¸ì„œ ìƒì„±\n",
      "âœ… 2017ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 105ê°œ ë¬¸ì„œ ìƒì„±\n",
      "âœ… 2018ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 105ê°œ ë¬¸ì„œ ìƒì„±\n",
      "âœ… 2019ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: 105ê°œ ë¬¸ì„œ ìƒì„±\n",
      "\n",
      "ğŸ‰ ì´ ìƒì„±ëœ CSV ë¬¸ì„œ ìˆ˜: 626ê°œ\n",
      "âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: 1ê°œ ë¬¸ì„œ\n",
      "\n",
      "ğŸ“¦ ì´ CSV ë¬¸ì„œ: 627ê°œ\n",
      "ğŸ“„ GPA ë³€í™˜ í…Œì´ë¸” 26ê°œ í•­ëª© ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ‰ GPA Document ìƒì„± ì™„ë£Œ: 26ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 2. CSV í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©)\n",
    "# ==================================================================\n",
    "\n",
    "def coalesce(*values: object) -> str:\n",
    "    \"\"\"ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’ì„ ë°˜í™˜\"\"\"\n",
    "    for value in values:\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            continue\n",
    "        text = str(value).strip()\n",
    "        if text:\n",
    "            return text\n",
    "    return \"\"\n",
    "\n",
    "# ë‹¤í˜„ë‹˜ ì‘ì—…: í•™ìœ„ ë¬¸ì¥ ìƒì„± ë°©ì‹\n",
    "def build_degree_sentences_from_csv(df: pd.DataFrame, year: str = \"ìµœì‹  ê°œì •\") -> List[str]:\n",
    "    \"\"\"CSVì—ì„œ í•™ìœ„ ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ ë°©ì‹)\"\"\"\n",
    "    sentences = []\n",
    "    for idx, row in df.iterrows():\n",
    "        college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"), row.get(\"ëŒ€í•™\"))\n",
    "        degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"), row.get(\"í•™ìœ„ì¢…ë¥˜\"), row.get(\"í•™ì‚¬\"))\n",
    "        major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"), row.get(\"ì „ê³µ\"))\n",
    "        \n",
    "        if not college or not degree or not major:\n",
    "            continue\n",
    "            \n",
    "        sentence = f\"{college}ì˜ {major} ì „ê³µì€ {degree} í•™ìœ„ë¥¼ ìˆ˜ì—¬í•œë‹¤. ({year})\"\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# ì‚¬ìš©ì ì‘ì—…: í‘œ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def build_quota_text(row: pd.Series) -> str:\n",
    "    \"\"\"ì…í•™ì •ì› í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    year = coalesce(row.get(\"í•™ë…„ë„\"), \"2019í•™ë…„ë„\")\n",
    "    college = coalesce(row.get(\"ëŒ€í•™\"))\n",
    "    faculty = coalesce(row.get(\"í•™ë¶€\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"))\n",
    "    quota = coalesce(row.get(\"ì •ì›\"), row.get(\"ì…í•™ì •ì›_ëª…\")) or \"ë¯¸ìƒ\"\n",
    "    \n",
    "    prefix_parts = [year]\n",
    "    if college:\n",
    "        prefix_parts.append(college)\n",
    "    if faculty:\n",
    "        prefix_parts.append(faculty)\n",
    "    prefix = \" \".join(prefix_parts)\n",
    "    return f\"{prefix} ì†Œì† {major}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "def build_degree_text(row: pd.Series) -> str:\n",
    "    \"\"\"í•™ìœ„ í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    sentence = f\"{college} ì†Œì† {major} ì „ê³µìì—ê²Œ ìˆ˜ì—¬í•˜ëŠ” í•™ìœ„ëŠ” {degree}ì…ë‹ˆë‹¤.\"\n",
    "    extras = []\n",
    "    if quota:\n",
    "        extras.append(f\"ì…í•™ ì •ì›ì€ {quota}ëª…\")\n",
    "    if period:\n",
    "        extras.append(f\"ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„ì€ {period}\")\n",
    "    if extras:\n",
    "        sentence += \" \" + \", \".join(extras) + \"ì…ë‹ˆë‹¤.\"\n",
    "    return sentence\n",
    "\n",
    "def build_contract_text(row: pd.Series) -> str:\n",
    "    \"\"\"ê³„ì•½í•™ê³¼ í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    form = coalesce(row.get(\"ì„¤ì¹˜í˜•íƒœ\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    parts = [f\"ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ ì •ë³´: ì„¤ì¹˜ëŒ€í•™={college}\"]\n",
    "    if form:\n",
    "        parts.append(f\"ì„¤ì¹˜í˜•íƒœ={form}\")\n",
    "    if major:\n",
    "        parts.append(f\"í•™ê³¼/ì „ê³µ={major}\")\n",
    "    if degree:\n",
    "        parts.append(f\"ìˆ˜ì—¬ í•™ìœ„={degree}\")\n",
    "    if quota:\n",
    "        parts.append(f\"ì…í•™ ì •ì›={quota}ëª…\")\n",
    "    if period:\n",
    "        parts.append(f\"ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„={period}\")\n",
    "    return \", \".join(parts) + \".\"\n",
    "\n",
    "# CSV íŒŒì¼ ì²˜ë¦¬\n",
    "csv_documents = []\n",
    "\n",
    "# 1) degrees í´ë”ì˜ íŒŒì¼ ì²˜ë¦¬ (ìˆ˜ë™ ì •ë¦¬ëœ íŒŒì¼ ì‚¬ìš©)\n",
    "degrees_dir = CURRENT_DIR / \"degrees\"\n",
    "degree_sentences_file = degrees_dir / \"degree_sentences_all.csvì˜ ì‚¬ë³¸\"\n",
    "degree_latest_file = degrees_dir / \"degree_latest.csvì˜ ì‚¬ë³¸\"\n",
    "\n",
    "# ìš°ì„ ìˆœìœ„ 1: ì´ë¯¸ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ ì‚¬ìš©\n",
    "if degree_sentences_file.exists():\n",
    "    df_sentences = pd.read_csv(degree_sentences_file, encoding='utf-8-sig')\n",
    "    for _, row in df_sentences.iterrows():\n",
    "        sentence = str(row.get(\"sentence\", \"\")).strip()\n",
    "        if sentence:\n",
    "            csv_documents.append(Document(\n",
    "                page_content=sentence,\n",
    "                metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "            ))\n",
    "    print(f\"âœ… degree_sentences_all.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_sentences)}ê°œ ë¬¸ì¥\")\n",
    "# ìš°ì„ ìˆœìœ„ 2: degree_latest.csvë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ìƒì„±\n",
    "elif degree_latest_file.exists():\n",
    "    df_degrees = pd.read_csv(degree_latest_file, encoding='utf-8-sig')\n",
    "    # ë¹ˆ í–‰ ì œê±°\n",
    "    df_degrees = df_degrees.dropna(subset=['ëŒ€í•™', 'í•™ì‚¬', 'ì „ê³µ'], how='all')\n",
    "    # ë‹¤í˜„ë‹˜ ë°©ì‹: í•™ìœ„ ë¬¸ì¥ ìƒì„±\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degree_latest.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "# ìš°ì„ ìˆœìœ„ 3: ê¸°ì¡´ degrees.csv ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)\n",
    "elif (CURRENT_DIR / \"degrees.csv\").exists():\n",
    "    df_degrees = pd.read_csv(CURRENT_DIR / \"degrees.csv\", encoding='utf-8-sig')\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degrees.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ degrees ê´€ë ¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2) 2019_quota.csv ì²˜ë¦¬\n",
    "# ==================================================================\n",
    "# í•˜ë‚˜ì˜ CSV(ì—°ë„ í¬í•¨)ì—ì„œ ìë™ìœ¼ë¡œ quota ë¬¸ì„œ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "csv_documents = []\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ (íŒŒì¼ëª…ë§Œ ë§ê²Œ ìˆ˜ì •í•˜ë©´ ë¨)\n",
    "CSV_PATH = CURRENT_DIR / \"capacity.csv\"\n",
    "\n",
    "# CSV íŒŒì¼ ì²´í¬\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}\")\n",
    "\n",
    "# CSV ë¡œë“œ\n",
    "df = pd.read_csv(CSV_PATH, encoding='utf-8-sig')\n",
    "\n",
    "required_cols = {\"year\", \"college\", \"department\", \"major\", \"quota\"}\n",
    "if not required_cols.issubset(df.columns):\n",
    "    raise ValueError(f\"CSV íŒŒì¼ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {required_cols}\")\n",
    "\n",
    "# ì—°ë„ ëª©ë¡ ì¶”ì¶œ\n",
    "years = sorted(df[\"year\"].dropna().unique())\n",
    "print(\"ğŸ“Œ CSVì— í¬í•¨ëœ ì—°ë„:\", years)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í• ì§€ í•¨ìˆ˜ ì •ì˜\n",
    "def build_quota_text(row):\n",
    "    \"\"\"ì •ì› ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ìì—°ì–´ë¡œ ìƒì„±\"\"\"\n",
    "    year = int(row[\"year\"])\n",
    "    college = str(row[\"college\"]) if pd.notna(row[\"college\"]) else \"\"\n",
    "    department = str(row[\"department\"]) if pd.notna(row[\"department\"]) else \"\"\n",
    "    major = str(row[\"major\"]) if pd.notna(row[\"major\"]) else \"\"\n",
    "    quota = str(row[\"quota\"]) if pd.notna(row[\"quota\"]) else \"\"\n",
    "\n",
    "    text = f\"\"\"\n",
    "    {year}í•™ë…„ë„ ì…í•™ì •ì› ì •ë³´:\n",
    "\n",
    "    - ëŒ€í•™: {college}\n",
    "    - í•™ë¶€/í•™ê³¼: {department}\n",
    "    - ì „ê³µ: {major}\n",
    "    - ì •ì›: {quota}ëª…\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# ì—°ë„ë³„ë¡œ ì²˜ë¦¬\n",
    "for year in years:\n",
    "    df_year = df[df[\"year\"] == year]\n",
    "\n",
    "    count = 0\n",
    "    for _, row in df_year.iterrows():\n",
    "        text = build_quota_text(row)\n",
    "\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": f\"[ë³„í‘œ 1] {year}í•™ë…„ë„ ì…í•™ì •ì›\",\n",
    "                \"year\": year,\n",
    "                \"college\": row[\"college\"],\n",
    "                \"department\": row[\"department\"],\n",
    "                \"major\": row[\"major\"],\n",
    "                \"type\": \"quota\"\n",
    "            }\n",
    "        ))\n",
    "        count += 1\n",
    "\n",
    "    print(f\"âœ… {year}ë…„ë„ ì²˜ë¦¬ ì™„ë£Œ: {count}ê°œ ë¬¸ì„œ ìƒì„±\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ì´ ìƒì„±ëœ CSV ë¬¸ì„œ ìˆ˜: {len(csv_documents)}ê°œ\")\n",
    "\n",
    "\n",
    "# 3) contract_dept.csv ì²˜ë¦¬\n",
    "contract_path = CURRENT_DIR / \"contract_dept.csv\"\n",
    "if contract_path.exists():\n",
    "    df_contract = pd.read_csv(contract_path, encoding='utf-8-sig')\n",
    "    for _, row in df_contract.iterrows():\n",
    "        text = build_contract_text(row)\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 3] ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜\", \"page\": 53, \"type\": \"contract\"}\n",
    "        ))\n",
    "    print(f\"âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_contract)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ contract_dept.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ CSV ë¬¸ì„œ: {len(csv_documents)}ê°œ\")\n",
    "# ==================================================================\n",
    "# 1980ë…„ ê¸°ì¤€ GPA ë³€í™˜ í…Œì´ë¸” CSV â†’ Document ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "grade_documents = []\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ\n",
    "CSV_PATH = CURRENT_DIR / \"grade.csv\"   # â†’ íŒŒì¼ ì´ë¦„ ë§ê²Œ ìˆ˜ì •í•´ë„ ë¨\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}\")\n",
    "\n",
    "# CSV ë¡œë“œ\n",
    "df_grade = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ ì²´í¬\n",
    "required_cols = {\"year\", \"grade\", \"gpa\"}\n",
    "if not required_cols.issubset(df_grade.columns):\n",
    "    raise ValueError(f\"CSV íŒŒì¼ì— í•„ìš”í•œ ì»¬ëŸ¼ ë¶€ì¡±: {required_cols}\")\n",
    "\n",
    "print(f\"ğŸ“„ GPA ë³€í™˜ í…Œì´ë¸” {len(df_grade)}ê°œ í•­ëª© ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def build_gpa_text(row):\n",
    "    year_cond = str(row[\"year\"])  # \"from 1980\" ë˜ëŠ” \"before 1980\"\n",
    "    grade = str(row[\"grade\"])\n",
    "    gpa = str(row[\"gpa\"])\n",
    "\n",
    "    text = f\"\"\"\n",
    "    GPA ë³€í™˜ ê¸°ì¤€:\n",
    "    - ê¸°ì¤€ ì—°ë„: {year_cond}\n",
    "    - ë“±ê¸‰(Grade): {grade}\n",
    "    - í™˜ì‚° ì ìˆ˜(GPA): {gpa}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Document ìƒì„±\n",
    "for _, row in df_grade.iterrows():\n",
    "    text = build_gpa_text(row)\n",
    "\n",
    "    grade_documents.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            \"source\": \"GPA ë³€í™˜í‘œ\",\n",
    "            \"year_condition\": row[\"year\"],\n",
    "            \"grade\": row[\"grade\"],\n",
    "            \"gpa\": row[\"gpa\"],\n",
    "            \"type\": \"grade_table\"\n",
    "        }\n",
    "    ))\n",
    "\n",
    "print(f\"ğŸ‰ GPA Document ìƒì„± ì™„ë£Œ: {len(grade_documents)}ê°œ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“¦ ë²¡í„° ìŠ¤í† ì–´ì— í¬í•¨í•  ë¬¸ì„œ: 1103ê°œ (Wikipedia ì œì™¸)\n",
      "ğŸ§¹ í•„í„°ë§ í›„ ìœ íš¨ ë¬¸ì„œ ìˆ˜: 1103\n",
      "\n",
      "ğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„:\n",
      "  - main_text: 223ê°œ\n",
      "  - appendix_text: 253ê°œ\n",
      "  - quota: 626ê°œ\n",
      "  - contract: 1ê°œ\n",
      "ğŸ“‚ Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
      "ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: C:\\Users\\user\\AppData\\Local\\Temp\\rag_ewha_vectorstore\n",
      "\n",
      "ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ì¬ìƒì„± ëª¨ë“œ (Wikipedia ë¬¸ì„œ ì œì™¸)\n",
      "\n",
      "ğŸ“¦ ì´ 1103ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°í™” ì¤‘ì…ë‹ˆë‹¤...\n",
      "âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ\n",
      "\n",
      "ğŸ¯ ì´ KB ë¬¸ì„œ ìˆ˜: 1103ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 3. ëª¨ë“  ë¬¸ì„œ í†µí•© ë° FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-embedding-1-large\"\n",
    ")\n",
    "print(\"âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ í†µí•© (PDF ë³¸ë¬¸/ë¶€ì¹™ + CSV í‘œ ë°ì´í„°ë§Œ)\n",
    "# Wikipedia ë¬¸ì„œëŠ” ì§ˆë¬¸ë§ˆë‹¤ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì¶”ê°€ (ë²¡í„° ìŠ¤í† ì–´ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ)\n",
    "all_documents = main_chunks + appendix_chunks + csv_documents\n",
    "print(f\"ğŸ“¦ ë²¡í„° ìŠ¤í† ì–´ì— í¬í•¨í•  ë¬¸ì„œ: {len(all_documents)}ê°œ (Wikipedia ì œì™¸)\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë³¸ë¬¸/ë¶€ì¹™ ì²­í¬)\n",
    "for i, doc in enumerate(main_chunks):\n",
    "    if not hasattr(doc, 'metadata') or not doc.metadata:\n",
    "        doc.metadata = {}\n",
    "    doc.metadata[\"source\"] = \"ewha.pdf ë³¸ë¬¸\"\n",
    "    doc.metadata[\"type\"] = \"main_text\"\n",
    "\n",
    "for i, doc in enumerate(appendix_chunks):\n",
    "    if not hasattr(doc, 'metadata') or not doc.metadata:\n",
    "        doc.metadata = {}\n",
    "    doc.metadata[\"source\"] = \"ewha.pdf ë¶€ì¹™\"\n",
    "    doc.metadata[\"type\"] = \"appendix_text\"\n",
    "\n",
    "# ìœ íš¨í•œ ë¬¸ì„œë§Œ í•„í„°ë§\n",
    "valid_documents = [doc for doc in all_documents if doc.page_content and doc.page_content.strip()]\n",
    "print(f\"ğŸ§¹ í•„í„°ë§ í›„ ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(valid_documents)}\")\n",
    "\n",
    "# ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„\n",
    "doc_types = {}\n",
    "for doc in valid_documents:\n",
    "    doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "\n",
    "print(\"\\nğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„:\")\n",
    "for doc_type, count in doc_types.items():\n",
    "    print(f\"  - {doc_type}: {count}ê°œ\")\n",
    "\n",
    "# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "# Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì˜ì–´ ê²½ë¡œë§Œ ìˆëŠ” ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "import platform\n",
    "import tempfile\n",
    "\n",
    "# Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ë‚˜ ì˜ì–´ ê²½ë¡œ ì‚¬ìš©\n",
    "if platform.system() == 'Windows':\n",
    "    # ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ TEMP ì‚¬ìš© (ë³´í†µ C:\\Users\\USERNAME\\AppData\\Local\\Temp)\n",
    "    temp_dir = Path(tempfile.gettempdir())\n",
    "    # í”„ë¡œì íŠ¸ë³„ ê³ ìœ í•œ ë””ë ‰í† ë¦¬ëª… ìƒì„±\n",
    "    vector_db_base = temp_dir / \"rag_ewha_vectorstore\"\n",
    "    vector_db_base.mkdir(parents=True, exist_ok=True)\n",
    "    vector_db_path = str(vector_db_base.resolve())\n",
    "    print(f\"ğŸ“‚ Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©\")\n",
    "    print(f\"ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: {vector_db_path}\")\n",
    "else:\n",
    "    # Windowsê°€ ì•„ë‹Œ ê²½ìš° ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©\n",
    "    VECTOR_DB_DIR = CURRENT_DIR / \"vectorstore\"\n",
    "    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    vector_db_path = str(VECTOR_DB_DIR.resolve())\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ì¬ìƒì„± ì˜µì…˜ (Wikipedia ë¬¸ì„œ ì œê±°ë¥¼ ìœ„í•´)\n",
    "FORCE_REBUILD = True  # Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±\n",
    "\n",
    "vector_store = None\n",
    "vector_db_dir = Path(vector_db_path)\n",
    "\n",
    "if not FORCE_REBUILD and vector_db_dir.exists() and (vector_db_dir / \"index.faiss\").exists():\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(\n",
    "            folder_path=vector_db_path,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        print(f\"\\nğŸ“‚ ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as exc:\n",
    "        print(f\"âš ï¸ ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {exc}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        vector_store = None\n",
    "else:\n",
    "    if FORCE_REBUILD:\n",
    "        print(f\"\\nğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ì¬ìƒì„± ëª¨ë“œ (Wikipedia ë¬¸ì„œ ì œì™¸)\")\n",
    "\n",
    "if vector_store is None:\n",
    "    print(f\"\\nğŸ“¦ ì´ {len(valid_documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°í™” ì¤‘ì…ë‹ˆë‹¤...\")\n",
    "    vector_store = FAISS.from_documents(valid_documents, embeddings)\n",
    "    # ì˜ì–´ ê²½ë¡œ ì‚¬ìš© (í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°)\n",
    "    vector_store.save_local(vector_db_path)\n",
    "    print(f\"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì´ KB ë¬¸ì„œ ìˆ˜: {len(valid_documents)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4972f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def retrieve_wikipedia_docs(query: str, lang: str = 'en', chunk_size: int = 1000, chunk_overlap: int = 100):\n",
    "    docs = []\n",
    "    try:\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(\n",
    "            language=lang,\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "            user_agent=\"NLP_Project\"\n",
    "        )\n",
    "        \n",
    "        page_py = wiki_wiki.page(query)\n",
    "        \n",
    "        if page_py.exists() and page_py.text:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            chunks = text_splitter.split_text(page_py.text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if chunk and chunk.strip():\n",
    "                    docs.append(Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"source\": \"wikipedia\",\n",
    "                            \"title\": page_py.title,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"type\": \"wikipedia\"\n",
    "                        }\n",
    "                    ))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return docs\n",
    "def extract_wikipedia_keywords(question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    í†µí•© ì—…ê·¸ë ˆì´ë“œ ë²„ì „:\n",
    "    - ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ\n",
    "    - ì˜ì–´ ê¸°ë°˜ MMLU ì§€ì‹ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    - í•œêµ­ì–´ ë¶„ì•¼ ë§¤í•‘\n",
    "    - Law / Psych / Philosophy / History ë¶„ì•¼ ìë™ ê°ì§€\n",
    "    - ëŒ€í‘œ Wikipedia ë¬¸ì„œ ìë™ ë§¤í•‘\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = []\n",
    "    q = question.lower()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1) ê³ ìœ ëª…ì‚¬(ì¸ë¬¼/ì‚¬ê±´/ê°œë…) ì¶”ì¶œ (ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ìŒ)\n",
    "    # ----------------------------------------------------\n",
    "    proper_nouns = re.findall(r\"\\b[A-Z][a-zA-Z]+\\b\", question)\n",
    "    keywords.extend(proper_nouns)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2) ê¸°ì¡´ academic_keywords (ì˜ì–´ ê¸°ë°˜ ë§¤í•‘)\n",
    "    # ----------------------------------------------------\n",
    "    academic_keywords = {\n",
    "        'homo': 'Human evolution',\n",
    "        'sapiens': 'Human evolution',\n",
    "        'australopithecus': 'Human evolution',\n",
    "        'erectus': 'Human evolution',\n",
    "        'habilis': 'Human evolution',\n",
    "        'bipolar': 'Psychology',\n",
    "        'disorder': 'Psychology',\n",
    "        'manic': 'Psychology',\n",
    "        'kohlberg': 'Moral development',\n",
    "        'kant': 'Philosophy',\n",
    "        'crime': 'Criminology',\n",
    "        'aristotle': 'Philosophy',\n",
    "        'jurisprudence': 'Law',\n",
    "        'ethics': 'Ethics',\n",
    "        'larceny': 'Law',\n",
    "        'criminal': 'Law',\n",
    "        'clemenceau': 'History',\n",
    "        'world war': 'History',\n",
    "        'du fu': 'Literature',\n",
    "        'poem': 'Literature',\n",
    "    }\n",
    "\n",
    "    for key, value in academic_keywords.items():\n",
    "        if key in q:\n",
    "            keywords.append(value)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3) í•œêµ­ì–´ í‚¤ì›Œë“œ â†’ ë¶„ì•¼ ë§¤í•‘\n",
    "    # ----------------------------------------------------\n",
    "    keyword_mapping_kr = {\n",
    "        'ì‹¬ë¦¬': 'Psychology',\n",
    "        'ì‹¬ë¦¬í•™': 'Psychology',\n",
    "        'ì‚¬íšŒ': 'Sociology',\n",
    "        'ì‚¬íšŒí•™': 'Sociology',\n",
    "        'í†µê³„': 'Statistics',\n",
    "        'ì² í•™': 'Philosophy',\n",
    "        'ì—­ì‚¬': 'History',\n",
    "        'ê²½ì œ': 'Economics',\n",
    "        'ì •ì¹˜': 'Political Science',\n",
    "        'ë²•': 'Law',\n",
    "        'ì˜í•™': 'Medicine',\n",
    "        'ê°„í˜¸': 'Nursing',\n",
    "        'ë¬¸í•™': 'Literature',\n",
    "    }\n",
    "\n",
    "    for k, v in keyword_mapping_kr.items():\n",
    "        if k in question:\n",
    "            keywords.append(v)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4) ë¶„ì•¼ ê°ì§€ìš© íŠ¸ë¦¬ê±° ë‹¨ì–´ (Law/Psych/Philosophy/History)\n",
    "    # ----------------------------------------------------\n",
    "    field_triggers = {\n",
    "        \"law\": [\"law\", \"legal\", \"crime\", \"contract\", \"tort\", \"jurisprudence\", \"homicide\", \"larceny\"],\n",
    "        \"psych\": [\"psychology\", \"behavior\", \"cognitive\", \"mental\", \"disorder\", \"personality\"],\n",
    "        \"phil\": [\"philosophy\", \"ethics\", \"moral\", \"kant\", \"aristotle\", \"epistemology\", \"metaphysics\", \"logic\"],\n",
    "        \"hist\": [\"history\", \"war\", \"revolution\", \"ancient\", \"empire\", \"dynasty\", \"ww1\", \"ww2\", \"cold war\"],\n",
    "    }\n",
    "\n",
    "    detected_fields = set()\n",
    "    for field, triggers in field_triggers.items():\n",
    "        for t in triggers:\n",
    "            if t in q:\n",
    "                detected_fields.add(field)\n",
    "                break\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 5) ëŒ€í‘œ Wikipedia ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë§¤í•‘\n",
    "    # ----------------------------------------------------\n",
    "    representative_docs = {\n",
    "        \"law\": [\n",
    "            \"Law\", \"Jurisprudence\", \"Criminal law\", \"Contract\", \"Constitutional law\",\n",
    "            \"International law\", \"Law of war\", \"Mens rea\", \"Larceny\"\n",
    "        ],\n",
    "        \"psych\": [\n",
    "            \"Psychology\", \"Cognitive psychology\", \"Behaviorism\",\n",
    "            \"Abnormal psychology\", \"Personality psychology\",\n",
    "            \"Bipolar disorder\", \"Anxiety disorder\", \"Classical conditioning\"\n",
    "        ],\n",
    "        \"phil\": [\n",
    "            \"Philosophy\", \"Ethics\", \"Deontological ethics\", \"Utilitarianism\",\n",
    "            \"Virtue ethics\", \"Epistemology\", \"Metaphysics\",\n",
    "            \"Immanuel Kant\", \"Aristotle\", \"Categorical imperative\"\n",
    "        ],\n",
    "        \"hist\": [\n",
    "            \"History\", \"World history\", \"French Revolution\", \"American Revolution\",\n",
    "            \"World War I\", \"World War II\", \"Cold War\", \"Industrial Revolution\",\n",
    "            \"History of Europe\", \"History of Asia\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for field in detected_fields:\n",
    "        keywords.extend(representative_docs[field])\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 6) ì˜ì–´ ê¸°ë°˜ ë¶„ì•¼ ë§¤í•‘ (ì¶”ê°€ ê°ì§€)\n",
    "    # ----------------------------------------------------\n",
    "    english_keywords = [\n",
    "        'Psychology', 'Education', 'Sociology', 'Statistics', 'Mathematics',\n",
    "        'Computer Science', 'Biology', 'Chemistry', 'Physics', 'Philosophy',\n",
    "        'History', 'Literature', 'Economics', 'Political Science', 'Law',\n",
    "        'Medicine', 'Nursing', 'Pharmacy', 'Art', 'Music'\n",
    "    ]\n",
    "\n",
    "    for keyword in english_keywords:\n",
    "        if keyword.lower() in q:\n",
    "            keywords.append(keyword)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 7) ì¤‘ë³µ ì œê±° & ê³ ìœ ëª…ì‚¬ ìš°ì„ ìˆœìœ„ ì •ë ¬\n",
    "    # ----------------------------------------------------\n",
    "    keywords = list(set(keywords))\n",
    "\n",
    "    def priority(k):\n",
    "        return 0 if k in proper_nouns else 1\n",
    "\n",
    "    keywords.sort(key=priority)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cfb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: 5)\n",
      "âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 50ê°œ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "â–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "\n",
      "Q1: QUESTION1) ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ í•˜ë ¤ë©´ í•™ê¸° ê°œì‹œì¼ë¡œë¶€í„° ë©°ì¹  ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì•¼í•˜ë‚˜ìš”?\n",
      "(...\n",
      "ì‘ë‹µ: ì •ë‹µ: (D) 90ì¼  \n",
      "\n",
      "ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ ì œ26ì¡° â‘¥í•­ì— ëª…ì‹œëœ ë‚´ìš©ì— ë”°ë¥´ë©´, ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ ì‹ ì²­í•  ê²½ìš° \"í•™ê¸°ê°œì‹œì¼ë¡œë¶€í„° 90ì¼ ì´ë‚´\"ì— ì‹ ì²­í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì •ë‹µì€ (D)ì…ë‹ˆë‹¤.  \n",
      "\n",
      "(D)...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: ì—†ìŒ)\n",
      "\n",
      "Q2: QUESTION2) 'ì¬ì…í•™ì€ aíšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ì œ 28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ...\n",
      "ì‘ë‹µ: ì •ë‹µ: (A)  \n",
      "ì¬ì…í•™ íšŸìˆ˜(a)ëŠ” \"1íšŒ\"ë¡œ ì œ30ì¡°ì œ2í•­ì— ëª…ì‹œë˜ì–´ ìˆê³ , ì œì  í›„ ì¬ì…í•™ ê°€ëŠ¥ ê¸°ê°„(b)ì€ ì œ28ì¡°ì œ4í˜¸ ê´€ë ¨ ê·œì •ì—ì„œ \"1ë…„\"ìœ¼ë¡œ ì œì‹œë©ë‹ˆë‹¤. ë”°ë¼ì„œ a+b = 1+1 = 2ë¡œ (A)ê°€ ì •ë‹µì…ë‹ˆë‹¤.\n",
      "\n",
      "(A)...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: ì—†ìŒ)\n",
      "\n",
      "Q3: QUESTION3) í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ ëª‡í•™ì  ì´ìƒ ...\n",
      "ì‘ë‹µ: ì •ë‹µ: (C) 21í•™ì \n",
      "\n",
      "Contextì˜ \"6.23) â‘¢ í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ 21í•™ì  ì´ìƒ ì·¨ë“í•œ ë•Œì—ëŠ” ë¶€ì „ê³µì„ ì´ìˆ˜í•œ ê²ƒìœ¼ë¡œ ì¸ì •í•œë‹¤.\"ì— ë”°ë¼ ì •ë‹µì€ (C)ì…ë‹ˆë‹¤....\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: ì—†ìŒ)\n",
      "\n",
      "Q4: QUESTION4) ë‹¤ìŒ ë³´ê¸°ì˜ í•™ìƒë“¤ ì¤‘ ì œì ì„ ë‹¹í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒì„ ê³ ë¥´ë©´?\n",
      "(A) íŒœ : ì§•ê³„ì— ì˜í•´ í‡´...\n",
      "ì‘ë‹µ: ì •ë‹µ: (D)  \n",
      "ì œì  ì‚¬ìœ  ê´€ë ¨ ê·œì •ì— ë”°ë¼:  \n",
      "- (A) í‡´í•™ì²˜ë¶„ì€ ì œì ì— í•´ë‹¹  \n",
      "- (B) í‰ì í‰ê·  1.2ë¡œ ì—°ì† 3íšŒ í•™ì‚¬ê²½ê³  ì‹œ ì œì  (í•™ì¹™ ì œ4ì¡° â‘£í•­)  \n",
      "- (C) ìˆ˜ì—…ë£Œ ë¯¸ë‚©ì€ ì œì  ì‚¬ìœ  (í•™ì¹™ ì œ5ì¡° â‘ í•­)  \n",
      "- (D) \"ì •ë‹¹í•œ ì´ìœ \"ë¡œ ë³µí•™í•˜ì§€...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: ì—†ìŒ)\n",
      "\n",
      "Q5: QUESTION5) 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ ëª‡ ëª…ì¸ê°€? \n",
      "(A) 90ëª… \n",
      "(B) 1...\n",
      "ì‘ë‹µ: ì •ë‹µ: (C) \n",
      "\n",
      "ì»¨í…ìŠ¤íŠ¸ì—ì„œ 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ \"110ëª…\"ìœ¼ë¡œ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì„ íƒì§€ (C)ì— í•´ë‹¹í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í•™ë¶€/í•™ê³¼ì˜ ì •ë³´ëŠ” ë³¸ ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ë¯€ë¡œ ë°°ì œí•©ë‹ˆë‹¤. \n",
      "\n",
      "ìµœì¢… ë‹µë³€: (C)...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ 5ê°œ, Wikipedia 0ê°œ (í‚¤ì›Œë“œ: ì—†ìŒ)\n",
      "\n",
      "âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ê²½ë¡œ\n",
    "TESTSET_PATH = CURRENT_DIR / \"testset.csv\"\n",
    "TOP_K = 5  # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì¦ê°€\n",
    "\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "# LLM ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "llm = ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro2\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    \n",
    "    **ì¤‘ìš” ì§€ì¹¨:**\n",
    "    1. ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš” (êµ¬ì²´ì ì¸ ê·œì •)\n",
    "    2. Wikipedia ë¬¸ì„œëŠ” ì¼ë°˜ì ì¸ í•™ë¬¸ ì§€ì‹ ì°¸ê³ ìš©ì…ë‹ˆë‹¤\n",
    "    3. ì •í™•íˆ í•˜ë‚˜ì˜ ì„ íƒì§€ë§Œ ë‹µë³€í•˜ì„¸ìš”\n",
    "    4. ë‹µë³€ í˜•ì‹: \"ì •ë‹µ: (X)\" ë˜ëŠ” \"(X)\" í˜•ì‹ìœ¼ë¡œ ë‹µë³€\n",
    "    \n",
    "    \n",
    "    \n",
    "    ì§ˆë¬¸ì´ ì„ íƒì§€ í˜•ì‹ì¸ ê²½ìš°, ì •ë‹µ ì„ íƒì§€ì˜ ë¬¸ì(ì˜ˆ: (A), (B), (C), (D), (E), (I), (J), (G), (H) ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.\n",
    "    \n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "print(f\"âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: {TOP_K})\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ\n",
    "def read_data(file_path: Path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data[\"prompts\"], data.get(\"answers\")\n",
    "\n",
    "try:\n",
    "    prompts, answers = read_data(TESTSET_PATH)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ ë¡œë“œ ì™„ë£Œ\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ {TESTSET_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    prompts, answers = [], None\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²˜ìŒ 5ê°œë§Œ)\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"\\nâ–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompts[:5], 1):\n",
    "        # 1. ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰\n",
    "        source_docs = retriever.invoke(prompt)\n",
    "       \n",
    "\n",
    "        # 2. ì§ˆë¬¸ì—ì„œ Wikipedia í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê²€ìƒ‰ (ë‚˜í˜„ë‹˜ ë°©ì‹)\n",
    "        wiki_keywords = extract_wikipedia_keywords(prompt)\n",
    "        wiki_docs = []\n",
    "        if wiki_keywords:\n",
    "            for keyword in wiki_keywords:\n",
    "                try:\n",
    "                    keyword_docs = retrieve_wikipedia_docs(keyword, lang='en')\n",
    "                    wiki_docs.extend(keyword_docs)\n",
    "                except Exception as e:\n",
    "                    # ê°œë³„ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰\n",
    "                    pass\n",
    "        \n",
    "        # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context_parts = []\n",
    "        if source_docs:\n",
    "            context_parts.append(\"=== ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in source_docs))\n",
    "        if wiki_docs:\n",
    "            context_parts.append(\"=== Wikipedia ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in wiki_docs))\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "        \n",
    "        # 4. ì‘ë‹µ ìƒì„±\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "        \n",
    "        print(f\"Q{i}: {prompt[:60]}...\")\n",
    "        print(f\"ì‘ë‹µ: {response.content[:150]}...\")\n",
    "        print(f\"ê²€ìƒ‰ ë¬¸ì„œ: ì´í™”ì—¬ëŒ€ {len(source_docs)}ê°œ, Wikipedia {len(wiki_docs)}ê°œ (í‚¤ì›Œë“œ: {', '.join(wiki_keywords) if wiki_keywords else 'ì—†ìŒ'})\\n\")\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d5b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\n",
      "\n",
      "ğŸ“‹ ì´ 50ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\n",
      "\n",
      "ğŸ”„ Q1/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q2/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q3/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q4/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q5/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q6/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q7/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q8/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q9/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q10/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 10/50 (20.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q11/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q12/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q13/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q14/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: A, Gold: (C) (C)\n",
      "ğŸ”„ Q15/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q16/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q17/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q18/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q19/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q20/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "  â””â”€ ì§„í–‰ë¥ : 20/50 (40.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q21/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q22/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q23/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q24/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: C, Gold: (D) (D)\n",
      "ğŸ”„ Q25/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: A, Gold: (B) (B)\n",
      "ğŸ”„ Q26/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: D, Gold: (D) (D)\n",
      "ğŸ”„ Q27/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: A, Gold: (A) (A)\n",
      "ğŸ”„ Q28/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "ğŸ”„ Q29/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: B, Gold: (I) (I)\n",
      "ğŸ”„ Q30/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "  â””â”€ ì§„í–‰ë¥ : 30/50 (60.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q31/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q32/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: G, Gold: (C) (C)\n",
      "ğŸ”„ Q33/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: I, Gold: (I) (I)\n",
      "ğŸ”„ Q34/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: G, Gold: (E) (E)\n",
      "ğŸ”„ Q35/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "ğŸ”„ Q36/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: J, Gold: (J) (J)\n",
      "ğŸ”„ Q37/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q38/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: G, Gold: (D) (D)\n",
      "ğŸ”„ Q39/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: D, Gold: (G) (G)\n",
      "ğŸ”„ Q40/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (I) (I)\n",
      "  â””â”€ ì§„í–‰ë¥ : 40/50 (80.0%)\n",
      "----------------------------------------\n",
      "ğŸ”„ Q41/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: E, Gold: (E) (E)\n",
      "ğŸ”„ Q42/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (D) (D)\n",
      "ğŸ”„ Q43/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: C, Gold: (C) (C)\n",
      "ğŸ”„ Q44/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: C, Gold: (F) (F)\n",
      "ğŸ”„ Q45/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (J) (J)\n",
      "ğŸ”„ Q46/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: I, Gold: (I) (I)\n",
      "ğŸ”„ Q47/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: G, Gold: (G) (G)\n",
      "ğŸ”„ Q48/50 ì²˜ë¦¬ ì¤‘... âŒ Pred: E, Gold: (H) (H)\n",
      "ğŸ”„ Q49/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: G, Gold: (G) (G)\n",
      "ğŸ”„ Q50/50 ì²˜ë¦¬ ì¤‘... âœ… Pred: B, Gold: (B) (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 50/50 (100.0%)\n",
      "----------------------------------------\n",
      "\n",
      "âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: c:\\Users\\user\\-NLP-RAG-Project\\rag_results_combined.csv\n",
      "\n",
      "ğŸ“Š ì •í™•ë„: 37/50 (74.0%)\n",
      "   - ì •ë‹µ: 37ê°œ\n",
      "   - ì˜¤ë‹µ: 13ê°œ\n",
      "   - ë¯¸ì‘ë‹µ: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 5. ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ (ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ ë°©ì‹ ê²°í•©)\n",
    "# ==================================================================\n",
    "\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ ê°œì„  ë²„ì „ (ëª¨ë“  ì•ŒíŒŒë²³ ì„ íƒì§€ ì§€ì›: A-Z)\n",
    "    - ì—¬ëŸ¬ ì„ íƒì§€ê°€ ìˆì„ ê²½ìš° ê°€ì¥ í™•ì‹¤í•œ ê²ƒ ì„ íƒ\n",
    "    - \"ì •ë‹µ:\", \"Answer:\", \"ë‹µ:\" ë“±ì˜ í‚¤ì›Œë“œ ìš°ì„ \n",
    "    \"\"\"\n",
    "    # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì ì¸ ì •ë‹µ í‘œì‹œ (í•œêµ­ì–´/ì˜ì–´)\n",
    "    explicit_patterns = [\n",
    "        r\"ì •ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"Answer[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"\\[ANSWER\\]:\\s*\\(([A-Z])\\)\",\n",
    "        r\"ìµœì¢…\\s*ë‹µ[:\\s]*\\(([A-Z])\\)\",\n",
    "        r\"Final\\s*Answer[:\\s]*\\(([A-Z])\\)\",\n",
    "    ]\n",
    "    for pattern in explicit_patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 2: (X) í˜•ì‹ (ë§ˆì§€ë§‰ ë°œìƒ) - ëª¨ë“  ì•ŒíŒŒë²³ ì§€ì›\n",
    "    # ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ (ë³´í†µ ìµœì¢… ë‹µë³€)\n",
    "    pattern2 = r\"\\(([A-Z])\\)\"\n",
    "    matches = list(re.finditer(pattern2, response))\n",
    "    if matches:\n",
    "        return matches[-1].group(1)\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 3: ë‹¨ë… ë¬¸ì (ë§ˆì§€ë§‰ ë°œìƒ) - ëª¨ë“  ì•ŒíŒŒë²³ ì§€ì›\n",
    "    # ë‹¨ì–´ ê²½ê³„ì—ì„œ ì•ŒíŒŒë²³ë§Œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)\n",
    "    pattern3 = r\"\\b([A-Z])\\b\"\n",
    "    matches = list(re.finditer(pattern3, response, re.IGNORECASE))\n",
    "    if matches:\n",
    "        return matches[-1].group(1).upper()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ì „ì²´ í‰ê°€ ì‹¤í–‰\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\\n\")\n",
    "    print(f\"ğŸ“‹ ì´ {len(prompts)}ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\\n\")\n",
    "    \n",
    "    all_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        try:\n",
    "            print(f\"ğŸ”„ Q{i}/{len(prompts)} ì²˜ë¦¬ ì¤‘...\", end=\" \")\n",
    "            \n",
    "            # 1. ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰\n",
    "            source_docs = retriever.invoke(prompt)\n",
    "            \n",
    "            # 2. ì§ˆë¬¸ì—ì„œ Wikipedia í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê²€ìƒ‰ (ë‚˜í˜„ë‹˜ ë°©ì‹)\n",
    "            wiki_keywords = extract_wikipedia_keywords(prompt)\n",
    "            wiki_docs = []\n",
    "            if wiki_keywords:\n",
    "                for keyword in wiki_keywords:\n",
    "                    try:\n",
    "                        keyword_docs = retrieve_wikipedia_docs(keyword, lang='en')\n",
    "                        wiki_docs.extend(keyword_docs)\n",
    "                    except Exception as e:\n",
    "                        # ê°œë³„ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰\n",
    "                        pass\n",
    "                \n",
    "                # Wikipedia ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ìœ¼ë©´ í˜¼ë€)\n",
    "                if len(wiki_docs) > 10:\n",
    "                    wiki_docs = wiki_docs[:10]  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©\n",
    "            \n",
    "            # 3. ê¸°ì¡´ ë¬¸ì„œì™€ Wikipedia ë¬¸ì„œ í†µí•©\n",
    "            all_source_docs = source_docs + wiki_docs\n",
    "            \n",
    "            # 4. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            context_parts = []\n",
    "            if source_docs:\n",
    "                context_parts.append(\"=== ì´í™”ì—¬ëŒ€ í•™ì¹™ ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in source_docs))\n",
    "            if wiki_docs:\n",
    "                context_parts.append(\"=== Wikipedia ë¬¸ì„œ ===\\n\" + \"\\n\\n\".join(doc.page_content for doc in wiki_docs))\n",
    "            \n",
    "            context_text = \"\\n\\n\".join(context_parts) or \"\"\n",
    "            \n",
    "            # 5. ì‘ë‹µ ìƒì„±\n",
    "            response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "            \n",
    "            pred_raw = response.content.strip()\n",
    "            pred = extract_answer(pred_raw)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            all_responses.append({\n",
    "                \"question_num\": i,\n",
    "                \"question\": prompt[:100],\n",
    "                \"prediction\": pred,\n",
    "                \"response\": pred_raw[:200],\n",
    "                \"context_count\": len(source_docs),\n",
    "                \"wikipedia_count\": len(wiki_docs),\n",
    "                \"wikipedia_keywords\": \", \".join(wiki_keywords) if wiki_keywords else \"\"\n",
    "            })\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ëª¨ë“  ë¬¸ì œì— ëŒ€í•´)\n",
    "            gold = answers.iloc[i-1] if answers is not None else \"N/A\"\n",
    "            # Gold ë‹µë³€ì—ì„œ ì„ íƒì§€ ë¬¸ì ì¶”ì¶œ (ì˜ˆ: (D) -> D)\n",
    "            gold_match = re.search(r\"\\(([A-Z])\\)\", str(gold))\n",
    "            gold_letter = gold_match.group(1) if gold_match else str(gold).strip()\n",
    "            \n",
    "            # ì •ë‹µ ì—¬ë¶€ í™•ì¸ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)\n",
    "            is_correct = pred and str(pred).upper() == str(gold_letter).upper()\n",
    "            status = \"âœ…\" if is_correct else \"âŒ\" if pred else \"âš ï¸\"\n",
    "            print(f\"{status} Pred: {pred}, Gold: {gold} ({gold_letter})\")\n",
    "            \n",
    "            # 10ê°œë§ˆë‹¤ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "            if i % 10 == 0 or i == len(prompts):\n",
    "                print(f\"  â””â”€ ì§„í–‰ë¥ : {i}/{len(prompts)} ({i/len(prompts)*100:.1f}%)\")\n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = str(e)\n",
    "            error_trace = traceback.format_exc()\n",
    "            print(f\"âŒ ì—ëŸ¬ ë°œìƒ: {error_msg}\")\n",
    "            print(f\"  â””â”€ Q{i} ì²˜ë¦¬ ì‹¤íŒ¨, Noneìœ¼ë¡œ ì €ì¥\")\n",
    "            if i == 43:  # 43ë²ˆ ë¬¸ì œ ë””ë²„ê¹…\n",
    "                print(f\"  â””â”€ ìƒì„¸ ì—ëŸ¬:\\n{error_trace}\")\n",
    "            predictions.append(None)\n",
    "            all_responses.append({\n",
    "                \"question_num\": i,\n",
    "                \"question\": prompt[:100] if prompt else \"\",\n",
    "                \"prediction\": None,\n",
    "                \"response\": f\"Error: {error_msg}\",\n",
    "                \"context_count\": 0,\n",
    "                \"wikipedia_count\": 0,\n",
    "                \"wikipedia_keywords\": \"\"\n",
    "            })\n",
    "            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰\n",
    "            continue\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results_df = pd.DataFrame(all_responses)\n",
    "    results_path = CURRENT_DIR / \"rag_results_combined.csv\"\n",
    "    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}\")\n",
    "    \n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    if answers is not None:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (pred, gold) in enumerate(zip(predictions, answers)):\n",
    "            if pred is None:\n",
    "                continue\n",
    "            gold_match = re.search(r\"\\(([A-Z])\\)\", str(gold))\n",
    "            gold_letter = gold_match.group(1) if gold_match else str(gold).strip()\n",
    "            if str(pred).upper() == str(gold_letter).upper():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        if total > 0:\n",
    "            accuracy = correct / total * 100\n",
    "            print(f\"\\nğŸ“Š ì •í™•ë„: {correct}/{total} ({accuracy:.1f}%)\")\n",
    "            print(f\"   - ì •ë‹µ: {correct}ê°œ\")\n",
    "            print(f\"   - ì˜¤ë‹µ: {total - correct}ê°œ\")\n",
    "            print(f\"   - ë¯¸ì‘ë‹µ: {len(predictions) - total}ê°œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
