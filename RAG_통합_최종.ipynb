{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae73e88",
   "metadata": {},
   "source": [
    "# RAG í†µí•© ìµœì¢… ë²„ì „\n",
    "## ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì„¸ ë¶„ì˜ ì‘ì—…ì„ í†µí•©í•œ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤:\n",
    "- **ë‹¤í˜„ë‹˜ ì‘ì—…**: ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬, í•™ìœ„/ì •ì› ë¬¸ì¥ ìƒì„±, ë³¸ë¬¸ ì²­í‚¹\n",
    "- **ë‚˜í˜„ë‹˜ ì‘ì—…**: UpstageDocumentParseLoader í™œìš© (ì°¸ê³ )\n",
    "- **ì‚¬ìš©ì ì‘ì—…**: í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„°í™”\n",
    "\n",
    "### í†µí•© ì „ëµ\n",
    "1. PDF ë³¸ë¬¸ê³¼ ë¶€ì¹™ì„ ë¶„ë¦¬í•˜ì—¬ ì²­í‚¹\n",
    "2. CSV í‘œ ë°ì´í„°ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "3. ëª¨ë“  ë¬¸ì„œë¥¼ FAISS ë²¡í„° ìŠ¤í† ì–´ì— í†µí•©\n",
    "4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° í‰ê°€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75a63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‘ì—… ë””ë ‰í† ë¦¬: c:\\Users\\janen\\Documents\\25-2 ê°•ì˜\\ìì—°ì–´ì²˜ë¦¬\\í”„ë¡œì íŠ¸\\ewha\n",
      "âœ… Upstage API Key: up_EoF0I0CzeHxuDYmf0...\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Upstage API Key ì„¤ì •\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"up_EoF0I0CzeHxuDYmf0we544GMPCFIT\")\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "CURRENT_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "if Path.cwd() != CURRENT_DIR:\n",
    "    os.chdir(CURRENT_DIR)\n",
    "\n",
    "print(f\"âœ… ì‘ì—… ë””ë ‰í† ë¦¬: {CURRENT_DIR}\")\n",
    "print(f\"âœ… Upstage API Key: {UPSTAGE_API_KEY[:20]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31860c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_upstage in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.3.35)\n",
      "Requirement already satisfied: openai in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (1.107.2)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.78 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (0.3.79)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (4.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21.0,>=0.20.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_upstage) (0.20.3)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (6.0.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.78->langchain_upstage) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<0.4.0,>=0.3.78->langchain_upstage) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tokenizers<0.21.0,>=0.20.0->langchain_upstage) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain_upstage) (2025.10.0)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\janen\\appdata\\local\\programs\\python\\nlp_env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install langchain_upstage langchain_community langchain-openai openai pdfplumber pandas faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "303f6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "âœ… ì´ 38í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“Œ ë¶€ì¹™ ì‹œì‘ ì¸ë±ìŠ¤: 20919\n",
      "ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: 20919\n",
      "ğŸ“„ ë¶€ì¹™ ê¸¸ì´: 20222\n",
      "âœ… ë³¸ë¬¸ ì²­í¬ ìˆ˜: 23\n",
      "âœ… ë¶€ì¹™ ì²­í¬ ìˆ˜: 23\n",
      "\n",
      "ğŸ“¦ ì´ í…ìŠ¤íŠ¸ ì²­í¬: 46ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 1. PDF ë³¸ë¬¸/ë¶€ì¹™ ë¶„ë¦¬ ë° ì²­í‚¹ (ë‹¤í˜„ë‹˜ ì‘ì—… ê¸°ë°˜)\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "PDF_PATH = CURRENT_DIR / \"ewha.pdf\"\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{PDF_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ğŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()\n",
    "docs = [d for d in docs if int(d.metadata[\"page\"]) < 38]  # ë¶€ì¹™ ì´ì „ í˜ì´ì§€ë§Œ\n",
    "print(f\"âœ… ì´ {len(docs)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ì „ì²´ í…ìŠ¤íŠ¸ ë³‘í•©\n",
    "full_text = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# \"ë¶€ì¹™\" ë“±ì¥ ì§€ì  ì°¾ê¸°\n",
    "split_index = full_text.find(\"ë¶€ì¹™\")\n",
    "print(f\"ğŸ“Œ ë¶€ì¹™ ì‹œì‘ ì¸ë±ìŠ¤: {split_index}\")\n",
    "\n",
    "# ë³¸ë¬¸ / ë¶€ì¹™ ë¶„ë¦¬\n",
    "if split_index != -1:\n",
    "    main_text = full_text[:split_index]\n",
    "    appendix_text = full_text[split_index:]\n",
    "else:\n",
    "    main_text = full_text\n",
    "    appendix_text = \"\"\n",
    "\n",
    "print(f\"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(main_text)}\")\n",
    "print(f\"ğŸ“„ ë¶€ì¹™ ê¸¸ì´: {len(appendix_text)}\")\n",
    "\n",
    "# ë³¸ë¬¸ ì²­í‚¹ (ë‹¤í˜„ë‹˜ ì„¤ì • ì‚¬ìš©)\n",
    "main_doc = [Document(page_content=main_text)]\n",
    "text_splitter_main = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\"ë³„í‘œ\", \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    ")\n",
    "main_chunks = text_splitter_main.split_documents(main_doc)\n",
    "print(f\"âœ… ë³¸ë¬¸ ì²­í¬ ìˆ˜: {len(main_chunks)}\")\n",
    "\n",
    "# ë¶€ì¹™ ì²­í‚¹\n",
    "if appendix_text:\n",
    "    appendix_doc = [Document(page_content=appendix_text)]\n",
    "    text_splitter_appendix = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"ë¶€ì¹™\", \"ë³„í‘œ\", \"\\n\\nì œ\", \"\\nì œ\", \"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    )\n",
    "    appendix_chunks = text_splitter_appendix.split_documents(appendix_doc)\n",
    "    print(f\"âœ… ë¶€ì¹™ ì²­í¬ ìˆ˜: {len(appendix_chunks)}\")\n",
    "else:\n",
    "    appendix_chunks = []\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ í…ìŠ¤íŠ¸ ì²­í¬: {len(main_chunks) + len(appendix_chunks)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae83ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… degrees.csv ì²˜ë¦¬ ì™„ë£Œ: 2ê°œ ë¬¸ì¥\n",
      "âœ… 2019_quota.csv ì²˜ë¦¬ ì™„ë£Œ: 79ê°œ ë¬¸ì„œ\n",
      "âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: 1ê°œ ë¬¸ì„œ\n",
      "\n",
      "ğŸ“¦ ì´ CSV ë¬¸ì„œ: 82ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 2. CSV í‘œ ë°ì´í„° ì²˜ë¦¬ ë° ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ + ì‚¬ìš©ì ì‘ì—… í†µí•©)\n",
    "# ==================================================================\n",
    "\n",
    "def coalesce(*values: object) -> str:\n",
    "    \"\"\"ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’ì„ ë°˜í™˜\"\"\"\n",
    "    for value in values:\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            continue\n",
    "        text = str(value).strip()\n",
    "        if text:\n",
    "            return text\n",
    "    return \"\"\n",
    "\n",
    "# ë‹¤í˜„ë‹˜ ì‘ì—…: í•™ìœ„ ë¬¸ì¥ ìƒì„± ë°©ì‹\n",
    "def build_degree_sentences_from_csv(df: pd.DataFrame, year: str = \"ìµœì‹  ê°œì •\") -> List[str]:\n",
    "    \"\"\"CSVì—ì„œ í•™ìœ„ ë¬¸ì¥ ìƒì„± (ë‹¤í˜„ë‹˜ ë°©ì‹)\"\"\"\n",
    "    sentences = []\n",
    "    for idx, row in df.iterrows():\n",
    "        college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"), row.get(\"ëŒ€í•™\"))\n",
    "        degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"), row.get(\"í•™ìœ„ì¢…ë¥˜\"), row.get(\"í•™ì‚¬\"))\n",
    "        major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"), row.get(\"ì „ê³µ\"))\n",
    "        \n",
    "        if not college or not degree or not major:\n",
    "            continue\n",
    "            \n",
    "        sentence = f\"{college}ì˜ {major} ì „ê³µì€ {degree} í•™ìœ„ë¥¼ ìˆ˜ì—¬í•œë‹¤. ({year})\"\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# ì‚¬ìš©ì ì‘ì—…: í‘œ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def build_quota_text(row: pd.Series) -> str:\n",
    "    \"\"\"ì…í•™ì •ì› í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    year = coalesce(row.get(\"í•™ë…„ë„\"), \"2019í•™ë…„ë„\")\n",
    "    college = coalesce(row.get(\"ëŒ€í•™\"))\n",
    "    faculty = coalesce(row.get(\"í•™ë¶€\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"), row.get(\"í•™ê³¼ ë˜ëŠ” ì „ê³µ\"))\n",
    "    quota = coalesce(row.get(\"ì •ì›\"), row.get(\"ì…í•™ì •ì›_ëª…\")) or \"ë¯¸ìƒ\"\n",
    "    \n",
    "    prefix_parts = [year]\n",
    "    if college:\n",
    "        prefix_parts.append(college)\n",
    "    if faculty:\n",
    "        prefix_parts.append(faculty)\n",
    "    prefix = \" \".join(prefix_parts)\n",
    "    return f\"{prefix} ì†Œì† {major}ì˜ ì…í•™ ì •ì›ì€ {quota}ëª…ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "def build_degree_text(row: pd.Series) -> str:\n",
    "    \"\"\"í•™ìœ„ í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    sentence = f\"{college} ì†Œì† {major} ì „ê³µìì—ê²Œ ìˆ˜ì—¬í•˜ëŠ” í•™ìœ„ëŠ” {degree}ì…ë‹ˆë‹¤.\"\n",
    "    extras = []\n",
    "    if quota:\n",
    "        extras.append(f\"ì…í•™ ì •ì›ì€ {quota}ëª…\")\n",
    "    if period:\n",
    "        extras.append(f\"ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„ì€ {period}\")\n",
    "    if extras:\n",
    "        sentence += \" \" + \", \".join(extras) + \"ì…ë‹ˆë‹¤.\"\n",
    "    return sentence\n",
    "\n",
    "def build_contract_text(row: pd.Series) -> str:\n",
    "    \"\"\"ê³„ì•½í•™ê³¼ í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°©ì‹)\"\"\"\n",
    "    college = coalesce(row.get(\"ì„¤ì¹˜ëŒ€í•™\"))\n",
    "    form = coalesce(row.get(\"ì„¤ì¹˜í˜•íƒœ\"))\n",
    "    major = coalesce(row.get(\"í•™ê³¼_ì „ê³µ\"))\n",
    "    degree = coalesce(row.get(\"í•™ìœ„_ì¢…ë¥˜\"))\n",
    "    quota = coalesce(row.get(\"ì…í•™ì •ì›_ëª…\"))\n",
    "    period = coalesce(row.get(\"ì„¤ì¹˜_ìš´ì˜ê¸°ê°„\"))\n",
    "    \n",
    "    parts = [f\"ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜ ì •ë³´: ì„¤ì¹˜ëŒ€í•™={college}\"]\n",
    "    if form:\n",
    "        parts.append(f\"ì„¤ì¹˜í˜•íƒœ={form}\")\n",
    "    if major:\n",
    "        parts.append(f\"í•™ê³¼/ì „ê³µ={major}\")\n",
    "    if degree:\n",
    "        parts.append(f\"ìˆ˜ì—¬ í•™ìœ„={degree}\")\n",
    "    if quota:\n",
    "        parts.append(f\"ì…í•™ ì •ì›={quota}ëª…\")\n",
    "    if period:\n",
    "        parts.append(f\"ì„¤ì¹˜Â·ìš´ì˜ ê¸°ê°„={period}\")\n",
    "    return \", \".join(parts) + \".\"\n",
    "\n",
    "# CSV íŒŒì¼ ì²˜ë¦¬\n",
    "csv_documents = []\n",
    "\n",
    "# 1) degrees.csv ì²˜ë¦¬ (ë‹¤í˜„ë‹˜ ë°©ì‹ + ì‚¬ìš©ì ë°©ì‹ ê²°í•©)\n",
    "degrees_path = CURRENT_DIR / \"degrees.csv\"\n",
    "if degrees_path.exists():\n",
    "    df_degrees = pd.read_csv(degrees_path, encoding='utf-8-sig')\n",
    "    # ë‹¤í˜„ë‹˜ ë°©ì‹: í•™ìœ„ ë¬¸ì¥ ìƒì„±\n",
    "    degree_sentences = build_degree_sentences_from_csv(df_degrees, \"ìµœì‹  ê°œì •\")\n",
    "    for sentence in degree_sentences:\n",
    "        csv_documents.append(Document(\n",
    "            page_content=sentence,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 2] í•™ì‚¬í•™ìœ„ì˜ ì¢…ë¥˜\", \"page\": 51, \"type\": \"degree\"}\n",
    "        ))\n",
    "    print(f\"âœ… degrees.csv ì²˜ë¦¬ ì™„ë£Œ: {len(degree_sentences)}ê°œ ë¬¸ì¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ degrees.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2) 2019_quota.csv ì²˜ë¦¬\n",
    "quota_path = CURRENT_DIR / \"2019_quota.csv\"\n",
    "if quota_path.exists():\n",
    "    df_quota = pd.read_csv(quota_path, encoding='utf-8-sig')\n",
    "    for _, row in df_quota.iterrows():\n",
    "        text = build_quota_text(row)\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 1] 2019í•™ë…„ë„ ì…í•™ì •ì›\", \"page\": 39, \"type\": \"quota\"}\n",
    "        ))\n",
    "    print(f\"âœ… 2019_quota.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_quota)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ 2019_quota.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3) contract_dept.csv ì²˜ë¦¬\n",
    "contract_path = CURRENT_DIR / \"contract_dept.csv\"\n",
    "if contract_path.exists():\n",
    "    df_contract = pd.read_csv(contract_path, encoding='utf-8-sig')\n",
    "    for _, row in df_contract.iterrows():\n",
    "        text = build_contract_text(row)\n",
    "        csv_documents.append(Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": \"[ë³„í‘œ 3] ê³„ì•½í•™ê³¼ ì„¤ì¹˜Â·ìš´ì˜\", \"page\": 53, \"type\": \"contract\"}\n",
    "        ))\n",
    "    print(f\"âœ… contract_dept.csv ì²˜ë¦¬ ì™„ë£Œ: {len(df_contract)}ê°œ ë¬¸ì„œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ contract_dept.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ì´ CSV ë¬¸ì„œ: {len(csv_documents)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6ba918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ§¹ í•„í„°ë§ í›„ ìœ íš¨ ë¬¸ì„œ ìˆ˜: 128\n",
      "\n",
      "ğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„:\n",
      "  - main_text: 23ê°œ\n",
      "  - appendix_text: 23ê°œ\n",
      "  - degree: 2ê°œ\n",
      "  - quota: 79ê°œ\n",
      "  - contract: 1ê°œ\n",
      "ğŸ“‚ Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
      "ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: C:\\Users\\janen\\AppData\\Local\\Temp\\rag_ewha_vectorstore\n",
      "\n",
      "ğŸ“¦ ì´ 128ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°í™” ì¤‘ì…ë‹ˆë‹¤...\n",
      "âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ\n",
      "\n",
      "ğŸ¯ ì´ KB ë¬¸ì„œ ìˆ˜: 128ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 3. ëª¨ë“  ë¬¸ì„œ í†µí•© ë° FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key=UPSTAGE_API_KEY,\n",
    "    model=\"solar-embedding-1-large\"\n",
    ")\n",
    "print(\"âœ… Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì„œ í†µí•©\n",
    "all_documents = main_chunks + appendix_chunks + csv_documents\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë³¸ë¬¸/ë¶€ì¹™ ì²­í¬)\n",
    "for i, doc in enumerate(main_chunks):\n",
    "    if not hasattr(doc, 'metadata') or not doc.metadata:\n",
    "        doc.metadata = {}\n",
    "    doc.metadata[\"source\"] = \"ewha.pdf ë³¸ë¬¸\"\n",
    "    doc.metadata[\"type\"] = \"main_text\"\n",
    "\n",
    "for i, doc in enumerate(appendix_chunks):\n",
    "    if not hasattr(doc, 'metadata') or not doc.metadata:\n",
    "        doc.metadata = {}\n",
    "    doc.metadata[\"source\"] = \"ewha.pdf ë¶€ì¹™\"\n",
    "    doc.metadata[\"type\"] = \"appendix_text\"\n",
    "\n",
    "# ìœ íš¨í•œ ë¬¸ì„œë§Œ í•„í„°ë§\n",
    "valid_documents = [doc for doc in all_documents if doc.page_content and doc.page_content.strip()]\n",
    "print(f\"ğŸ§¹ í•„í„°ë§ í›„ ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(valid_documents)}\")\n",
    "\n",
    "# ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„\n",
    "doc_types = {}\n",
    "for doc in valid_documents:\n",
    "    doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "\n",
    "print(\"\\nğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„:\")\n",
    "for doc_type, count in doc_types.items():\n",
    "    print(f\"  - {doc_type}: {count}ê°œ\")\n",
    "\n",
    "# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "# Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì˜ì–´ ê²½ë¡œë§Œ ìˆëŠ” ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "import platform\n",
    "import tempfile\n",
    "\n",
    "# Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ë‚˜ ì˜ì–´ ê²½ë¡œ ì‚¬ìš©\n",
    "if platform.system() == 'Windows':\n",
    "    # ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ TEMP ì‚¬ìš© (ë³´í†µ C:\\Users\\USERNAME\\AppData\\Local\\Temp)\n",
    "    temp_dir = Path(tempfile.gettempdir())\n",
    "    # í”„ë¡œì íŠ¸ë³„ ê³ ìœ í•œ ë””ë ‰í† ë¦¬ëª… ìƒì„±\n",
    "    vector_db_base = temp_dir / \"rag_ewha_vectorstore\"\n",
    "    vector_db_base.mkdir(parents=True, exist_ok=True)\n",
    "    vector_db_path = str(vector_db_base.resolve())\n",
    "    print(f\"ğŸ“‚ Windows í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°: ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©\")\n",
    "    print(f\"ğŸ“‚ ë²¡í„° DB ê²½ë¡œ: {vector_db_path}\")\n",
    "else:\n",
    "    # Windowsê°€ ì•„ë‹Œ ê²½ìš° ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©\n",
    "    VECTOR_DB_DIR = CURRENT_DIR / \"vectorstore\"\n",
    "    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    vector_db_path = str(VECTOR_DB_DIR.resolve())\n",
    "\n",
    "vector_store = None\n",
    "vector_db_dir = Path(vector_db_path)\n",
    "if vector_db_dir.exists() and (vector_db_dir / \"index.faiss\").exists():\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(\n",
    "            folder_path=vector_db_path,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        print(f\"\\nğŸ“‚ ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as exc:\n",
    "        print(f\"âš ï¸ ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {exc}. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        vector_store = None\n",
    "\n",
    "if vector_store is None:\n",
    "    print(f\"\\nğŸ“¦ ì´ {len(valid_documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°í™” ì¤‘ì…ë‹ˆë‹¤...\")\n",
    "    vector_store = FAISS.from_documents(valid_documents, embeddings)\n",
    "    # ì˜ì–´ ê²½ë¡œ ì‚¬ìš© (í•œê¸€ ê²½ë¡œ ë¬¸ì œ í•´ê²°)\n",
    "    vector_store.save_local(vector_db_path)\n",
    "    print(f\"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì´ KB ë¬¸ì„œ ìˆ˜: {len(valid_documents)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5cfb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: 5)\n",
      "âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 50ê°œ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "â–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "\n",
      "Q1: QUESTION1) ì¬í•™ ì¤‘ì¸ í•™ìƒì´ íœ´í•™ì„ í•˜ë ¤ë©´ í•™ê¸° ê°œì‹œì¼ë¡œë¶€í„° ë©°ì¹  ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì•¼í•˜ë‚˜ìš”?\n",
      "(...\n",
      "ì‘ë‹µ: (D) 90ì¼  \n",
      "\n",
      "**Answer:** (D) 90ì¼  \n",
      "\n",
      "**Context ê·¼ê±°:**  \n",
      "ì œ26ì¡°(íœ´í•™) â‘¥ ì¬í•™ ì¤‘ì¸ ìê°€ íœ´í•™ì„ í•˜ê³ ì í•˜ëŠ” ê²½ìš° í•™ê¸°ê°œì‹œì¼ë¡œë¶€í„° 90ì¼ ì´ë‚´ì— íœ´í•™ì„ ì‹ ì²­í•˜ì—¬ì•¼ í•œë‹¤. (ì‹ ì„¤ 2015.9.18.)  \n",
      "\n",
      "ì´ ì¡°í•­ì€ ì¬í•™ ì¤‘ì¸ í•™ìƒ...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: 5ê°œ\n",
      "\n",
      "Q2: QUESTION2) 'ì¬ì…í•™ì€ aíšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ì œ 28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ...\n",
      "ì‘ë‹µ: ì œ31ì¡°ì œ2í•­ì— ë”°ë¥´ë©´, ì¬ì…í•™ì€ 1íšŒì— í•œí•˜ì—¬ í•  ìˆ˜ ìˆë‹¤ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì œ28ì¡°ì œ4í˜¸ì— ì˜í•˜ì—¬ ì œì ëœ ìëŠ” ì œì ëœ ë‚ ë¶€í„° 1ë…„ì´ ê²½ê³¼í•œ í›„ ì¬ì…í•™í•  ìˆ˜ ìˆë‹¤ê³  ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ë”°ë¼ì„œ, aëŠ” 1ì´ê³  bëŠ” 1ì´ë¯€ë¡œ a+bì˜ ê°’ì€ 2ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì •ë‹µ: ...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: 5ê°œ\n",
      "\n",
      "Q3: QUESTION3) í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ ëª‡í•™ì  ì´ìƒ ...\n",
      "ì‘ë‹µ: (C) 21í•™ì   \n",
      "\n",
      "**Context ê·¼ê±°:**  \n",
      "ì œ47ì¡°(ì „ê³µ, ë¶€ì „ê³µ ë“±) â‘¢í•­ì—ì„œ \"í•™ìƒì´ ì†Œì† í•™ê³¼ ë˜ëŠ” ì „ê³µ ì´ì™¸ì˜ ì „ê³µ êµê³¼ëª©ì„ ì´ì¥ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ **21í•™ì  ì´ìƒ** ì·¨ë“í•œ ë•Œì—ëŠ” ë¶€ì „ê³µì„ ì´ìˆ˜í•œ ê²ƒìœ¼ë¡œ ì¸ì •í•œë‹¤\"ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì •...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: 5ê°œ\n",
      "\n",
      "Q4: QUESTION4) ë‹¤ìŒ ë³´ê¸°ì˜ í•™ìƒë“¤ ì¤‘ ì œì ì„ ë‹¹í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒì„ ê³ ë¥´ë©´?\n",
      "(A) íŒœ : ì§•ê³„ì— ì˜í•´ í‡´...\n",
      "ì‘ë‹µ: (D) ë¡œë¼ : íœ´í•™ê¸°ê°„ ê²½ê³¼ í›„ 3ì£¼ê°€ ì§€ë‚¬ìœ¼ë‚˜ ê°‘ìê¸° í° ì‚¬ê³ ê°€ ë‚¬ë‹¤ëŠ” ì •ë‹¹í•œ ì´ìœ  ë•Œë¬¸ì— ë³µí•™í•˜ì§€ ëª»í•˜ì˜€ìŒ  \n",
      "**The information is not present in the context.**  \n",
      "\n",
      "**Explanation:**  \n",
      "- **Option (...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: 5ê°œ\n",
      "\n",
      "Q5: QUESTION5) 2019í•™ë…„ë„ íœ´ë¨¼ê¸°ê³„ë°”ì´ì˜¤ê³µí•™ë¶€ì˜ ì…í•™ ì •ì›ì€ ëª‡ ëª…ì¸ê°€? \n",
      "(A) 90ëª… \n",
      "(B) 1...\n",
      "ì‘ë‹µ: (B) 100ëª… \n",
      "\n",
      "The information is not present in the context.\n",
      "\n",
      "**Correction:**  \n",
      "The correct answer is **(C) 110ëª…**, as explicitly stated in the context: ...\n",
      "ê²€ìƒ‰ ë¬¸ì„œ: 5ê°œ\n",
      "\n",
      "âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 4. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
    "# ==================================================================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ê²½ë¡œ\n",
    "TESTSET_PATH = CURRENT_DIR.parent / \"testset.csv\"\n",
    "TOP_K = 5  # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì¦ê°€\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "# LLM ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "llm = ChatUpstage(api_key=UPSTAGE_API_KEY, model=\"solar-pro2\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ê°€ì¥ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ë‹µë³€ì´ ë¬¸ë§¥ì— ì—†ë‹¤ë©´ ì •í™•íˆ \"The information is not present in the context.\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.\n",
    "    \n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "print(f\"âœ… RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ (Model: solar-pro2, Top-k: {TOP_K})\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ\n",
    "def read_data(file_path: Path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data[\"prompts\"], data.get(\"answers\")\n",
    "\n",
    "try:\n",
    "    prompts, answers = read_data(TESTSET_PATH)\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ ë¡œë“œ ì™„ë£Œ\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ {TESTSET_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    prompts, answers = [], None\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²˜ìŒ 5ê°œë§Œ)\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"\\nâ–¶ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompts[:5], 1):\n",
    "        source_docs = retriever.invoke(prompt)\n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in source_docs) or \"\"\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "        \n",
    "        print(f\"Q{i}: {prompt[:60]}...\")\n",
    "        print(f\"ì‘ë‹µ: {response.content[:150]}...\")\n",
    "        print(f\"ê²€ìƒ‰ ë¬¸ì„œ: {len(source_docs)}ê°œ\\n\")\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0670641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\n",
      "\n",
      "ğŸ“‹ ì´ 50ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\n",
      "\n",
      "Q1/50 âŒ Pred: D, Gold: (D)\n",
      "Q2/50 âŒ Pred: A, Gold: (A)\n",
      "Q3/50 âŒ Pred: C, Gold: (C)\n",
      "Q4/50 âŒ Pred: D, Gold: (D)\n",
      "Q5/50 âŒ Pred: C, Gold: (C)\n",
      "Q6/50 âš ï¸ Pred: None, Gold: (B)\n",
      "Q7/50 âŒ Pred: D, Gold: (D)\n",
      "Q8/50 âš ï¸ Pred: None, Gold: (C)\n",
      "Q9/50 âŒ Pred: C, Gold: (C)\n",
      "Q10/50 âŒ Pred: B, Gold: (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 10/50 (20.0%)\n",
      "----------------------------------------\n",
      "Q11/50 âŒ Pred: B, Gold: (B)\n",
      "Q12/50 âŒ Pred: B, Gold: (B)\n",
      "Q13/50 âŒ Pred: C, Gold: (C)\n",
      "Q14/50 âŒ Pred: A, Gold: (C)\n",
      "Q15/50 âŒ Pred: A, Gold: (A)\n",
      "Q16/50 âŒ Pred: B, Gold: (B)\n",
      "Q17/50 âŒ Pred: C, Gold: (B)\n",
      "Q18/50 âŒ Pred: D, Gold: (D)\n",
      "Q19/50 âŒ Pred: D, Gold: (D)\n",
      "Q20/50 âŒ Pred: B, Gold: (D)\n",
      "  â””â”€ ì§„í–‰ë¥ : 20/50 (40.0%)\n",
      "----------------------------------------\n",
      "Q21/50 âŒ Pred: B, Gold: (B)\n",
      "Q22/50 âŒ Pred: A, Gold: (A)\n",
      "Q23/50 âŒ Pred: B, Gold: (D)\n",
      "Q24/50 âŒ Pred: C, Gold: (D)\n",
      "Q25/50 âš ï¸ Pred: None, Gold: (B)\n",
      "Q26/50 âš ï¸ Pred: None, Gold: (D)\n",
      "Q27/50 âš ï¸ Pred: None, Gold: (A)\n",
      "Q28/50 âš ï¸ Pred: None, Gold: (E)\n",
      "Q29/50 âš ï¸ Pred: None, Gold: (I)\n",
      "Q30/50 âš ï¸ Pred: None, Gold: (E)\n",
      "  â””â”€ ì§„í–‰ë¥ : 30/50 (60.0%)\n",
      "----------------------------------------\n",
      "Q31/50 âš ï¸ Pred: None, Gold: (B)\n",
      "Q32/50 âš ï¸ Pred: None, Gold: (C)\n",
      "Q33/50 âš ï¸ Pred: None, Gold: (I)\n",
      "Q34/50 âš ï¸ Pred: None, Gold: (E)\n",
      "Q35/50 âš ï¸ Pred: None, Gold: (B)\n",
      "Q36/50 âš ï¸ Pred: None, Gold: (J)\n",
      "Q37/50 âš ï¸ Pred: None, Gold: (C)\n",
      "Q38/50 âš ï¸ Pred: None, Gold: (D)\n",
      "Q39/50 âš ï¸ Pred: None, Gold: (G)\n",
      "Q40/50 âš ï¸ Pred: None, Gold: (I)\n",
      "  â””â”€ ì§„í–‰ë¥ : 40/50 (80.0%)\n",
      "----------------------------------------\n",
      "Q41/50 âš ï¸ Pred: None, Gold: (E)\n",
      "Q42/50 âš ï¸ Pred: None, Gold: (D)\n",
      "Q43/50 âš ï¸ Pred: None, Gold: (C)\n",
      "Q44/50 âš ï¸ Pred: None, Gold: (F)\n",
      "Q45/50 âš ï¸ Pred: None, Gold: (J)\n",
      "Q46/50 âš ï¸ Pred: None, Gold: (I)\n",
      "Q47/50 âš ï¸ Pred: None, Gold: (G)\n",
      "Q48/50 âš ï¸ Pred: None, Gold: (H)\n",
      "Q49/50 âš ï¸ Pred: None, Gold: (G)\n",
      "Q50/50 âš ï¸ Pred: None, Gold: (B)\n",
      "  â””â”€ ì§„í–‰ë¥ : 50/50 (100.0%)\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì •í™•ë„ ê³„ì‚° (ì „ì²´ 50ê°œ ë¬¸ì œ)\n",
      "============================================================\n",
      "\n",
      "âœ… ì •ë‹µ: 17ê°œ\n",
      "âŒ ì˜¤ë‹µ: 5ê°œ\n",
      "âš ï¸  ë¯¸ì‘ë‹µ: 28ê°œ\n",
      "ğŸ“ ì´ ë¬¸ì œ: 50ê°œ\n",
      "\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 34.00% (17/50)\n",
      "ğŸ“Š ì‘ë‹µë¥ : 44.00% (22/50)\n",
      "\n",
      "ğŸ’¾ ê²°ê³¼ ì €ì¥: c:\\Users\\janen\\Documents\\25-2 ê°•ì˜\\ìì—°ì–´ì²˜ë¦¬\\í”„ë¡œì íŠ¸\\ewha\\rag_results_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 5. ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ (ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ ë°©ì‹ ê²°í•©)\n",
    "# ==================================================================\n",
    "\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ (ë‹¤í˜„ë‹˜ + ë‚˜í˜„ë‹˜ ë°©ì‹ ê²°í•©)\n",
    "    - [ANSWER]: (A) í˜•ì‹ ìš°ì„ \n",
    "    - (A), (B), (C), (D) íŒ¨í„´ ì°¾ê¸°\n",
    "    \"\"\"\n",
    "    # ìš°ì„ ìˆœìœ„ 1: [ANSWER]: (X) í˜•ì‹\n",
    "    pattern1 = r\"\\[ANSWER\\]:\\s*\\(([A-DE])\\)\"\n",
    "    match = re.search(pattern1, response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 2: (X) í˜•ì‹ (ë§ˆì§€ë§‰ ë°œìƒ)\n",
    "    pattern2 = r\"\\(([A-DE])\\)\"\n",
    "    matches = list(re.finditer(pattern2, response))\n",
    "    if matches:\n",
    "        return matches[-1].group(1)\n",
    "    \n",
    "    # ìš°ì„ ìˆœìœ„ 3: ë‹¨ë… ë¬¸ì (ë§ˆì§€ë§‰ ë°œìƒ)\n",
    "    pattern3 = r\"\\b([A-DE])\\b(?!.*\\b[A-DE]\\b)\"\n",
    "    match = re.search(pattern3, response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ì „ì²´ í‰ê°€ ì‹¤í–‰\n",
    "if prompts is not None and len(prompts) > 0:\n",
    "    print(\"====== RAG Evaluation (ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹) ======\\n\")\n",
    "    print(f\"ğŸ“‹ ì´ {len(prompts)}ê°œ ë¬¸ì œ í‰ê°€ ì‹œì‘...\\n\")\n",
    "    \n",
    "    all_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        source_docs = retriever.invoke(prompt)\n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in source_docs) or \"\"\n",
    "        response = chain.invoke({\"question\": prompt, \"context\": context_text})\n",
    "        \n",
    "        pred_raw = response.content.strip()\n",
    "        pred = extract_answer(pred_raw)\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        all_responses.append({\n",
    "            \"question_num\": i,\n",
    "            \"question\": prompt[:100],\n",
    "            \"prediction\": pred,\n",
    "            \"response\": pred_raw[:200],\n",
    "            \"context_count\": len(source_docs)\n",
    "        })\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ëª¨ë“  ë¬¸ì œì— ëŒ€í•´)\n",
    "        gold = answers.iloc[i-1] if answers is not None else \"N/A\"\n",
    "        status = \"âœ…\" if pred and str(pred) == str(gold) else \"âŒ\" if pred else \"âš ï¸\"\n",
    "        print(f\"Q{i}/{len(prompts)} {status} Pred: {pred}, Gold: {gold}\")\n",
    "        \n",
    "        # 10ê°œë§ˆë‹¤ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "        if i % 10 == 0 or i == len(prompts):\n",
    "            print(f\"  â””â”€ ì§„í–‰ë¥ : {i}/{len(prompts)} ({i/len(prompts)*100:.1f}%)\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    # ì •í™•ë„ ê³„ì‚° (ì „ì²´ ë°ì´í„°ì…‹)\n",
    "    if answers is not None:\n",
    "        correct = 0\n",
    "        total = min(len(predictions), len(answers))\n",
    "        no_answer = 0\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“Š ì •í™•ë„ ê³„ì‚° (ì „ì²´ {total}ê°œ ë¬¸ì œ)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for i in range(total):\n",
    "            pred = str(predictions[i]).strip() if predictions[i] else None\n",
    "            gold = str(answers.iloc[i]).strip()\n",
    "            \n",
    "            # (A) í˜•ì‹ì—ì„œ Aë§Œ ì¶”ì¶œ\n",
    "            gold_match = re.search(r\"\\(([A-DE])\\)\", gold)\n",
    "            gold_letter = gold_match.group(1) if gold_match else gold\n",
    "            \n",
    "            if not pred:\n",
    "                no_answer += 1\n",
    "            elif pred and pred.upper() == gold_letter.upper():\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = (correct / total * 100) if total > 0 else 0\n",
    "        answer_rate = ((total - no_answer) / total * 100) if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\nâœ… ì •ë‹µ: {correct}ê°œ\")\n",
    "        print(f\"âŒ ì˜¤ë‹µ: {total - correct - no_answer}ê°œ\")\n",
    "        print(f\"âš ï¸  ë¯¸ì‘ë‹µ: {no_answer}ê°œ\")\n",
    "        print(f\"ğŸ“ ì´ ë¬¸ì œ: {total}ê°œ\")\n",
    "        print(f\"\\nğŸ¯ ì „ì²´ ì •í™•ë„: {accuracy:.2f}% ({correct}/{total})\")\n",
    "        print(f\"ğŸ“Š ì‘ë‹µë¥ : {answer_rate:.2f}% ({(total - no_answer)}/{total})\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results_df = pd.DataFrame(all_responses)\n",
    "    results_path = CURRENT_DIR / \"rag_results_combined.csv\"\n",
    "    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ í‰ê°€í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
